{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import os\n",
    "import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import graphviz\n",
    "\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:07<00:00, 15.99it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/html/*.html\"\n",
    "\n",
    "code2convos = dict()\n",
    "\n",
    "pbar = tqdm.tqdm(sorted(list(glob(data_path))))\n",
    "for path in pbar:\n",
    "    # print(Path.cwd() / path)\n",
    "    file_code = os.path.basename(path).split(\".\")[0]\n",
    "    with open(path, \"r\", encoding=\"latin1\") as fh:\n",
    "            \n",
    "        # get the file id to USE IT AS A KEY later on\n",
    "        fid = os.path.basename(path).split(\".\")[0]\n",
    "\n",
    "        # read the html file\n",
    "        html_page = fh.read()\n",
    "\n",
    "        # parse the html file with bs4 so we can extract needed stuff\n",
    "        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "        # grab the conversations with the data-testid pattern\n",
    "        data_test_id_pattern = re.compile(r\"conversation-turn-[0-9]+\")\n",
    "        conversations = soup.find_all(\"div\", attrs={\"data-testid\": data_test_id_pattern})\n",
    "\n",
    "        convo_texts = []\n",
    "\n",
    "        for i, convo in enumerate(conversations):\n",
    "            convo = convo.find_all(\"div\", attrs={\"data-message-author-role\":re.compile( r\"[user|assistant]\") })\n",
    "            if len(convo) > 0:\n",
    "                role = convo[0].get(\"data-message-author-role\")\n",
    "                convo_texts.append({\n",
    "                        \"role\" : role,\n",
    "                        \"text\" : convo[0].text\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "        code2convos[file_code] = convo_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user',\n",
      "  'text': 'Load a CSV file into a Pandas in Python. The file is named '\n",
      "          \"'cs412_hw1_dataset.csv' and contains columns like 'Species', \"\n",
      "          \"'Island', 'Sex', 'Diet', 'Year', 'Life Stage', 'Body Mass (g)', \"\n",
      "          \"'Bill Length (mm)', 'Bill Depth (mm)', 'Flipper Length (mm)', and \"\n",
      "          \"'Health Metrics'. \\n\"},\n",
      " {'role': 'assistant',\n",
      "  'text': 'You can load a CSV file into a Pandas DataFrame in Python using the '\n",
      "          \"pandas.read_csv() function. Here's how you can do it for your \"\n",
      "          'specific CSV file:pythonCopy codeimport pandas as pd\\n'\n",
      "          '\\n'\n",
      "          '# Specify the file path\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          '\\n'\n",
      "          '# Load the CSV file into a Pandas DataFrame\\n'\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          '# Display the first few rows of the DataFrame to verify the data '\n",
      "          'has been loaded\\n'\n",
      "          'print(df.head())\\n'\n",
      "          \"This code assumes that the CSV file 'cs412_hw1_dataset.csv' is in \"\n",
      "          'the same directory as your Python script or Jupyter Notebook. If '\n",
      "          'the file is in a different directory, you can specify the full path '\n",
      "          'to the file in the file_path variable.The pd.read_csv() function '\n",
      "          'will automatically infer the column names from the first row of the '\n",
      "          \"CSV file, so you don't need to explicitly specify the column names \"\n",
      "          'unless they are different from what you provided. You can access '\n",
      "          \"columns in the DataFrame using their names, e.g., df['Species'], \"\n",
      "          \"df['Island'], etc.\"},\n",
      " {'role': 'user',\n",
      "  'text': 'Provide Python code to understand a dataset using Pandas. Find the '\n",
      "          'shape of the dataset, display variable names, display a summary of '\n",
      "          'the dataset with the info() function, and show the first 5 rows '\n",
      "          'using the head() function.\\n'},\n",
      " {'role': 'assistant',\n",
      "  'text': 'You can use Pandas to understand a dataset by performing various '\n",
      "          \"exploratory data analysis (EDA) tasks. Here's a Python code snippet \"\n",
      "          'that accomplishes the tasks you mentioned:pythonCopy codeimport '\n",
      "          'pandas as pd\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          '# 1. Find the shape of the dataset\\n'\n",
      "          'shape = df.shape\\n'\n",
      "          'print(\"Shape of the dataset (rows, columns):\", shape)\\n'\n",
      "          '\\n'\n",
      "          '# 2. Display variable names (column names)\\n'\n",
      "          'variable_names = df.columns\\n'\n",
      "          'print(\"Variable names (column names):\")\\n'\n",
      "          'for name in variable_names:\\n'\n",
      "          '    print(name)\\n'\n",
      "          '\\n'\n",
      "          '# 3. Display a summary of the dataset with the info() function\\n'\n",
      "          'print(\"\\\\nSummary of the dataset:\")\\n'\n",
      "          'df.info()\\n'\n",
      "          '\\n'\n",
      "          '# 4. Show the first 5 rows using the head() function\\n'\n",
      "          'print(\"\\\\nFirst 5 rows of the dataset:\")\\n'\n",
      "          'print(df.head())\\n'\n",
      "          \"Here's a breakdown of what each part of the code does:It calculates \"\n",
      "          'and prints the shape of the dataset using df.shape, which gives you '\n",
      "          'the number of rows and columns in the DataFrame.It retrieves and '\n",
      "          'prints the variable names (column names) using df.columns.It '\n",
      "          'displays a summary of the dataset, including information about the '\n",
      "          'data types, non-null counts, and memory usage, using the df.info() '\n",
      "          'function.Finally, it displays the first 5 rows of the dataset using '\n",
      "          'df.head() to provide a glimpse of the data.You can run this code to '\n",
      "          'get an initial understanding of your dataset using Pandas.'},\n",
      " {'role': 'user',\n",
      "  'text': 'You will preprocess the data now\\n'\n",
      "          '1. Check for missing values and handle them by either dropping or '\n",
      "          'filling them with the most common values. Ensure that there is '\n",
      "          'enough data for training the model. You can only use %80 data for '\n",
      "          'training and %20 for testing\\n'\n",
      "          '2. Encode labels with mappings using the map function. Mapping '\n",
      "          'names: \\n'\n",
      "          \"   - sex_map = {'female': 1, 'male': 0}\\n\"\n",
      "          \"   - island_map = {'Biscoe': 1, 'Dream': 2, 'Torgensen': 3}\\n\"\n",
      "          \"   - diet_map = {'fish': 1, 'krill': 2, 'squid': 3, 'parental': 4}\\n\"\n",
      "          \"   - life_stage_map = {'chick': 1, 'juvenile': 2, 'adult': 3}\\n\"\n",
      "          \"   - health_metrics_map = {'healthy': 1, 'overweight': 2, \"\n",
      "          \"'underweight': 3}\\n\"\n",
      "          \"The dataset named 'df'.\"},\n",
      " {'role': 'assistant',\n",
      "  'text': 'To preprocess the dataset as per your requirements, you can follow '\n",
      "          'these steps:Check for missing values and handle them.Encode labels '\n",
      "          'with mappings using the map function.Split the data into training '\n",
      "          \"(80%) and testing (20%) sets.Here's the Python code to achieve \"\n",
      "          'this:pythonCopy codeimport pandas as pd\\n'\n",
      "          'from sklearn.model_selection import train_test_split\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          '# 1. Check for missing values and handle them\\n'\n",
      "          '# You can either drop rows with missing values or fill them with '\n",
      "          'the most common value in the column.\\n'\n",
      "          \"# In this example, we'll fill missing values with the most common \"\n",
      "          'value in each column.\\n'\n",
      "          '\\n'\n",
      "          \"# Fill missing values in the 'Sex' column with the most common \"\n",
      "          'value\\n'\n",
      "          \"most_common_sex = df['Sex'].mode()[0]\\n\"\n",
      "          \"df['Sex'].fillna(most_common_sex, inplace=True)\\n\"\n",
      "          '\\n'\n",
      "          \"# Fill missing values in the 'Island' column with the most common \"\n",
      "          'value\\n'\n",
      "          \"most_common_island = df['Island'].mode()[0]\\n\"\n",
      "          \"df['Island'].fillna(most_common_island, inplace=True)\\n\"\n",
      "          '\\n'\n",
      "          \"# Fill missing values in the 'Diet' column with the most common \"\n",
      "          'value\\n'\n",
      "          \"most_common_diet = df['Diet'].mode()[0]\\n\"\n",
      "          \"df['Diet'].fillna(most_common_diet, inplace=True)\\n\"\n",
      "          '\\n'\n",
      "          \"# Fill missing values in the 'Life Stage' column with the most \"\n",
      "          'common value\\n'\n",
      "          \"most_common_life_stage = df['Life Stage'].mode()[0]\\n\"\n",
      "          \"df['Life Stage'].fillna(most_common_life_stage, inplace=True)\\n\"\n",
      "          '\\n'\n",
      "          \"# Fill missing values in the 'Health Metrics' column with the most \"\n",
      "          'common value\\n'\n",
      "          \"most_common_health_metrics = df['Health Metrics'].mode()[0]\\n\"\n",
      "          \"df['Health Metrics'].fillna(most_common_health_metrics, \"\n",
      "          'inplace=True)\\n'\n",
      "          '\\n'\n",
      "          '# 2. Encode labels with mappings\\n'\n",
      "          \"sex_map = {'female': 1, 'male': 0}\\n\"\n",
      "          \"island_map = {'Biscoe': 1, 'Dream': 2, 'Torgensen': 3}\\n\"\n",
      "          \"diet_map = {'fish': 1, 'krill': 2, 'squid': 3, 'parental': 4}\\n\"\n",
      "          \"life_stage_map = {'chick': 1, 'juvenile': 2, 'adult': 3}\\n\"\n",
      "          \"health_metrics_map = {'healthy': 1, 'overweight': 2, 'underweight': \"\n",
      "          '3}\\n'\n",
      "          '\\n'\n",
      "          \"df['Sex'] = df['Sex'].map(sex_map)\\n\"\n",
      "          \"df['Island'] = df['Island'].map(island_map)\\n\"\n",
      "          \"df['Diet'] = df['Diet'].map(diet_map)\\n\"\n",
      "          \"df['Life Stage'] = df['Life Stage'].map(life_stage_map)\\n\"\n",
      "          \"df['Health Metrics'] = df['Health \"\n",
      "          \"Metrics'].map(health_metrics_map)\\n\"\n",
      "          '\\n'\n",
      "          '# 3. Split the data into training (80%) and testing (20%) sets\\n'\n",
      "          \"X = df.drop(columns=['Species'])  # Features (excluding 'Species' \"\n",
      "          'column)\\n'\n",
      "          \"y = df['Species']  # Target variable ('Species' column)\\n\"\n",
      "          '\\n'\n",
      "          'X_train, X_test, y_train, y_test = train_test_split(X, y, '\n",
      "          'test_size=0.2, random_state=42)\\n'\n",
      "          '\\n'\n",
      "          '# Now, you can use X_train, X_test, y_train, and y_test for '\n",
      "          'training and testing your machine learning model.\\n'\n",
      "          'This code performs the following steps:Handles missing values by '\n",
      "          'filling them with the most common values in each respective '\n",
      "          'column.Encodes categorical labels with mappings provided.Splits the '\n",
      "          'dataset into training (80%) and testing (20%) sets using '\n",
      "          'train_test_split from scikit-learn, allowing you to use the data '\n",
      "          'for training and testing your machine learning model.'},\n",
      " {'role': 'user',\n",
      "  'text': 'You already provide code for this but please recreate that part for '\n",
      "          'shuffling if needed. \\n'\n",
      "          \"1. Shuffle dataframe named 'df'.\\n\"\n",
      "          \"2. Separate the dataset into a feature matrix 'X' and a target \"\n",
      "          \"vector 'y'. The column 'health_metrics' is y, and all other columns \"\n",
      "          'should be included in X.\\n'\n",
      "          '3. Split the data into training and test sets with 80% of the data '\n",
      "          'for training and 20% for testing. \\n'\n",
      "          'Ensure that the splitting is random.\\n'},\n",
      " {'role': 'assistant',\n",
      "  'text': \"Certainly, here's the code to shuffle the DataFrame, separate it \"\n",
      "          \"into feature matrix 'X' and target vector 'y', and split the data \"\n",
      "          'into training and test sets with random shuffling:pythonCopy '\n",
      "          'codeimport pandas as pd\\n'\n",
      "          'from sklearn.model_selection import train_test_split\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          '# 1. Shuffle the DataFrame\\n'\n",
      "          'df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # '\n",
      "          'Shuffle with random_state for reproducibility\\n'\n",
      "          '\\n'\n",
      "          \"# 2. Separate the dataset into feature matrix 'X' and target vector \"\n",
      "          \"'y'\\n\"\n",
      "          \"X = df.drop(columns=['Health Metrics'])  # Features (excluding \"\n",
      "          \"'Health Metrics' column)\\n\"\n",
      "          \"y = df['Health Metrics']  # Target variable ('Health Metrics' \"\n",
      "          'column)\\n'\n",
      "          '\\n'\n",
      "          '# 3. Split the data into training and test sets with 80% for '\n",
      "          'training and 20% for testing\\n'\n",
      "          'X_train, X_test, y_train, y_test = train_test_split(X, y, '\n",
      "          'test_size=0.2, random_state=42)\\n'\n",
      "          '\\n'\n",
      "          '# Now, you have shuffled data in X_train, X_test, y_train, and '\n",
      "          'y_test for training and testing your machine learning model.\\n'\n",
      "          'In this code:We shuffle the DataFrame using the sample method with '\n",
      "          'frac=1 to shuffle all rows. We also set a random_state for '\n",
      "          'reproducibility.We separate the dataset into the feature matrix '\n",
      "          \"'X', which includes all columns except 'Health Metrics', and the \"\n",
      "          \"target vector 'y', which contains only the 'Health Metrics' \"\n",
      "          'column.We split the shuffled data into training (80%) and test '\n",
      "          '(20%) sets using train_test_split from scikit-learn, ensuring that '\n",
      "          'the splitting is random with a specified random_state.'},\n",
      " {'role': 'user',\n",
      "  'text': 'Calculate and Visualize the correlations of all features in a '\n",
      "          \"Pandas DataFrame with 'health_metrics'. The DataFrame is named \"\n",
      "          \"'df'. Ensure the code includes:\\n\"\n",
      "          '1. Calculation of correlation coefficients.\\n'\n",
      "          '2. Visualization of these correlations in a heatmap.\\n'\n",
      "          'Also, instruct on how to interpret the heatmap to highlight any '\n",
      "          'strong correlations with the target variable. Note: Use lowercase '\n",
      "          \"and underscores for column names, e.g., 'health_metrics' instead of \"\n",
      "          \"'Health Metrics'.\\n\"},\n",
      " {'role': 'assistant',\n",
      "  'text': 'To calculate and visualize the correlations of all features with '\n",
      "          \"the 'health_metrics' column in a Pandas DataFrame named 'df,' \"\n",
      "          'including the calculation of correlation coefficients and creating '\n",
      "          'a heatmap for visualization, you can use the following '\n",
      "          'code:pythonCopy codeimport pandas as pd\\n'\n",
      "          'import seaborn as sns\\n'\n",
      "          'import matplotlib.pyplot as plt\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          '# Calculate correlation coefficients\\n'\n",
      "          \"correlations = df.corr()['health_metrics'].drop('health_metrics')  \"\n",
      "          '# Remove self-correlation and select the target variable\\n'\n",
      "          '\\n'\n",
      "          '# Visualize the correlations in a heatmap\\n'\n",
      "          'plt.figure(figsize=(10, 6))\\n'\n",
      "          'sns.heatmap(df.corr(), annot=True, cmap=\\'coolwarm\\', fmt=\".2f\")\\n'\n",
      "          \"plt.title('Correlation Heatmap')\\n\"\n",
      "          'plt.show()\\n'\n",
      "          '\\n'\n",
      "          '# Interpretation of the heatmap:\\n'\n",
      "          '# - Correlation coefficients range from -1 to 1, where:\\n'\n",
      "          '#   - -1 indicates a perfect negative correlation (as one variable '\n",
      "          'goes up, the other goes down).\\n'\n",
      "          '#   - 1 indicates a perfect positive correlation (both variables '\n",
      "          'move in the same direction).\\n'\n",
      "          '#   - 0 indicates no correlation.\\n'\n",
      "          '# - In the heatmap, positive correlations are shown in warmer '\n",
      "          'colors (e.g., red), while negative correlations are shown in cooler '\n",
      "          'colors (e.g., blue).\\n'\n",
      "          \"# - Strong correlations with the target variable ('health_metrics') \"\n",
      "          'are indicated by values closer to -1 or 1, and these are the '\n",
      "          'features that have the most influence on the target.\\n'\n",
      "          '\\n'\n",
      "          '# To identify strong correlations with the target variable, you can '\n",
      "          'look for rows or columns in the heatmap with notably dark or light '\n",
      "          'colors (depending on the sign of the correlation).\\n'\n",
      "          'This code performs the following steps:Calculates the correlation '\n",
      "          'coefficients between all columns (features) and the '\n",
      "          \"'health_metrics' column using the corr() method. It then selects \"\n",
      "          \"the correlations for all features except 'health_metrics' \"\n",
      "          'itself.Creates a heatmap using Seaborn to visualize the '\n",
      "          'correlations. In the heatmap, positive correlations are represented '\n",
      "          'by warmer colors (e.g., red), while negative correlations are '\n",
      "          'represented by cooler colors (e.g., blue).The interpretation of the '\n",
      "          'heatmap is provided in the comments. Strong correlations with the '\n",
      "          \"target variable ('health_metrics') are indicated by values closer \"\n",
      "          'to -1 or 1, which are represented by notably dark or light colors '\n",
      "          'in the heatmap.'},\n",
      " {'role': 'user',\n",
      "  'text': \"ValueError: could not convert string to float: 'Chinstrap'\\n\"\n",
      "          'please recreate the code based on this error'},\n",
      " {'role': 'assistant',\n",
      "  'text': 'The error message \"ValueError: could not convert string to float: '\n",
      "          '\\'Chinstrap\\'\" suggests that you are trying to calculate '\n",
      "          \"correlations between a string column ('Species' in this case) and \"\n",
      "          \"the 'health_metrics' column, which contains non-numeric values. To \"\n",
      "          'calculate correlations, you need to convert the categorical column '\n",
      "          \"('Species') into a numeric format, such as one-hot encoding or \"\n",
      "          \"label encoding. Let's update the code to handle this situation \"\n",
      "          \"using label encoding for the 'Species' column:pythonCopy codeimport \"\n",
      "          'pandas as pd\\n'\n",
      "          'import seaborn as sns\\n'\n",
      "          'import matplotlib.pyplot as plt\\n'\n",
      "          'from sklearn.preprocessing import LabelEncoder\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          \"# 1. Label encode the 'Species' column\\n\"\n",
      "          'label_encoder = LabelEncoder()\\n'\n",
      "          \"df['Species_encoded'] = label_encoder.fit_transform(df['Species'])\\n\"\n",
      "          '\\n'\n",
      "          '# 2. Calculate correlation coefficients\\n'\n",
      "          \"correlations = df.corr()['health_metrics'].drop(['health_metrics', \"\n",
      "          \"'Species_encoded'])\\n\"\n",
      "          '\\n'\n",
      "          '# 3. Visualize the correlations in a heatmap\\n'\n",
      "          'plt.figure(figsize=(10, 6))\\n'\n",
      "          'sns.heatmap(df.corr(), annot=True, cmap=\\'coolwarm\\', fmt=\".2f\")\\n'\n",
      "          \"plt.title('Correlation Heatmap')\\n\"\n",
      "          'plt.show()\\n'\n",
      "          '\\n'\n",
      "          '# Interpretation of the heatmap remains the same as described in '\n",
      "          'the previous response.\\n'\n",
      "          'In this updated code:We use LabelEncoder from scikit-learn to '\n",
      "          \"convert the 'Species' column into a numeric format \"\n",
      "          \"('Species_encoded'). This allows us to calculate correlations with \"\n",
      "          \"'health_metrics' since it's not possible to directly calculate \"\n",
      "          'correlations with string values.We calculate the correlation '\n",
      "          \"coefficients, excluding 'health_metrics' and 'Species_encoded', as \"\n",
      "          'they are not relevant for this purpose.We create the heatmap to '\n",
      "          'visualize the correlations as previously explained.Now, this code '\n",
      "          'should work without raising the \"ValueError\" and provide '\n",
      "          \"correlations with the 'health_metrics' column.\"},\n",
      " {'role': 'user',\n",
      "  'text': 'Dataset includes both numerical and categorical columns. I need to '\n",
      "          'calculate the correlations of all numerical features with a target '\n",
      "          \"variable 'health_metrics'. However, I'm encountering a ValueError \"\n",
      "          'because of non-numeric columns. Modify the code for checking if the '\n",
      "          'value is a numerical one first After calculating the correlations, '\n",
      "          'also guide me on how to visualize these correlations in a heatmap '\n",
      "          'using seaborn or matplotlib. Please ensure that all column names '\n",
      "          'are in lowercase and use underscores instead of spaces.\\n'},\n",
      " {'role': 'assistant',\n",
      "  'text': 'To calculate the correlations of only numerical features with the '\n",
      "          \"target variable 'health_metrics' and avoid issues with non-numeric \"\n",
      "          'columns, you can modify the code as follows:pythonCopy codeimport '\n",
      "          'pandas as pd\\n'\n",
      "          'import seaborn as sns\\n'\n",
      "          'import matplotlib.pyplot as plt\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          '# Filter out only the numerical columns\\n'\n",
      "          \"numerical_columns = df.select_dtypes(include=['number'])\\n\"\n",
      "          '\\n'\n",
      "          '# Calculate correlation coefficients\\n'\n",
      "          'correlations = '\n",
      "          \"numerical_columns.corr()['health_metrics'].drop('health_metrics')\\n\"\n",
      "          '\\n'\n",
      "          '# Visualize the correlations in a heatmap\\n'\n",
      "          'plt.figure(figsize=(10, 6))\\n'\n",
      "          \"sns.heatmap(correlations.to_frame(), annot=True, cmap='coolwarm', \"\n",
      "          'fmt=\".2f\")\\n'\n",
      "          \"plt.title('Correlation Heatmap')\\n\"\n",
      "          'plt.show()\\n'\n",
      "          '\\n'\n",
      "          '# Interpretation of the heatmap remains the same as described in '\n",
      "          'the previous response.\\n'\n",
      "          'In this modified code:We first filter out only the numerical '\n",
      "          \"columns using select_dtypes(include=['number']). This ensures that \"\n",
      "          'we are calculating correlations only with numerical features.We '\n",
      "          \"calculate the correlation coefficients with the 'health_metrics' \"\n",
      "          'column and drop the correlation with itself (which is always 1).We '\n",
      "          'create the heatmap to visualize the correlations using Seaborn as '\n",
      "          'previously explained.The heatmap will now display correlations only '\n",
      "          \"between numerical features and the 'health_metrics' target \"\n",
      "          'variable. You can interpret it as described in the previous '\n",
      "          'response, looking for notably dark or light colors to identify '\n",
      "          'strong correlations with the target variable.'},\n",
      " {'role': 'user',\n",
      "  'text': \"I have a dataset in a Pandas DataFrame named 'df' with both \"\n",
      "          'numerical and categorical columns. The target variable '\n",
      "          \"'health_metrics' is categorical with values like 'overweight', \"\n",
      "          \"'underweight', etc. I need to encode this target variable \"\n",
      "          'numerically and then calculate the correlations of all features '\n",
      "          'with this encoded target. Please guide me on how to encode '\n",
      "          \"'health_metrics' into a numerical form and then compute \"\n",
      "          'correlations with other features. Additionally, provide '\n",
      "          'instructions on visualizing these correlations in a heatmap. '\n",
      "          'Remember to use lowercase and underscores for column names.\\n'},\n",
      " {'role': 'assistant',\n",
      "  'text': \"To encode the categorical target variable 'health_metrics' into a \"\n",
      "          'numerical form and calculate correlations with other features, you '\n",
      "          \"can follow these steps:Encode 'health_metrics' into numerical \"\n",
      "          'values.Calculate correlations between all features (both numerical '\n",
      "          'and encoded categorical) with the encoded '\n",
      "          \"'health_metrics'.Visualize these correlations in a heatmap using \"\n",
      "          \"Seaborn or Matplotlib.Here's the Python code to achieve \"\n",
      "          'this:pythonCopy codeimport pandas as pd\\n'\n",
      "          'import seaborn as sns\\n'\n",
      "          'import matplotlib.pyplot as plt\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          \"# 1. Encode 'health_metrics' into numerical values\\n\"\n",
      "          \"health_metrics_map = {'healthy': 1, 'overweight': 2, 'underweight': \"\n",
      "          '3}\\n'\n",
      "          \"df['health_metrics_encoded'] = \"\n",
      "          \"df['health_metrics'].map(health_metrics_map)\\n\"\n",
      "          '\\n'\n",
      "          '# 2. Calculate correlations with all features, including the '\n",
      "          'encoded target\\n'\n",
      "          'correlations = '\n",
      "          \"df.corr()['health_metrics_encoded'].drop(['health_metrics_encoded', \"\n",
      "          \"'health_metrics'])\\n\"\n",
      "          '\\n'\n",
      "          '# 3. Visualize the correlations in a heatmap\\n'\n",
      "          'plt.figure(figsize=(10, 6))\\n'\n",
      "          \"sns.heatmap(correlations.to_frame(), annot=True, cmap='coolwarm', \"\n",
      "          'fmt=\".2f\")\\n'\n",
      "          \"plt.title('Correlation Heatmap')\\n\"\n",
      "          'plt.show()\\n'\n",
      "          '\\n'\n",
      "          '# Interpretation of the heatmap:\\n'\n",
      "          '# - Darker colors indicate stronger correlations, whether positive '\n",
      "          \"or negative, with the encoded 'health_metrics'.\\n\"\n",
      "          '# - Positive correlations are represented by values closer to 1, '\n",
      "          'while negative correlations are closer to -1.\\n'\n",
      "          \"In this code:We encode the categorical 'health_metrics' column into \"\n",
      "          'numerical values using the provided health_metrics_map. The encoded '\n",
      "          \"values are stored in a new column named 'health_metrics_encoded.'We \"\n",
      "          'calculate the correlations between all features (both numerical and '\n",
      "          \"the encoded 'health_metrics') with the encoded \"\n",
      "          \"'health_metrics_encoded' column.We create a heatmap to visualize \"\n",
      "          'these correlations. Darker colors indicate stronger correlations, '\n",
      "          'and you can interpret them as described in the comments.This code '\n",
      "          'will help you visualize the correlations between all features and '\n",
      "          \"the encoded 'health_metrics' target variable.\"},\n",
      " {'role': 'user',\n",
      "  'text': \"I've encoded the categorical target variable 'health_metrics' into \"\n",
      "          \"a numerical format. However, I'm facing a ValueError: could not \"\n",
      "          'convert string to float when trying to calculate feature '\n",
      "          'correlations. How can I modify my code to calculate correlations '\n",
      "          \"only between numerical features and the encoded 'health_metrics' \"\n",
      "          \"column, excluding any other categorical columns like 'species'? \"\n",
      "          'Please provide a Python code snippet that correctly computes these '\n",
      "          'correlations and a way to visualize them in a heatmap.\\n'},\n",
      " {'role': 'assistant',\n",
      "  'text': 'To calculate correlations only between numerical features and the '\n",
      "          \"encoded 'health_metrics' column while excluding other categorical \"\n",
      "          \"columns like 'species,' you can follow these steps:Encode the \"\n",
      "          \"'health_metrics' column into numerical values.Select only the \"\n",
      "          'numerical columns for correlation calculation.Calculate the '\n",
      "          'correlations.Visualize these correlations in a heatmap using '\n",
      "          \"Seaborn or Matplotlib.Here's a Python code snippet that \"\n",
      "          'accomplishes this:pythonCopy codeimport pandas as pd\\n'\n",
      "          'import seaborn as sns\\n'\n",
      "          'import matplotlib.pyplot as plt\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          \"# 1. Encode 'health_metrics' into numerical values\\n\"\n",
      "          \"health_metrics_map = {'healthy': 1, 'overweight': 2, 'underweight': \"\n",
      "          '3}\\n'\n",
      "          \"df['health_metrics_encoded'] = \"\n",
      "          \"df['health_metrics'].map(health_metrics_map)\\n\"\n",
      "          '\\n'\n",
      "          '# 2. Select only the numerical columns for correlation calculation\\n'\n",
      "          \"numerical_columns = df.select_dtypes(include=['number'])\\n\"\n",
      "          '\\n'\n",
      "          '# 3. Calculate correlations\\n'\n",
      "          'correlations = '\n",
      "          \"numerical_columns.corrwith(df['health_metrics_encoded']).sort_values(ascending=False)\\n\"\n",
      "          '\\n'\n",
      "          '# 4. Visualize the correlations in a heatmap\\n'\n",
      "          'plt.figure(figsize=(10, 6))\\n'\n",
      "          \"sns.heatmap(correlations.to_frame(), annot=True, cmap='coolwarm', \"\n",
      "          'fmt=\".2f\")\\n'\n",
      "          \"plt.title('Correlations with Encoded health_metrics')\\n\"\n",
      "          'plt.show()\\n'\n",
      "          '\\n'\n",
      "          '# Interpretation of the heatmap:\\n'\n",
      "          '# - Darker colors indicate stronger positive or negative '\n",
      "          'correlations between numerical features and the encoded '\n",
      "          \"'health_metrics'.\\n\"\n",
      "          '# - Positive correlations are represented by values closer to 1, '\n",
      "          'while negative correlations are closer to -1.\\n'\n",
      "          \"In this code:We encode the categorical 'health_metrics' column into \"\n",
      "          'numerical values, as you did before.We select only the numerical '\n",
      "          \"columns using select_dtypes(include=['number']), ensuring that we \"\n",
      "          'are including only numerical features for correlation '\n",
      "          'calculation.We calculate the correlations between the selected '\n",
      "          \"numerical columns and the encoded 'health_metrics_encoded' column \"\n",
      "          'using corrwith.We create a heatmap to visualize these correlations, '\n",
      "          'where darker colors indicate stronger correlations, and you can '\n",
      "          'interpret them as described in the comments.This code should '\n",
      "          'correctly calculate and visualize the correlations between '\n",
      "          \"numerical features and the encoded 'health_metrics' column while \"\n",
      "          \"excluding other categorical columns like 'species.'\"},\n",
      " {'role': 'user',\n",
      "  'text': 'With computed correlations between features and an encoded target '\n",
      "          'variable, guide me on how to select a subset of features that are '\n",
      "          'likely strong predictors for the target. Explain how to interpret '\n",
      "          'the correlation values to identify these features and provide a '\n",
      "          'Python code example for selecting them based on their correlation '\n",
      "          'strengths. Emphasize the justification for choosing these features '\n",
      "          'based on the correlation analysis.'},\n",
      " {'role': 'assistant',\n",
      "  'text': 'To select a subset of features that are likely strong predictors '\n",
      "          'for the target variable based on correlation analysis, you can '\n",
      "          'follow these steps:Identify the features with the highest positive '\n",
      "          'and negative correlation values with the target variable.Choose a '\n",
      "          'threshold for correlation strength that determines which features '\n",
      "          'to select.Justify your selection based on the correlation values '\n",
      "          \"and domain knowledge.Here's a Python code example that demonstrates \"\n",
      "          'how to select features based on correlation strengths:pythonCopy '\n",
      "          'codeimport pandas as pd\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          \"# Encode the 'health_metrics' column into numerical values\\n\"\n",
      "          \"health_metrics_map = {'healthy': 1, 'overweight': 2, 'underweight': \"\n",
      "          '3}\\n'\n",
      "          \"df['health_metrics_encoded'] = \"\n",
      "          \"df['health_metrics'].map(health_metrics_map)\\n\"\n",
      "          '\\n'\n",
      "          '# Calculate correlations between numerical features and the encoded '\n",
      "          \"'health_metrics'\\n\"\n",
      "          'correlations = '\n",
      "          \"df.corrwith(df['health_metrics_encoded']).sort_values(ascending=False)\\n\"\n",
      "          '\\n'\n",
      "          '# Set a threshold for selecting strong predictors\\n'\n",
      "          'correlation_threshold = 0.2  # You can adjust this threshold based '\n",
      "          'on your requirements\\n'\n",
      "          '\\n'\n",
      "          '# Select features with correlations above the threshold (positive '\n",
      "          'and negative)\\n'\n",
      "          'selected_features = correlations[(correlations > '\n",
      "          'correlation_threshold) | (correlations < -correlation_threshold)]\\n'\n",
      "          '\\n'\n",
      "          '# Display the selected features and their correlation values\\n'\n",
      "          'print(\"Selected Features:\")\\n'\n",
      "          'print(selected_features)\\n'\n",
      "          '\\n'\n",
      "          '# Interpretation and justification:\\n'\n",
      "          '# - Features with positive correlations indicate that as the '\n",
      "          \"feature increases, the likelihood of 'health_metrics' being \"\n",
      "          \"'overweight' or 'underweight' also increases.\\n\"\n",
      "          '# - Features with negative correlations indicate the opposite, as '\n",
      "          \"the feature decreases, the likelihood of 'health_metrics' being \"\n",
      "          \"'overweight' or 'underweight' increases.\\n\"\n",
      "          '# - By setting a threshold (e.g., 0.2), you can choose features '\n",
      "          \"that have a relatively strong impact on 'health_metrics'.\\n\"\n",
      "          '# - The threshold can be adjusted based on your domain knowledge '\n",
      "          'and the specific requirements of your predictive model.\\n'\n",
      "          '\\n'\n",
      "          '# You can further analyze these selected features and perform '\n",
      "          'machine learning modeling with them.\\n'\n",
      "          'In this code:We calculate the correlations between numerical '\n",
      "          \"features and the encoded 'health_metrics' column.We set a threshold \"\n",
      "          '(correlation_threshold) to determine which features are considered '\n",
      "          'strong predictors. You can adjust this threshold based on your '\n",
      "          'domain knowledge and requirements. Features with correlations above '\n",
      "          'or below this threshold (positive or negative) are selected.We '\n",
      "          'display the selected features and their correlation values, '\n",
      "          'providing an initial list of potential predictors for the '\n",
      "          \"'health_metrics' target variable.Remember that correlation does not \"\n",
      "          'imply causation, and the selected features should be further '\n",
      "          'analyzed and validated in the context of your specific prediction '\n",
      "          'or classification problem. Domain knowledge and domain-specific '\n",
      "          'expertise are crucial for making informed decisions about feature '\n",
      "          'selection.'},\n",
      " {'role': 'user',\n",
      "  'text': 'I need to propose two hypothetical features that could enhance the '\n",
      "          \"predictive accuracy for the target variable 'health_metrics'. Can \"\n",
      "          'you suggest two potential new features, explaining how they might '\n",
      "          'be derived from the existing data or external sources, and their '\n",
      "          \"expected impact on the model's accuracy? Also, provide a \"\n",
      "          'hypothetical example of how to calculate and show the correlations '\n",
      "          'of these new features with the target variable using Python. Note '\n",
      "          \"that the target variable 'health_metrics' is categorical and has \"\n",
      "          'been numerically encoded.\"\\n'},\n",
      " {'role': 'assistant',\n",
      "  'text': 'Certainly! Here are two hypothetical features that could enhance '\n",
      "          \"predictive accuracy for the target variable 'health_metrics' based \"\n",
      "          \"on the existing data and their expected impact on the model's \"\n",
      "          'accuracy:Daily Caloric Intake (New Feature 1):Derivation: Calculate '\n",
      "          'the daily caloric intake of each penguin based on their diet and '\n",
      "          'body mass. You can estimate the caloric intake required for '\n",
      "          'different diets (e.g., fish, krill, squid, parental) and then '\n",
      "          'calculate the daily caloric intake by multiplying the estimated '\n",
      "          \"value by the penguin's body mass (in grams).Expected Impact: \"\n",
      "          'Penguins with insufficient daily caloric intake compared to their '\n",
      "          'dietary requirements may have a higher likelihood of being '\n",
      "          \"'underweight.' Conversely, those with excessive caloric intake may \"\n",
      "          \"have a higher likelihood of being 'overweight.' This feature could \"\n",
      "          \"provide valuable information to the model about the penguin's \"\n",
      "          'nutrition and health status.Seasonality Indicator (New Feature '\n",
      "          '2):Derivation: Determine the seasonality of data collection by '\n",
      "          \"examining the 'Year' column. You can categorize the years into \"\n",
      "          \"different seasons (e.g., summer, winter) based on the hemisphere's \"\n",
      "          \"calendar and location of the penguins' habitat.Expected Impact: \"\n",
      "          'Seasonal changes may influence the availability of food and, '\n",
      "          'consequently, the health of penguins. For example, during certain '\n",
      "          'seasons, food sources may be more abundant, leading to better '\n",
      "          'health. This feature can help the model account for '\n",
      "          \"seasonality-related variations in 'health_metrics.'Here's a \"\n",
      "          'hypothetical Python code snippet to calculate and visualize the '\n",
      "          'correlations of these new features with the encoded '\n",
      "          \"'health_metrics' target variable:pythonCopy codeimport pandas as \"\n",
      "          'pd\\n'\n",
      "          'import seaborn as sns\\n'\n",
      "          'import matplotlib.pyplot as plt\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          \"# Encode the 'health_metrics' column into numerical values\\n\"\n",
      "          \"health_metrics_map = {'healthy': 1, 'overweight': 2, 'underweight': \"\n",
      "          '3}\\n'\n",
      "          \"df['health_metrics_encoded'] = \"\n",
      "          \"df['health_metrics'].map(health_metrics_map)\\n\"\n",
      "          '\\n'\n",
      "          '# Hypothetical Feature 1: Daily Caloric Intake\\n'\n",
      "          '# Calculate daily caloric intake (hypothetical calculation)\\n'\n",
      "          \"caloric_intake = df['Diet'].map({'fish': 500, 'krill': 300, \"\n",
      "          \"'squid': 400, 'parental': 600}) * df['Body Mass (g)']\\n\"\n",
      "          '\\n'\n",
      "          '# Hypothetical Feature 2: Seasonality Indicator\\n'\n",
      "          '# Categorize years into seasons (hypothetical categorization)\\n'\n",
      "          \"seasonality = df['Year'].map({2017: 'Winter', 2018: 'Spring', 2019: \"\n",
      "          \"'Summer', 2020: 'Autumn'})\\n\"\n",
      "          '\\n'\n",
      "          '# Calculate correlations with the new features\\n'\n",
      "          'correlation_caloric_intake = '\n",
      "          \"df['health_metrics_encoded'].corr(caloric_intake)\\n\"\n",
      "          'correlation_seasonality = '\n",
      "          \"df['health_metrics_encoded'].corr(seasonality)\\n\"\n",
      "          '\\n'\n",
      "          '# Visualize the correlations\\n'\n",
      "          'plt.figure(figsize=(10, 6))\\n'\n",
      "          \"sns.barplot(x=['Daily Caloric Intake', 'Seasonality'], \"\n",
      "          'y=[correlation_caloric_intake, correlation_seasonality])\\n'\n",
      "          \"plt.title('Correlations of New Features with Encoded \"\n",
      "          \"health_metrics')\\n\"\n",
      "          \"plt.ylabel('Correlation Coefficient')\\n\"\n",
      "          'plt.ylim(-1, 1)\\n'\n",
      "          'plt.show()\\n'\n",
      "          '\\n'\n",
      "          '# Interpretation:\\n'\n",
      "          '# - Positive correlations indicate that the feature is positively '\n",
      "          \"related to 'health_metrics.'\\n\"\n",
      "          '# - Negative correlations indicate a negative relationship.\\n'\n",
      "          '# - The higher the absolute correlation coefficient, the stronger '\n",
      "          \"the feature's influence on 'health_metrics.'\\n\"\n",
      "          'In this code, we calculate the correlations of the hypothetical new '\n",
      "          \"features ('Daily Caloric Intake' and 'Seasonality Indicator') with \"\n",
      "          \"the encoded 'health_metrics' target variable and visualize them \"\n",
      "          'using a bar plot. Positive correlations suggest a positive '\n",
      "          \"relationship with 'health_metrics,' while negative correlations \"\n",
      "          'suggest a negative relationship. The magnitude of the correlation '\n",
      "          'coefficient indicates the strength of the relationship.'},\n",
      " {'role': 'user',\n",
      "  'text': \"Encountered a KeyError for the column 'Diet'. How can I identify \"\n",
      "          'and correct the issue causing this error? Please guide me on how to '\n",
      "          'solve If the column name is different, provide guidance on how to '\n",
      "          'access it correctly in Python. Additionally, if the column does not '\n",
      "          'exist, suggest ways to handle this situation.\"\\n'},\n",
      " {'role': 'assistant',\n",
      "  'text': \"If you encountered a KeyError for the column 'Diet,' it suggests \"\n",
      "          \"that the column 'Diet' does not exist in your DataFrame. To \"\n",
      "          'identify and correct the issue, you can follow these steps:Check '\n",
      "          \"the column names: Verify that the column name 'Diet' exists in your \"\n",
      "          \"DataFrame. It's possible that the column name is different from \"\n",
      "          'what you expected. You can print the list of column names to '\n",
      "          \"confirm.Handle missing columns: If the 'Diet' column is missing, \"\n",
      "          'you can consider the following options:Check for typos: Ensure that '\n",
      "          'you have spelled the column name correctly. Column names are '\n",
      "          'case-sensitive.Confirm dataset structure: Review the dataset or its '\n",
      "          \"documentation to make sure that the 'Diet' column is supposed to be \"\n",
      "          'present.Update column name: If the column name is different, you '\n",
      "          \"can access it using the correct name.Here's how you can check the \"\n",
      "          'column names and handle the situation if the column name is '\n",
      "          \"different or if the column doesn't exist:pythonCopy codeimport \"\n",
      "          'pandas as pd\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          '# 1. Check the column names\\n'\n",
      "          'print(\"Column names in the DataFrame:\")\\n'\n",
      "          'print(df.columns)\\n'\n",
      "          '\\n'\n",
      "          '# 2. Handle missing columns\\n'\n",
      "          \"if 'Diet' not in df.columns:\\n\"\n",
      "          \"    # If 'Diet' is not found, you can handle it based on your \"\n",
      "          'needs.\\n'\n",
      "          '    # For example, you can access a different column with a similar '\n",
      "          'purpose if available, or perform other data handling tasks.\\n'\n",
      "          '    print(\"The \\'Diet\\' column is missing. Please check the dataset '\n",
      "          'structure or handle this situation accordingly.\")\\n'\n",
      "          'else:\\n'\n",
      "          \"    # If 'Diet' exists, you can access it as df['Diet'].\\n\"\n",
      "          \"    diet_column = df['Diet']\\n\"\n",
      "          \"    # Now, you can use the 'diet_column' for your calculations.\\n\"\n",
      "          '    print(\"The \\'Diet\\' column exists, and you can access it.\")\\n'\n",
      "          \"By following these steps, you can identify whether the 'Diet' \"\n",
      "          'column is present in your DataFrame, handle situations where the '\n",
      "          'column name is different, or take appropriate actions if the column '\n",
      "          \"doesn't exist.\"},\n",
      " {'role': 'user',\n",
      "  'text': 'i find out the error is because you used capital letters on column '\n",
      "          'names dont do it again. Also daily calorie intake  correlecian '\n",
      "          'coefficient is 0 (straight line) and seasonality is no line at all '\n",
      "          'please solve these issues'},\n",
      " {'role': 'assistant',\n",
      "  'text': 'I apologize for the oversight in using capital letters for column '\n",
      "          \"names. To resolve the correlation issues, let's make sure we are \"\n",
      "          'using the correct column names and address any issues with the '\n",
      "          \"calculations. Here's the updated code:pythonCopy codeimport pandas \"\n",
      "          'as pd\\n'\n",
      "          'import seaborn as sns\\n'\n",
      "          'import matplotlib.pyplot as plt\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          \"# Encode the 'health_metrics' column into numerical values\\n\"\n",
      "          \"health_metrics_map = {'healthy': 1, 'overweight': 2, 'underweight': \"\n",
      "          '3}\\n'\n",
      "          \"df['health_metrics_encoded'] = df['Health \"\n",
      "          \"Metrics'].map(health_metrics_map)\\n\"\n",
      "          '\\n'\n",
      "          '# Hypothetical Feature 1: Daily Caloric Intake\\n'\n",
      "          '# Calculate daily caloric intake (hypothetical calculation)\\n'\n",
      "          \"diet_caloric_map = {'fish': 500, 'krill': 300, 'squid': 400, \"\n",
      "          \"'parental': 600}\\n\"\n",
      "          \"df['Daily Caloric Intake'] = df['Diet'].map(diet_caloric_map) * \"\n",
      "          \"df['Body Mass (g)']\\n\"\n",
      "          '\\n'\n",
      "          '# Hypothetical Feature 2: Seasonality Indicator\\n'\n",
      "          '# Categorize years into seasons (hypothetical categorization)\\n'\n",
      "          \"seasonality_map = {2007: 'Winter', 2008: 'Spring', 2009: 'Summer', \"\n",
      "          \"2010: 'Autumn'}\\n\"\n",
      "          \"df['Seasonality'] = df['Year'].map(seasonality_map)\\n\"\n",
      "          '\\n'\n",
      "          '# Calculate correlations with the new features\\n'\n",
      "          'correlation_caloric_intake = '\n",
      "          \"df['health_metrics_encoded'].corr(df['Daily Caloric Intake'])\\n\"\n",
      "          'correlation_seasonality = '\n",
      "          \"df['health_metrics_encoded'].corr(df['Seasonality'])\\n\"\n",
      "          '\\n'\n",
      "          '# Visualize the correlations\\n'\n",
      "          'plt.figure(figsize=(10, 6))\\n'\n",
      "          \"sns.barplot(x=['Daily Caloric Intake', 'Seasonality'], \"\n",
      "          'y=[correlation_caloric_intake, correlation_seasonality])\\n'\n",
      "          \"plt.title('Correlations of New Features with Encoded Health \"\n",
      "          \"Metrics')\\n\"\n",
      "          \"plt.ylabel('Correlation Coefficient')\\n\"\n",
      "          'plt.ylim(-1, 1)\\n'\n",
      "          'plt.show()\\n'\n",
      "          '\\n'\n",
      "          '# Interpretation:\\n'\n",
      "          \"# - A correlation of 0 for 'Daily Caloric Intake' suggests no \"\n",
      "          \"linear relationship with 'Health Metrics.'\\n\"\n",
      "          \"# - No line for 'Seasonality' indicates no linear relationship as \"\n",
      "          'well.\\n'\n",
      "          'In this updated code:We corrected the column names to use lowercase '\n",
      "          'and underscores.We addressed the issues with correlation '\n",
      "          \"coefficients. If the correlations for 'Daily Caloric Intake' and \"\n",
      "          \"'Seasonality' are 0 or close to 0, it suggests that there may not \"\n",
      "          'be a strong linear relationship between these features and the '\n",
      "          \"encoded 'Health Metrics.' In such cases, it's essential to explore \"\n",
      "          'other feature engineering or selection techniques to improve model '\n",
      "          'accuracy.'},\n",
      " {'role': 'user',\n",
      "  'text': 'Provide guidance on using GridSearchCV in scikit-learn to tune '\n",
      "          'hyperparameters of a DecisionTreeClassifier. I want to choose two '\n",
      "          'hyperparameters to tune based on the Scikit-learn decision tree '\n",
      "          'documentation. Explain how to set up GridSearchCV with a '\n",
      "          'cross-validation value of 5 and use validation accuracy to '\n",
      "          'determine the best hyperparameters. The dataset is in a DataFrame '\n",
      "          \"named 'df' and the target variable is 'health_metrics'.\\n\"},\n",
      " {'role': 'assistant',\n",
      "  'text': 'To use GridSearchCV in scikit-learn to tune hyperparameters of a '\n",
      "          'DecisionTreeClassifier, you can follow these steps:Import necessary '\n",
      "          'libraries.Define the parameter grid with the hyperparameters you '\n",
      "          'want to tune.Create a DecisionTreeClassifier.Create a GridSearchCV '\n",
      "          'instance with the classifier and parameter grid.Fit the '\n",
      "          'GridSearchCV object to your data and target variable.Retrieve and '\n",
      "          \"analyze the best hyperparameters.Here's a Python code example with \"\n",
      "          'explanations for each step:pythonCopy codeimport pandas as pd\\n'\n",
      "          'from sklearn.model_selection import GridSearchCV, train_test_split\\n'\n",
      "          'from sklearn.tree import DecisionTreeClassifier\\n'\n",
      "          '\\n'\n",
      "          '# Load the dataset into a Pandas DataFrame\\n'\n",
      "          \"file_path = 'cs412_hw1_dataset.csv'\\n\"\n",
      "          'df = pd.read_csv(file_path)\\n'\n",
      "          '\\n'\n",
      "          '# Split the data into features (X) and target variable (y)\\n'\n",
      "          \"X = df.drop(columns=['Health Metrics'])  # Features (excluding \"\n",
      "          'target variable)\\n'\n",
      "          \"y = df['Health Metrics']  # Target variable\\n\"\n",
      "          '\\n'\n",
      "          '# Define the parameter grid for tuning\\n'\n",
      "          'param_grid = {\\n'\n",
      "          \"    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree \"\n",
      "          '(None for unlimited depth)\\n'\n",
      "          \"    'min_samples_split': [2, 5, 10],  # Minimum number of samples \"\n",
      "          'required to split an internal node\\n'\n",
      "          '}\\n'\n",
      "          '\\n'\n",
      "          '# Create a DecisionTreeClassifier\\n'\n",
      "          'clf = DecisionTreeClassifier(random_state=42)\\n'\n",
      "          '\\n'\n",
      "          '# Create a GridSearchCV instance with cross-validation value of 5\\n'\n",
      "          'grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, '\n",
      "          \"cv=5, scoring='accuracy')\\n\"\n",
      "          '\\n'\n",
      "          '# Fit the GridSearchCV object to the data\\n'\n",
      "          'grid_search.fit(X, y)\\n'\n",
      "          '\\n'\n",
      "          '# Retrieve the best hyperparameters\\n'\n",
      "          \"best_max_depth = grid_search.best_params_['max_depth']\\n\"\n",
      "          'best_min_samples_split = '\n",
      "          \"grid_search.best_params_['min_samples_split']\\n\"\n",
      "          'best_accuracy = grid_search.best_score_\\n'\n",
      "          '\\n'\n",
      "          'print(f\"Best Hyperparameters: Max Depth = {best_max_depth}, Min '\n",
      "          'Samples Split = {best_min_samples_split}\")\\n'\n",
      "          'print(f\"Best Cross-Validation Accuracy: {best_accuracy:.2f}\")\\n'\n",
      "          'In this code:We load the dataset into a Pandas DataFrame and split '\n",
      "          'it into features (X) and the target variable (y).We define the '\n",
      "          'parameter grid (param_grid) with the hyperparameters you want to '\n",
      "          \"tune: 'max_depth' and 'min_samples_split'. You can add more \"\n",
      "          'hyperparameters to the grid if needed.We create a '\n",
      "          'DecisionTreeClassifier (clf) with a fixed random state for '\n",
      "          'reproducibility.We create a GridSearchCV instance (grid_search) '\n",
      "          'with the DecisionTreeClassifier, the parameter grid, 5-fold '\n",
      "          'cross-validation (cv=5), and scoring based on accuracy '\n",
      "          \"(scoring='accuracy').We fit the GridSearchCV object to the data, \"\n",
      "          'which performs a search over the specified hyperparameters and '\n",
      "          'cross-validates to find the best combination.We retrieve and print '\n",
      "          'the best hyperparameters (best_max_depth and '\n",
      "          'best_min_samples_split) and the best cross-validation accuracy '\n",
      "          '(best_accuracy).By running this code, you will find the best '\n",
      "          'hyperparameters for your DecisionTreeClassifier and their '\n",
      "          'corresponding cross-validation accuracy score. These '\n",
      "          'hyperparameters can be used to build a more optimized model for '\n",
      "          'your classification task.'}]\n"
     ]
    }
   ],
   "source": [
    "# let's see one of the conversations\n",
    "pprint(code2convos[\"0031c86e-81f4-4eef-9e0e-28037abf9883\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a text as an input and divides it into code,text,commentCode\n",
    "#by following a similar pattern used in ChatGPT responses that include code example.\n",
    "\n",
    "def distinguisher(response):\n",
    "    index=response.find(\":pythonCopy code\")\n",
    "    index2=response.rfind(\"\\n\")\n",
    "    \n",
    "    if(index!=-1):\n",
    "        text_part = response[:index]+ response[index2:]\n",
    "\n",
    "        code_part = response[index + len(\":pythonCopy code\"):index2].strip()\n",
    "        # Split the code into statements using '\\n' as the delimiter\n",
    "        code_part= code_part.split('\\n')\n",
    "\n",
    "        # Group statements based on whether they start with '#'\n",
    "        comment_code = [s for s in code_part if s.startswith('#') and s!=\"\"]\n",
    "        actual_code = [s for s in code_part if not s.startswith('#')and s!=\"\"]\n",
    "        \n",
    "        return{\"text\":text_part,\"code\":\" \".join(actual_code), \"commentCode\": \" \".join(comment_code) }\n",
    "        \n",
    "    else:\n",
    "        #that means the response does not include a code\n",
    "\n",
    "        text_part = response\n",
    "\n",
    "        code_part = \"\"\n",
    "        \n",
    "    return {\"text\": text_part, \"code\": \"\", \"commentCode\":\"\"}\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the dictionary formatted in the same way as the dictionary provided to us initially\n",
    "#and categorizes the output of ChatGPT as text, code ,commentCode\n",
    "\n",
    "def divideTextAndCode(code2convos):\n",
    "    for key, dictionaries in code2convos.items():\n",
    "        \n",
    "        # Iterate over the list of dictionaries for each key\n",
    "        for dictionary in dictionaries:\n",
    "            # Check if the role is \"assistant\"\n",
    "            if dictionary[\"role\"] == \"assistant\":\n",
    "                # Apply the distinguisher function to the \"text\" field\n",
    "                distinguished_parts = distinguisher(dictionary[\"text\"])\n",
    "                # Update the message dictionary with the modified \"text\" and \"code\" fields\n",
    "                dictionary[\"text\"]=distinguished_parts[\"text\"]\n",
    "                dictionary[\"code\"]=distinguished_parts[\"code\"]\n",
    "                dictionary[\"commentCode\"]=distinguished_parts[\"commentCode\"]\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "divideTextAndCode(code2convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'text': 'Read the .csv file with the pandas library can you do that'},\n",
       " {'role': 'assistant',\n",
       "  'text': 'Certainly! To read a CSV file using the pandas library in Python, you can use the read_csv function. Make sure you have the pandas library installed before running the code. You can install it using:bashCopy codepip install pandas\\nNow, assuming you have a CSV file named \"your_file.csv\", here\\'s an example code to read it using pandas\\nThis code assumes that the CSV file is in the same directory as your Python script or Jupyter Notebook. If the file is in a different directory, you should provide the full path or a relative path from your current working directory.You can customize the read_csv function with various parameters based on your specific CSV file format. For example, you can specify the delimiter, header presence, column names, etc. Refer to the pandas documentation for more details on the available options.',\n",
       "  'code': \"import pandas as pd file_path = 'your_file.csv' df = pd.read_csv(file_path) print(df)\",\n",
       "  'commentCode': \"# Replace 'your_file.csv' with the actual path to your CSV file # Read the CSV file into a pandas DataFrame # Now, 'df' is a DataFrame containing the data from the CSV file\"},\n",
       " {'role': 'user',\n",
       "  'text': 'Understanding the Dataset: (5 pts)\\n\\nFind the shape of the dataset (number of samples & number of attributes). (Hint: You can use the shape function)\\nDisplay variable names (both dependent and independent).\\nDisplay the summary of the dataset. (Hint: You can use the info function)\\nDisplay the first 5 rows from training dataset. (Hint: You can use the head function) can you code it'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Certainly! Assuming you've already read your dataset into a pandas DataFrame named df, here's how you can perform the tasks you've mentioned\\nMake sure to replace df with the actual name of your DataFrame if it's different. This code assumes that you have a typical dataset with both independent and dependent variables. The head() function is used to display the first 5 rows of the dataset, and info() provides a concise summary of the DataFrame, including information about the data types and missing values.\",\n",
       "  'code': 'print(\"Shape of the dataset:\", df.shape) print(\"\\\\nVariable names:\") print(\"Independent variables (features):\", df.columns.tolist()) print(\"\\\\nSummary of the dataset:\") df.info() print(\"\\\\nFirst 5 rows from the training dataset:\") print(df.head())',\n",
       "  'commentCode': '# Find the shape of the dataset (number of samples & number of attributes) # Display variable names (both dependent and independent) # Display the summary of the dataset # Display the first 5 rows from the training dataset'},\n",
       " {'role': 'user',\n",
       "  'text': 'Check if there are any missing values in the dataset. If there are, you can either drop these values or fill it with most common values in corresponding rows. Be careful that you have enough data for training the model.\\nEncode categorical labels with the mappings given in the cell below. (Hint: You can use map function) I have to this part and the given code is as following sex_map = {\\'female\\':1, \\'male\\': 0}\\n\\nisland_map = {\\'Biscoe\\': 1,\\n              \\'Dream\\': 2,\\n              \\'Torgensen\\': 3}\\n\\ndiet_map = {\\'fish\\': 1,\\n              \\'krill\\': 2,\\n              \\'squid\\': 3,\\n            \"parental\":4}\\n\\nlife_stage_map = {\\'chick\\': 1,\\n              \\'juvenile\\': 2,\\n              \\'adult\\': 3}\\n\\nhealth_metrics_map = {\\'healthy\\': 1,\\n              \\'overweight\\': 2,'},\n",
       " {'role': 'assistant',\n",
       "  'text': 'Certainly! Let\\'s start by addressing missing values and then proceed with encoding categorical labels using the provided mappings. Assuming your DataFrame is named df, here\\'s how you can perform these tasks\\nMake sure to uncomment either the \"Drop rows with missing values\" or \"Fill missing values with the most common value in each column\" section based on your preference. Choose the option that best fits your data and the amount of missing values in your dataset.',\n",
       "  'code': 'missing_values = df.isnull().sum() print(\"\\\\nMissing values in the dataset:\") print(missing_values) sex_map = {\\'female\\': 1, \\'male\\': 0} island_map = {\\'Biscoe\\': 1, \\'Dream\\': 2, \\'Torgensen\\': 3} diet_map = {\\'fish\\': 1, \\'krill\\': 2, \\'squid\\': 3, \\'parental\\': 4} life_stage_map = {\\'chick\\': 1, \\'juvenile\\': 2, \\'adult\\': 3} health_metrics_map = {\\'healthy\\': 1, \\'overweight\\': 2} df[\\'sex\\'] = df[\\'sex\\'].map(sex_map) df[\\'island\\'] = df[\\'island\\'].map(island_map) df[\\'diet\\'] = df[\\'diet\\'].map(diet_map) df[\\'life_stage\\'] = df[\\'life_stage\\'].map(life_stage_map) df[\\'health_metrics\\'] = df[\\'health_metrics\\'].map(health_metrics_map) print(\"\\\\nUpdated DataFrame after encoding categorical labels:\") print(df.head())',\n",
       "  'commentCode': '# Check for missing values in the dataset # Option 1: Drop rows with missing values # df = df.dropna() # Option 2: Fill missing values with the most common value in each column # df = df.apply(lambda x: x.fillna(x.value_counts().index[0])) # Note: Choose either option 1 or 2 based on your specific scenario and the amount of missing data. # Encoding categorical labels using the provided mappings # Apply the mapping to the respective columns # Display the updated DataFrame'},\n",
       " {'role': 'user',\n",
       "  'text': 'Encode categorical labels with the mappings given in the cell below. (Hint: You can use map function) Ä± have to do this part for species'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Certainly! If you have a column named 'species' in your DataFrame that you want to encode using a mapping, you can use the map function. Here's how you can do it using the provided mapping\\nReplace 'species' with the actual name of your column if it's different. The map function is used to replace values in the 'species' column with the corresponding values from the species_map. This will encode the categorical labels in the 'species' column according to the provided mapping.\",\n",
       "  'code': 'species_map = {\\'Adelie\\': 1, \\'Chinstrap\\': 2, \\'Gentoo\\': 3} df[\\'species\\'] = df[\\'species\\'].map(species_map) print(\"\\\\nUpdated DataFrame after encoding \\'species\\' column:\") print(df.head())',\n",
       "  'commentCode': \"# Given species mapping # Apply the mapping to the 'species' column # Display the updated DataFrame\"},\n",
       " {'role': 'user', 'text': 'how can Ä± label the year part'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"If your dataset includes a column with the year information, and you want to label it, you can create a new column for the labels based on the year values. Here's an example assuming you have a column named 'year'\\nIn this example, a new column named 'year_label' is created based on the 'year' column values using the provided mapping (year_labels). Adjust the column names and mapping according to your actual dataset.This will create a new column that represents the labeled version of the 'year' information based on the specified mapping.\",\n",
       "  'code': 'year_labels = {2019: \\'Year_2019\\', 2020: \\'Year_2020\\', 2021: \\'Year_2021\\', 2022: \\'Year_2022\\'} df[\\'year_label\\'] = df[\\'year\\'].map(year_labels) print(\"\\\\nUpdated DataFrame with labeled \\'year\\' column:\") print(df.head())',\n",
       "  'commentCode': \"# Assuming you have a column named 'year' in your DataFrame # If 'year' is not present, replace it with the actual name of your column # Create a mapping for the year labels # Apply the mapping to the 'year' column to create a new labeled column # Display the updated DataFrame\"},\n",
       " {'role': 'user', 'text': 'Is there any short way as labelencoder'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Yes, you can use the LabelEncoder from the sklearn.preprocessing module to encode the 'year' column or any other categorical column with numerical labels. Here's an example\\nIn this example, the LabelEncoder is used to transform the 'year' column into numerical labels, and the result is stored in a new column named 'year_label'. This is a shorter way compared to manually creating a mapping. Keep in mind that LabelEncoder assigns unique numerical labels to each unique value in the column.Remember that this approach assumes that the 'year' column contains categorical values that can be encoded numerically. If the 'year' column is already numerical, you might not need to use LabelEncoder.\",\n",
       "  'code': 'from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() df[\\'year_label\\'] = label_encoder.fit_transform(df[\\'year\\']) print(\"\\\\nUpdated DataFrame with labeled \\'year\\' column:\") print(df.head())',\n",
       "  'commentCode': \"# Assuming you have a column named 'year' in your DataFrame # If 'year' is not present, replace it with the actual name of your column # Create an instance of LabelEncoder # Fit and transform the 'year' column to numerical labels # Display the updated DataFrame\"},\n",
       " {'role': 'user', 'text': 'print(df.head(3)) '},\n",
       " {'role': 'assistant',\n",
       "  'text': 'Sure, if you want to print the first 3 rows of your DataFrame df, you can use the head function as follows\\nThis will display the first 3 rows of your DataFrame. Adjust the argument to head if you want to display a different number of rows.',\n",
       "  'code': 'print(df.head(3))',\n",
       "  'commentCode': ''},\n",
       " {'role': 'user',\n",
       "  'text': 'Empty DataFrame\\nColumns: [species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, diet, life_stage, health_metrics, year, year_label]\\nIndex: [] it tells me this why'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"The output you provided indicates that your DataFrame is currently empty. There are a few potential reasons for an empty DataFrame:The CSV file is empty: If the CSV file you read into the DataFrame (df) is empty or contains only headers without any data, you'll end up with an empty DataFrame.Incorrect file path: Ensure that the file path you provided to read the CSV file is correct. Double-check the path and the file name to make sure it points to the actual location of your dataset.Data loading issues: There might be issues with reading the data from the CSV file. Verify that your CSV file is formatted correctly and that it contains the expected data.Here's a generic example of reading a CSV file\\nIf the issue persists, you may want to investigate further by checking the contents of your CSV file or providing additional information about your dataset and how you are loading it into the DataFrame.\",\n",
       "  'code': \"import pandas as pd file_path = 'your_file.csv' df = pd.read_csv(file_path) print(df.head(3))\",\n",
       "  'commentCode': \"# Replace 'your_file.csv' with the actual path to your CSV file # Read the CSV file into a pandas DataFrame # Display the first 3 rows to check the content\"},\n",
       " {'role': 'user',\n",
       "  'text': 'Shuffle the dataset.\\nSeperate your dependent variable X, and your independent variable y. The column health_metrics is y, the rest is X.\\nSplit training and test sets as 80% and 20%, respectively. Can you do this part'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Certainly! To shuffle the dataset, separate the dependent and independent variables, and split the data into training and test sets, you can use the shuffle and train_test_split functions from sklearn.model_selection. Here's an example\\nIn this example:The dataset is shuffled using the sample method with frac=1 to shuffle the entire DataFrame.The independent variables (X) are all columns except 'health_metrics'.The dependent variable (y) is the 'health_metrics' column.The train_test_split function is used to split the data into training and test sets. The test_size parameter is set to 0.2, indicating an 80%/20% split.The shapes of the resulting training and test sets are then displayed.Adjust the column names as needed based on your actual dataset structure.\",\n",
       "  'code': 'from sklearn.model_selection import train_test_split df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True) X = df_shuffled.drop(columns=[\\'health_metrics\\']) y = df_shuffled[\\'health_metrics\\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) print(\"Shape of X_train:\", X_train.shape) print(\"Shape of X_test:\", X_test.shape) print(\"Shape of y_train:\", y_train.shape) print(\"Shape of y_test:\", y_test.shape)',\n",
       "  'commentCode': '# Shuffle the dataset # Separate independent variables (X) and dependent variable (y) # Split the dataset into training and test sets (80% training, 20% test) # Display the shapes of the training and test sets'},\n",
       " {'role': 'user',\n",
       "  'text': 'df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True) is this for shuffling'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Yes, that's correct. The line df.sample(frac=1, random_state=42).reset_index(drop=True) is used for shuffling the DataFrame df. Let me break it down:df.sample(frac=1, random_state=42): This randomly samples the entire DataFrame (frac=1 means it takes the entire DataFrame) with a random seed (random_state=42 ensures reproducibility)..reset_index(drop=True): After shuffling, the indices are reset, and drop=True means the old index column is dropped.So, after this line is executed, df_shuffled contains a shuffled version of the original DataFrame df. Each row is randomly ordered, and the indices are reset.\",\n",
       "  'code': '',\n",
       "  'commentCode': ''},\n",
       " {'role': 'user', 'text': \"don't we have to use shuffle command\"},\n",
       " {'role': 'assistant',\n",
       "  'text': \"You are correct, and I appreciate the clarification. The df.sample(frac=1, random_state=42).reset_index(drop=True) is indeed a way to shuffle the DataFrame, and it's equivalent to shuffling using the shuffle method from sklearn.utils. Both approaches achieve the same result.If you prefer to use shuffle from sklearn.utils, you can do it like this\\nThis accomplishes the same shuffling effect. Both methods are commonly used, and you can choose the one that you find more readable or convenient. Thank you for bringing this up!\",\n",
       "  'code': 'from sklearn.utils import shuffle df_shuffled = shuffle(df, random_state=42).reset_index(drop=True)',\n",
       "  'commentCode': '# Shuffle the dataset using sklearn.utils.shuffle'},\n",
       " {'role': 'user', 'text': 'reset_index(drop=True) what is this'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"The reset_index(drop=True) method is used to reset the index of a DataFrame and, if drop=True, discard the old index. Let me explain this in more detail:reset_index(): By default, this method resets the index of the DataFrame. When you shuffle or manipulate a DataFrame, the index might get out of order or contain gaps. The reset_index() method is used to reset the DataFrame's index to the default integer-based one and creates a new DataFrame.drop=True: This parameter is optional, and when set to True, it discards the old index column. Without drop=True, the old index becomes a new column in the DataFrame.Here's an example to illustrate\\nIn the above example, reset_index(drop=True) is used to reset the index and drop the old index column. This is often used after operations like shuffling, where the original order of rows may not be necessary.\",\n",
       "  'code': 'import pandas as pd data = {\\'A\\': [1, 2, 3], \\'B\\': [4, 5, 6]} df = pd.DataFrame(data) print(\"Original DataFrame:\") print(df) df_reset = df.reset_index(drop=True) print(\"\\\\nDataFrame after resetting the index:\") print(df_reset)',\n",
       "  'commentCode': '# Create a simple DataFrame for demonstration # Display the original DataFrame # Reset the index, dropping the old index column # Display the DataFrame after resetting the index'},\n",
       " {'role': 'user',\n",
       "  'text': \"Correlations of features with health (4 points) Calculate the correlations for all features in dataset. Highlight any strong correlations with the target variable. Plot your results in a heatmap.\\n\\nFeature Selection (3 points) Select a subset of features that are likely strong predictors, justifying your choices based on the computed correlations.\\n\\nHypothetical Driver Features (3 points) Propose two hypothetical features that could enhance the model's predictive accuracy for Y, explaining how they might be derived and their expected impact. Show the resulting correlations with target variable. This is the task that you have to code\"},\n",
       " {'role': 'assistant',\n",
       "  'text': 'Certainly! Let\\'s start by calculating the correlations for all features in the dataset, highlighting strong correlations with the target variable (\\'health_metrics\\'), and then proceed with feature selection and proposing two hypothetical features. I\\'ll use the corr method to calculate correlations and seaborn for plotting a heatmap.pythonCopy codeimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Calculate correlations for all features\\ncorrelations = df.corr()\\n\\n# Highlight strong correlations with the target variable (\\'health_metrics\\')\\ntarget_correlations = correlations[\\'health_metrics\\'].sort_values(ascending=False)\\nstrong_correlations = target_correlations[abs(target_correlations) > 0.5]\\n\\n# Plot correlations in a heatmap\\nplt.figure(figsize=(12, 8))\\nsns.heatmap(correlations, annot=True, cmap=\\'coolwarm\\', fmt=\".2f\")\\nplt.title(\\'Correlation Heatmap\\')\\nplt.show()\\n\\n# Display features with strong correlations with the target variable\\nprint(\"\\\\nFeatures with strong correlations with \\'health_metrics\\':\")\\nprint(strong_correlations)\\n\\n# Feature Selection (you can customize this based on your dataset and business context)\\nselected_features = [\\'flipper_length_mm\\', \\'body_mass_g\\', \\'diet\\', \\'sex\\', \\'island\\']\\n\\n# Display the selected features\\nprint(\"\\\\nSelected features for prediction:\")\\nprint(selected_features)\\n\\n# Hypothetical Driver Features\\n# Let\\'s propose two hypothetical features and calculate their correlations with the target variable\\n# You can replace these with meaningful hypothetical features based on your domain knowledge\\n\\n# Hypothetical Feature 1: \\'temperature\\'\\n# Deriving a hypothetical temperature feature\\ndf[\\'temperature\\'] = df[\\'year\\'] * 10  # Just an example, replace with a meaningful calculation\\ncorr_temp = df[\\'temperature\\'].corr(df[\\'health_metrics\\'])\\nprint(\"\\\\nCorrelation between \\'temperature\\' and \\'health_metrics\\':\", corr_temp)\\n\\n# Hypothetical Feature 2: \\'ocean_current_speed\\'\\n# Deriving a hypothetical ocean current speed feature\\ndf[\\'ocean_current_speed\\'] = df[\\'flipper_length_mm\\'] / df[\\'body_mass_g\\']  # Just an example, replace with a meaningful calculation\\ncorr_current_speed = df[\\'ocean_current_speed\\'].corr(df[\\'health_metrics\\'])\\nprint(\"Correlation between \\'ocean_current_speed\\' and \\'health_metrics\\':\", corr_current_speed)\\nIn this code:The correlations are calculated using the corr method.A heatmap is created using seaborn to visualize the correlations.Features with strong correlations with the target variable are displayed.Feature selection is done manually based on correlation values and business context.Two hypothetical features (\\'temperature\\' and \\'ocean_current_speed\\') are proposed, and their correlations with the target variable are calculated.Please customize the feature selection and hypothetical features based on your dataset and domain knowledge.',\n",
       "  'code': '',\n",
       "  'commentCode': ''},\n",
       " {'role': 'user',\n",
       "  'text': 'orrelations of features with health (4 points) Calculate the correlations for all features in dataset. Highlight any strong correlations with the target variable. Plot your results in a heatmap. Just this part'},\n",
       " {'role': 'assistant',\n",
       "  'text': 'Certainly! Here\\'s the code to calculate the correlations for all features in the dataset, highlight strong correlations with the target variable (\\'health_metrics\\'), and plot the results in a heatmap\\nIn this code:The correlations are calculated using the corr method.Strong correlations with the target variable (\\'health_metrics\\') are highlighted.A heatmap is created using seaborn to visualize the correlations.You can adjust the threshold value (0.5 in this case) based on your definition of a \"strong correlation.\" Feel free to customize the code to better suit your specific requirements and preferences.',\n",
       "  'code': 'import seaborn as sns import matplotlib.pyplot as plt correlations = df.corr() target_correlations = correlations[\\'health_metrics\\'].sort_values(ascending=False) strong_correlations = target_correlations[abs(target_correlations) > 0.5] plt.figure(figsize=(12, 8)) sns.heatmap(correlations[[\\'health_metrics\\']], annot=True, cmap=\\'coolwarm\\', fmt=\".2f\") plt.title(\\'Correlation Heatmap with Target Variable (health_metrics)\\') plt.show() print(\"\\\\nFeatures with strong correlations with \\'health_metrics\\':\") print(strong_correlations)',\n",
       "  'commentCode': \"# Calculate correlations for all features # Highlight strong correlations with the target variable ('health_metrics') # Plot correlations in a heatmap # Display features with strong correlations with the target variable\"},\n",
       " {'role': 'user',\n",
       "  'text': '# code here\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Calculate correlations for all features\\ncorrelations = df.corr()\\n\\n# Highlight strong correlations with the target variable (\\'health_metrics\\')\\ntarget_correlations = correlations[\\'health_metrics\\'].sort_values(ascending=False)\\nprint(target_correlations)\\n\\n\\n# Plot correlations in a heatmap\\nplt.figure(figsize=(12, 8))\\nsns.heatmap(correlations[[\\'health_metrics\\']], annot=True, cmap=\\'coolwarm\\', fmt=\".2f\")\\nplt.title(\\'Correlation Heatmap with Target Variable (health_metrics)\\')\\nplt.show()'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Your code looks correct and should calculate the correlations for all features in the dataset, print the correlations with the target variable ('health_metrics'), and then plot the results in a heatmap. Just ensure that your DataFrame (df) is correctly loaded with the data.Here's a brief explanation of your code\\n\",\n",
       "  'code': 'import seaborn as sns import matplotlib.pyplot as plt correlations = df.corr()',\n",
       "  'commentCode': \"# Calculate correlations for all features # Print correlations with the target variable ('health\"},\n",
       " {'role': 'user',\n",
       "  'text': 'species             NaN\\nisland              NaN\\nbill_length_mm      NaN\\nbill_depth_mm       NaN\\nflipper_length_mm   NaN\\nbody_mass_g         NaN\\nsex                 NaN\\ndiet                NaN\\nlife_stage          NaN\\nhealth_metrics      NaN\\nyear                NaN\\nyear_label          NaN\\nName: health_metrics, dtype: float64\\n/usr/local/lib/python3.10/dist-packages/seaborn/matrix.py:202: RuntimeWarning: All-NaN slice encountered\\n  vmin = np.nanmin(calc_data)\\n/usr/local/lib/python3.10/dist-packages/seaborn/matrix.py:207: RuntimeWarning: All-NaN slice encountered\\n  vmax = np.nanmax(calc_data) WHY I SEE NAN'},\n",
       " {'role': 'assistant',\n",
       "  'text': 'If you\\'re seeing NaN (Not a Number) values in the correlation results, it may indicate that there are missing or non-numeric values in your dataset. The NaN values are likely affecting the calculation of correlations. Here are a few things to check and address:Check for Missing Values:Use df.isnull().sum() to check for missing values in your DataFrame. If there are missing values, you might want to handle them by either filling them or dropping the corresponding rows.pythonCopy code# Check for missing values\\nmissing_values = df.isnull().sum()\\nprint(\"Missing values in the dataset:\")\\nprint(missing_values)\\nEnsure Numeric Data:Ensure that all columns in your DataFrame are numeric. If there are non-numeric columns, they might be causing issues with the correlation calculation.pythonCopy code# Display data types of each column\\nprint(\"Data types of each column:\")\\nprint(df.dtypes)\\nHandle Categorical Data:If you have categorical columns (e.g., \\'species\\', \\'sex\\', \\'diet\\', \\'life_stage\\', etc.), you might want to encode them numerically before calculating correlations.pythonCopy code# Example: Encoding categorical columns\\nfrom sklearn.preprocessing import LabelEncoder\\n\\nlabel_encoder = LabelEncoder()\\ndf[\\'species\\'] = label_encoder.fit_transform(df[\\'species\\'])\\ndf[\\'sex\\'] = label_encoder.fit_transform(df[\\'sex\\'])\\n# ... repeat for other categorical columns ...\\nMake sure to handle missing values and encode categorical variables appropriately, and then rerun the code to calculate correlations. If the issue persists, please provide more details about your dataset and any preprocessing steps you have applied.',\n",
       "  'code': '',\n",
       "  'commentCode': ''},\n",
       " {'role': 'user',\n",
       "  'text': \"A value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_indexer,col_indexer] = value instead\\n\\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\\n  df['sex'] = df['sex'].map(sex_map)\\n<ipython-input-101-687cc92bb4e3>:30: SettingWithCopyWarning: \\nA value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_indexer,col_indexer] = value instead\\n\\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\\n  df['island'] = df['island'].map(island_map)\\n<ipython-input-101-687cc92bb4e3>:31: SettingWithCopyWarning: \\nA value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_indexer,col_indexer] = value instead\\n\\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\\n  df['diet'] = df['diet'].map(diet_map)\\n<ipython-input-101-687cc92bb4e3>:32: SettingWithCopyWarning: \\nA value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_indexer,col_indexer] = value instead\\n\\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\\n  df['life_stage'] = df['life_stage'].map(life_stage_map)\\n<ipython-input-101-687cc92bb4e3>:33: SettingWithCopyWarning: \\nA value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_indexer,col_indexer] = value instead\\n\\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\\n  df['health_metrics'] = df['health_metrics'].map(health_metrics_map)\\n<ipython-input-101-687cc92bb4e3>:34: SettingWithCopyWarning: \\nA value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_indexer,col_indexer] = value instead\\n\\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\\n  df['species'] = df['species'].map(species_map)\\n<ipython-input-101-687cc92bb4e3>:38: SettingWithCopyWarning: \\nA value is trying to be set on a copy of a slice from a DataFrame.\\nTry using .loc[row_indexer,col_indexer] = value instead\\n\\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\\n  df['year'] = label_encoder.fit_transform(df['year']) Bu ne demek istiyor\"},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Bu uyarÄ± (warning) genellikle bir DataFrame Ã¼zerinde yapÄ±lan iÅ\\x9flemler sÄ±rasÄ±nda, orijinal DataFrame'in bir kopyasÄ± Ã¼zerinde deÄ\\x9fiÅ\\x9fiklik yapma durumunda ortaya Ã§Ä±kar. Bu durum, pandas'Ä±n bellek tasarrufu ve performans nedeniyle bazÄ± iÅ\\x9flemlerde kopyalama yerine bir gÃ¶rÃ¼nÃ¼m (view) oluÅ\\x9fturmasÄ±yla ilgilidir.UyarÄ±, genellikle iÅ\\x9flemleri daha gÃ¼venli ve aÃ§Ä±k bir Å\\x9fekilde yapmak iÃ§in df.loc[] kullanmanÄ±n Ã¶nerildiÄ\\x9fi durumlarda ortaya Ã§Ä±kar. loc kullanmak, DataFrame Ã¼zerinde gerÃ§ekten bir atama iÅ\\x9flemi gerÃ§ekleÅ\\x9ftirdiÄ\\x9fi iÃ§in bu tÃ¼r uyarÄ±larÄ± Ã¶nleyebilir.Ã\\x96rneÄ\\x9fin, Å\\x9fu kod\\nBenzer deÄ\\x9fiÅ\\x9fiklikleri diÄ\\x9fer satÄ±rlar iÃ§in de yapabilirsiniz.Ancak, bu uyarÄ±lar sadece bir bilgilendirme amaÃ§lÄ±dÄ±r ve Ã§oÄ\\x9fu durumda iÅ\\x9flemlerinizi etkilemez. EÄ\\x9fer kodunuz beklediÄ\\x9finiz gibi Ã§alÄ±Å\\x9fÄ±yorsa ve uyarÄ±lar sizi rahatsÄ±z etmiyorsa, iÅ\\x9flemlerinizi devam ettirebilirsiniz. Ancak, uyarÄ±larÄ± Ã¶nlemek ve daha aÃ§Ä±k bir kod yazmak iÃ§in loc kullanÄ±mÄ±nÄ± tercih edebilirsiniz.\",\n",
       "  'code': \"df['sex'] = df['sex'].map(sex_map) yerine Å\\x9fu Å\\x9fekilde yazÄ±labilir:pythonCopy codedf.loc[:, 'sex'] = df['sex'].map(sex_map)\",\n",
       "  'commentCode': ''},\n",
       " {'role': 'user',\n",
       "  'text': \"Hypothetical Driver Features (3 points) Propose two hypothetical features that could enhance the model's predictive accuracy for Y, explaining how they might be derived and their expected impact. Show the resulting correlations with target variable. Can you do this part\"},\n",
       " {'role': 'assistant',\n",
       "  'text': 'Certainly! Let\\'s propose two hypothetical features and calculate their correlations with the target variable (\\'health_metrics\\'). For demonstration purposes, I\\'ll create two simple hypothetical features: \\'temperature\\' and \\'interaction_feature\\'. Please replace these with meaningful features based on your domain knowledge.pythonCopy code# Hypothetical Feature 1: \\'temperature\\'\\n# Deriving a hypothetical temperature feature\\ndf[\\'temperature\\'] = df[\\'year\\'] * 10  # Just an example, replace with a meaningful calculation\\ncorr_temp = df[\\'temperature\\'].corr(df[\\'health_metrics\\'])\\nprint(\"Correlation between \\'temperature\\' and \\'health_metrics\\':\", corr_temp)\\n\\n# Hypothetical Feature 2: \\'interaction_feature\\'\\n# Creating a hypothetical interaction feature\\ndf[\\'interaction_feature\\'] = df[\\'flipper_length_mm\\'] * df[\\'body_mass_g\\']  # Just an example, replace with a meaningful calculation\\ncorr_interaction = df[\\'interaction_feature\\'].corr(df[\\'health_metrics\\'])\\nprint(\"Correlation between \\'interaction_feature\\' and \\'health_metrics\\':\", corr_interaction)\\nIn this example:\\'temperature\\' is a hypothetical feature derived from the \\'year\\' column. The idea here is that the health of penguins might be influenced by the temperature, and \\'year\\' could be a proxy for changing temperature conditions. Replace the calculation with a meaningful one based on your knowledge.\\'interaction_feature\\' is a hypothetical feature created by multiplying \\'flipper_length_mm\\' and \\'body_mass_g\\'. The idea here is that there might be an interaction effect between these two physical characteristics of penguins that could impact health.Note: These are just examples, and you should replace them with features that make sense in the context of your data and domain knowledge. Always ensure that your features have a logical and meaningful interpretation in the context of your problem.',\n",
       "  'code': '',\n",
       "  'commentCode': ''},\n",
       " {'role': 'user',\n",
       "  'text': 'Choose 2 hyperparameters to tune. You can use the Scikit learn decision tree documentation for the available hyperparameters (Hyperparameters are listed under \"Parameters\" in the documentation). Use GridSearchCV for hyperparameter tuning, with a cross-validation value of 5. Use validation accuracy to pick the best hyper-parameter values. (15 pts)  Can you pick max_depth  and max_samples_split and code it '},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Certainly! For a decision tree classifier in scikit-learn, two commonly tuned hyperparameters are max_depth and min_samples_split. Here's an example of how you can use GridSearchCV to tune these hyperparameters\\nIn this example:max_depth is the maximum depth of the tree, and min_samples_split is the minimum number of samples required to split an internal node.The param_grid dictionary defines the hyperparameters and their possible values.GridSearchCV performs a search over the specified hyperparameter values using 5-fold cross-validation and selects the best combination based on accuracy.The best hyperparameter values are printed, and the best model is then evaluated on the test set to get the validation accuracy.Adjust the parameter grid, scoring metric, and other settings based on your specific requirements and the nature of your dataset.\",\n",
       "  'code': 'from sklearn.model_selection import GridSearchCV from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split dt_classifier = DecisionTreeClassifier() param_grid = {     \\'max_depth\\': [3, 5, 7, None],  # None means no maximum depth     \\'min_samples_split\\': [2, 5, 10] } grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring=\\'accuracy\\') grid_search.fit(X_train, y_train) print(\"Best Hyperparameters:\", grid_search.best_params_) best_dt_model = grid_search.best_estimator_ test_accuracy = best_dt_model.score(X_test, y_test) print(\"Validation Accuracy with Best Hyperparameters:\", test_accuracy)',\n",
       "  'commentCode': '# Assuming you have your features (X) and target variable (y) defined # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the decision tree classifier # Specify the hyperparameters and their possible values # Create the GridSearchCV object # Fit the model to the data to find the best hyperparameters # Print the best hyperparameter values # Get the best model # Evaluate the model on the test set'},\n",
       " {'role': 'user',\n",
       "  'text': 'Re-train model with the hyperparameters you have chosen in part 5). (10 pts)\\nPlot the tree you have trained. (5 pts)\\nHint: You can import the plot_tree function from the sklearn library. Whe have chosen max depth and min sample split can you code this part'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Certainly! After tuning the hyperparameters using GridSearchCV, you can re-train the model with the best hyperparameters and plot the decision tree. Here's how you can do that\\nIn this code:We retrieve the best hyperparameters from the GridSearchCV results.We create a new DecisionTreeClassifier instance with the best hyperparameters.We fit the model to the training data using these hyperparameters.We use plot_tree to visualize the decision tree.Make sure to replace X_train and X_test with your actual training and testing data. The plot_tree function will help you visualize the trained decision tree with the specified hyperparameters.\",\n",
       "  'code': 'from sklearn.tree import DecisionTreeClassifier, plot_tree import matplotlib.pyplot as plt best_max_depth = grid_search.best_params_[\\'max_depth\\'] best_min_samples_split = grid_search.best_params_[\\'min_samples_split\\'] best_dt_model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_split=best_min_samples_split) best_dt_model.fit(X_train, y_train) plt.figure(figsize=(15, 10)) plot_tree(best_dt_model, feature_names=X_train.columns, class_names=best_dt_model.classes_, filled=True, rounded=True) plt.title(\"Decision Tree with Best Hyperparameters\") plt.show()',\n",
       "  'commentCode': '# Assuming you have X_train, X_test, y_train, y_test from previous code # Re-train the model with the best hyperparameters # Create the decision tree classifier with the best hyperparameters # Fit the model to the training data # Plot the decision tree'},\n",
       " {'role': 'user',\n",
       "  'text': 'Re-train model with the hyperparameters you have chosen in part 5). (10 pts) can you code this part and then Plot the tree you have trained. (5 pts) this part seperatly'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Certainly! Let's first re-train the model with the best hyperparameters and then plot the trained decision tree separately:Re-train model with the best hyperparameters\\nIn the first part, we re-train the model using the best hyperparameters obtained from the grid search. In the second part, we use plot_tree to visualize the trained decision tree. Adjust the code based on your actual dataset and variable names.\",\n",
       "  'code': 'from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score best_max_depth = grid_search.best_params_[\\'max_depth\\'] best_min_samples_split = grid_search.best_params_[\\'min_samples_split\\'] best_dt_model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_split=best_min_samples_split) best_dt_model.fit(X_train, y_train) y_pred = best_dt_model.predict(X_test) test_accuracy = accuracy_score(y_test, y_pred) print(\"Test Accuracy with Best Hyperparameters:\", test_accuracy) Plot the trained decision tree:pythonCopy codefrom sklearn.tree import plot_tree import matplotlib.pyplot as plt plt.figure(figsize=(15, 10)) plot_tree(best_dt_model, feature_names=X_train.columns, class_names=best_dt_model.classes_, filled=True, rounded=True) plt.title(\"Decision Tree with Best Hyperparameters\") plt.show()',\n",
       "  'commentCode': '# Assuming you have X_train, X_test, y_train, y_test from previous code # Re-train the model with the best hyperparameters # Create the decision tree classifier with the best hyperparameters # Fit the model to the training data # Make predictions on the test set # Evaluate the model accuracy on the test set # Plot the decision tree'},\n",
       " {'role': 'user',\n",
       "  'text': '---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-120-c168748c2af7> in <cell line: 8>()\\n      6 # Plot the decision tree\\n      7 plt.figure(figsize=(15, 10))\\n----> 8 plot_tree(best_dt_model, feature_names=X_train.columns, class_names=best_dt_model.classes_, filled=True, rounded=True)\\n      9 plt.title(\"Decision Tree with Best Hyperparameters\")\\n     10 plt.show()\\n\\n3 frames\\n/usr/local/lib/python3.10/dist-packages/sklearn/tree/_export.py in node_to_str(self, tree, node_id, criterion)\\n    371                     characters[2],\\n    372                 )\\n--> 373             node_string += class_name\\n    374 \\n    375         # Clean up any trailing newlines\\n\\nTypeError: can only concatenate str (not \"numpy.int64\") to str'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"It seems like there is an issue with concatenating a NumPy integer (numpy.int64) to a string. This issue is likely related to the class_names argument when using the plot_tree function. The class_names argument expects an iterable of strings representing the class names, but it appears to be receiving an integer.To resolve this issue, we need to convert the class names to strings. If your classes are represented by integers, you can convert them to strings before passing them to the plot_tree function.Here's an updated version of the code\\nIn this updated code, I used list(map(str, best_dt_model.classes_)) to convert the class names to strings before passing them to the plot_tree function. This should resolve the TypeError you encountered. Adjust the code based on your specific dataset and class representation.\",\n",
       "  'code': 'from sklearn.tree import DecisionTreeClassifier, plot_tree import matplotlib.pyplot as plt best_max_depth = grid_search.best_params_[\\'max_depth\\'] best_min_samples_split = grid_search.best_params_[\\'min_samples_split\\'] best_dt_model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_split=best_min_samples_split) best_dt_model.fit(X_train, y_train) plt.figure(figsize=(15, 10)) plot_tree(best_dt_model, feature_names=X_train.columns, class_names=list(map(str, best_dt_model.classes_)), filled=True, rounded=True) plt.title(\"Decision Tree with Best Hyperparameters\") plt.show()',\n",
       "  'commentCode': '# Assuming you have X_train, X_test, y_train, y_test from previous code # Re-train the model with the best hyperparameters # Create the decision tree classifier with the best hyperparameters # Fit the model to the training data # Plot the decision tree'},\n",
       " {'role': 'user',\n",
       "  'text': 'from sklearn.tree import plot_tree\\n\\n#code here\\n\\n\\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\\nimport matplotlib.pyplot as plt\\n\\n# Assuming you have X_train, X_test, y_train, y_test from previous code\\n\\n# Re-train the model with the best hyperparameters\\nbest_max_depth = grid_search.best_params_[\\'max_depth\\']\\nbest_min_samples_split = grid_search.best_params_[\\'min_samples_split\\']\\n\\n# Create the decision tree classifier with the best hyperparameters\\nbest_dt_model_2 = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_split=best_min_samples_split)\\n\\n# Fit the model to the training data\\nbest_dt_model_2.fit(X_train, y_train)\\n\\n# Plot the decision tree\\nplt.figure(figsize=(15, 10))\\nplot_tree(best_dt_model_2, feature_names=X_train.columns, class_names=list(map(str, best_dt_model.classes_)), filled=True, rounded=True)\\nplt.title(\"Decision Tree with Best Hyperparameters\")\\nplt.show()\\n Ä± have uptaded like this we will use that so keep in mind'},\n",
       " {'role': 'assistant',\n",
       "  'text': 'Great! It looks like you\\'ve updated the code correctly. The use of best_dt_model_2 is now consistent throughout the code, and you\\'ve converted the class names to strings before passing them to plot_tree. This should resolve the TypeError.pythonCopy codefrom sklearn.tree import DecisionTreeClassifier, plot_tree\\nimport matplotlib.pyplot as plt\\n\\n# Assuming you have X_train, X_test, y_train, y_test from previous code\\n\\n# Re-train the model with the best hyperparameters\\nbest_max_depth = grid_search.best_params_[\\'max_depth\\']\\nbest_min_samples_split = grid_search.best_params_[\\'min_samples_split\\']\\n\\n# Create the decision tree classifier with the best hyperparameters\\nbest_dt_model_2 = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_split=best_min_samples_split)\\n\\n# Fit the model to the training data\\nbest_dt_model_2.fit(X_train, y_train)\\n\\n# Plot the decision tree\\nplt.figure(figsize=(15, 10))\\nplot_tree(best_dt_model_2, feature_names=X_train.columns, class_names=list(map(str, best_dt_model_2.classes_)), filled=True, rounded=True)\\nplt.title(\"Decision Tree with Best Hyperparameters\")\\nplt.show()\\nThis code should now correctly visualize the decision tree with the best hyperparameters. If you encounter any further issues or have additional questions, feel free to ask!',\n",
       "  'code': '',\n",
       "  'commentCode': ''},\n",
       " {'role': 'user',\n",
       "  'text': 'Predict the labels of testing data using the tree you have trained in step 6  (This is the code that Ä± have just provided). (10 pts)\\nReport the classification accuracy. (2 pts)\\nPlot & investigate the confusion matrix. Fill the following blanks. (8 pts)\\nThe model most frequently mistakes class(es) _____ for class(es) _____.\\n\\nHint: You can use the confusion_matrix function from sklearn.metrics What do Ä± heve to do on this task '},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Certainly! Here's the code for predicting labels on the testing data, reporting the classification accuracy, and plotting the confusion matrix\\nIn this code:We use the trained decision tree model (best_dt_model_2) to predict labels on the testing data (X_test).The classification accuracy is then reported using the accuracy_score function.We calculate the confusion matrix using the confusion_matrix function.The confusion matrix is then plotted using seaborn to visually inspect the model's performance.To fill in the blanks regarding the most frequently mistaken classes, you can examine the confusion matrix. Look for entries where the actual class is one value, but the predicted class is another. The confusion matrix heatmap will help you identify which classes are often confused with each other.\",\n",
       "  'code': 'from sklearn.metrics import accuracy_score, confusion_matrix import seaborn as sns import matplotlib.pyplot as plt y_pred_test = best_dt_model_2.predict(X_test) test_accuracy = accuracy_score(y_test, y_pred_test) print(\"Test Accuracy:\", test_accuracy) conf_matrix = confusion_matrix(y_test, y_pred_test) plt.figure(figsize=(8, 6)) sns.heatmap(conf_matrix, annot=True, fmt=\\'d\\', cmap=\\'Blues\\', cbar=False) plt.title(\"Confusion Matrix\") plt.xlabel(\"Predicted Labels\") plt.ylabel(\"True Labels\") plt.show()',\n",
       "  'commentCode': '# Assuming you have X_test and y_test from previous code # Predict labels on the testing data # Report the classification accuracy # Plot the confusion matrix # Plotting the confusion matrix using seaborn'},\n",
       " {'role': 'user',\n",
       "  'text': 'Find the information gain on the first split with Entropy according to the formula INFORMATÄ°ON GAIN= ENTROPY(PARENT) - [AVERAGE ENTROPY(CHÄ°LDREN)]. Can you code it'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Certainly! To calculate information gain for the first split using entropy, you'll need to follow these steps:Calculate the entropy of the parent node.Calculate the weighted average entropy of the child nodes.Subtract the weighted average entropy from the entropy of the parent node.Here's an example code snippet to calculate information gain for the first split in a decision tree using entropy\\nPlease note that this example assumes a binary split. If your tree has more than two children at the first split, you'll need to adapt the code accordingly. The calculate_entropy function is used to compute the entropy of a node based on the labels.\",\n",
       "  'code': 'from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import numpy as np iris = load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) dt_classifier = DecisionTreeClassifier(random_state=42) dt_classifier.fit(X_train, y_train) def calculate_entropy(labels):     unique_labels, label_counts = np.unique(labels, return_counts=True)     probabilities = label_counts / len(labels)     entropy = -np.sum(probabilities * np.log2(probabilities))     return entropy first_split_indices = np.where(dt_classifier.tree_.children_left == 1)[0] parent_entropy = calculate_entropy(y_train) left_child_labels = y_train[first_split_indices] left_child_entropy = calculate_entropy(left_child_labels) right_child_labels = y_train[~np.isin(np.arange(len(y_train)), first_split_indices)] right_child_entropy = calculate_entropy(right_child_labels) weighted_avg_entropy_children = (     (len(left_child_labels) / len(y_train)) * left_child_entropy +     (len(right_child_labels) / len(y_train)) * right_child_entropy ) information_gain = parent_entropy - weighted_avg_entropy_children print(\"Information Gain on the first split:\", information_gain)',\n",
       "  'commentCode': '# Load the iris dataset as an example # Split the dataset into training and testing sets # Create a decision tree classifier (you can use your trained model) # Calculate entropy of a node (parent or child) # Get indices of samples in the first split # Calculate entropy of the parent node # Calculate entropy of the left child node (assuming binary split) # Calculate entropy of the right child node # Calculate the weighted average entropy of the children # Calculate information gain'},\n",
       " {'role': 'user', 'text': 'can you code it with the codes that Ä± provided'},\n",
       " {'role': 'assistant',\n",
       "  'text': \"Certainly! Here's how you can calculate the information gain on the first split using entropy with the decision tree model you trained\\nIn this code:best_dt_model_2.tree_.impurity provides the impurity (entropy) values for each node in the tree.best_dt_model_2.tree_.children_left and best_dt_model_2.tree_.children_right provide indices of the left and right children of each node.We extract the indices of samples in the first split and calculate entropy for the parent, left child, and right child nodes.Information gain is then computed as the difference between the entropy of the parent and the weighted average entropy of the children.This code should work with the decision tree model you trained earlier.\",\n",
       "  'code': 'from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split import numpy as np first_split_indices = np.where(best_dt_model_2.tree_.children_left == 1)[0] parent_entropy = best_dt_model_2.tree_.impurity[0] left_child_labels = y_train[first_split_indices] left_child_entropy = best_dt_model_2.tree_.impurity[best_dt_model_2.tree_.children_left[0]] right_child_labels = y_train[~np.isin(np.arange(len(y_train)), first_split_indices)] right_child_entropy = best_dt_model_2.tree_.impurity[best_dt_model_2.tree_.children_right[0]] weighted_avg_entropy_children = (     (len(left_child_labels) / len(y_train)) * left_child_entropy +     (len(right_child_labels) / len(y_train)) * right_child_entropy ) information_gain = parent_entropy - weighted_avg_entropy_children print(\"Information Gain on the first split:\", information_gain)',\n",
       "  'commentCode': '# Assuming you have X_train, X_test, y_train, y_test, best_dt_model_2 from previous code # Get indices of samples in the first split # Calculate entropy of the parent node # Calculate entropy of the left child node # Calculate entropy of the right child node # Calculate the weighted average entropy of the children # Calculate information gain'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code2convos[\"41b82427-7ae2-4c55-b8a3-310bb4abada0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to do:\n",
    "- Prompt matching with questions\n",
    "- Feature Engineering\n",
    "- Question Grades preparation\n",
    "- Train/Test split\n",
    "- Fitting a model for predicting the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Matching\n",
    "> We want to match the prompts with the questions in the Homework Let's\n",
    "> do it with a simple term frequency vectorizing method. For each prompt,\n",
    "> we will come with a vector that represents it. We will do the same\n",
    "> thing with each of the homework questions. Then, we will calculate the\n",
    "> vectors distanance to do the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompts -> butun promptlar tek listede\n",
    "#code2prompts -> keyler html adlari , value olarak da o htmldeki promptların listesi\n",
    "\n",
    "\n",
    "prompts = []\n",
    "code2prompts = defaultdict(list)\n",
    "for code , convos in code2convos.items():\n",
    "    user_prompts = []\n",
    "    for conv in convos:\n",
    "        if conv[\"role\"] == \"user\":\n",
    "            prompts.append(conv[\"text\"])\n",
    "            user_prompts.append(conv[\"text\"])\n",
    "    code2prompts[code] = user_prompts    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Give an example of how to plot a heat map of a specific results with \"matplotlib\"',\n",
       " 'How to select a subset of features that are likely strong predictors, justifying them based on calculated correlations',\n",
       " 'How to use isnull()',\n",
       " 'How to do dataset comparison',\n",
       " 'How to separate the dependent variable X from the independent variable y',\n",
       " 'How to split training and testing sets by 80% and 20% respectively',\n",
       " 'How to calculate correlations of all features in a dataset',\n",
       " 'How to select a subset of features that are likely strong predictors, justifying them based on correlations',\n",
       " 'How to choose the best hypermeter?',\n",
       " 'How to apply hypermeter',\n",
       " 'training of hypermeters',\n",
       " 'How can I visualize the trained tree?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code2prompts[\"6312a21b-c6be-44f9-ad81-46307b339fb6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_code_comments(data):\n",
    "    user_code_dict = {}\n",
    "    user_comment_dict = {}\n",
    "\n",
    "    for key, values in data.items():\n",
    "        for item in values:\n",
    "            if item['role'] == 'assistant':\n",
    "                user_code_dict[key] = item.get('code', '')\n",
    "                user_comment_dict[key] = item.get('commentCode', '')\n",
    "\n",
    "    return user_code_dict, user_comment_dict\n",
    "\n",
    "# Example usage\n",
    "code_dict, comment_dict = extract_user_code_comments(code2convos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTIONS MODIFIED( point indicators etc. omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"\"\"\n",
    "First make a copy of the notebook given to you as a starter.\n",
    "Make sure you choose Connect form upper right.\n",
    "You may upload the data to the section on your left on Colab, than right click on the .csv file and get the path of the file by clicking on \"Copy Path\". You will be using it when loading the data.\n",
    "\n",
    "\"\"\",\n",
    "#####################\n",
    "    \"\"\"Load training dataset Read the .csv file with the pandas library\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"\n",
    "Find the shape of the dataset (number of samples & number of attributes). (Hint: You can use the **shape** function)\n",
    "Display variable names (both dependent and independent).\n",
    "Display the summary of the dataset. (Hint: You can use the **info** function)\n",
    "Display the first 5 rows from training dataset. (Hint: You can use the **head** function)\n",
    "\n",
    "\n",
    "Check if there are any missing values in the dataset. If there are, you can either drop these values or fill it with most common values in corresponding rows. **Be careful that you have enough data for training the  model.**\n",
    "\n",
    "Encode categorical labels with the mappings given in the cell below. (Hint: You can use **map** function)\n",
    "\"\"\",\n",
    "\"\"\"Set X & y, split data\n",
    "\n",
    "Shuffle the dataset.\n",
    "Seperate your dependent variable X, and your independent variable y. The column health_metrics is y, the rest is X.\n",
    "Split training and test sets as 80% and 20%, respectively.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"\n",
    "\n",
    "Correlations of features with health \n",
    "Calculate the correlations for all features in dataset. Highlight any strong correlations with the target variable. Plot your results in a heatmap.\n",
    "\n",
    "\n",
    "Select a subset of features that are likely strong predictors, justifying your choices based on the computed correlations.\n",
    "\n",
    "\n",
    "Propose two hypothetical features that could enhance the model's predictive accuracy for Y, explaining how they might be derived and their expected impact. Show the resulting correlations with target variable.\n",
    "\n",
    "\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"\n",
    "Choose 2 hyperparameters to tune. You can use the Scikit learn decision tree documentation for the available hyperparameters *(Hyperparameters are listed under \"Parameters\" in the documentation)*. Use GridSearchCV for hyperparameter tuning, with a cross-validation value of 5. Use validation accuracy to pick the best hyper-parameter values. (15 pts)\n",
    "Explain the hyperparameters you chose to tune. *(What are the hyperparameters you chose? Why did you choose them?)* (5 pts)\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Re-train and plot the decision tree with the hyperparameters you have chosen\n",
    "Re-train model with the hyperparameters you have chosen \n",
    "Plot the tree you have trained\n",
    "Hint: You can import the **plot_tree** function from the sklearn library.\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Test your classifier on the test set\n",
    "Predict the labels of testing data using the tree you have trained\n",
    "Report the classification accuracy.\n",
    "Plot & investigate the confusion matrix. Fill the following blanks. \n",
    "> The model most frequently mistakes class(es) _________ for class(es) _________.\n",
    "Hint: You can use the confusion_matrix function from sklearn.metrics\n",
    "\"\"\",\n",
    "#####################\n",
    "\"\"\"Find the information gain on the first split\"\"\",\n",
    "#####################\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULATING SIMILARITY FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer, util\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39mparaphrase-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_similarity(prompt, questions):\n",
    "\n",
    "    # Encode prompt and questions\n",
    "    prompt_embedding = model.encode(prompt, convert_to_tensor=True)\n",
    "    question_embeddings = model.encode(questions, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_scores = util.pytorch_cos_sim(prompt_embedding, question_embeddings)\n",
    "\n",
    "    # Convert the tensor to a list of similarity scores\n",
    "    similarity_scores = cosine_scores.tolist()[0]\n",
    "\n",
    "    return similarity_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING BASE DICTIONARY IN THE SPECIFIED FORMAT\n",
    "\n",
    "#### Result dict contains {\"htmlfilename\": [ [sim1,sim2,....sim9],[sim1,sim2,....sim9],[sim1,sim2,....sim9] .... ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# Calculate similarity for each prompt and each question\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m---> 13\u001b[0m     similarity_scores \u001b[39m=\u001b[39m calculate_similarity(prompt, questions)\n\u001b[1;32m     14\u001b[0m     prompt_similarities\u001b[39m.\u001b[39mappend(similarity_scores)\n\u001b[1;32m     16\u001b[0m result_dict[html_file] \u001b[39m=\u001b[39m prompt_similarities\n",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m, in \u001b[0;36mcalculate_similarity\u001b[0;34m(prompt, questions)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_similarity\u001b[39m(prompt, questions):\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[39m# Encode prompt and questions\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     prompt_embedding \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(prompt, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     question_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(questions, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     \u001b[39m# Calculate cosine similarity\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Result dict contains {\"htmlfilename\": [ [sim1,sim2,....sim9],[sim1,sim2,....sim9],[sim1,sim2,....sim9] .... ]\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "html_file_names = list(code2prompts.keys())\n",
    "\n",
    "for html_file in html_file_names:\n",
    "    prompts = code2prompts[html_file]\n",
    "    prompt_similarities = []\n",
    "\n",
    "    # Calculate similarity for each prompt and each question\n",
    "    for prompt in prompts:\n",
    "        similarity_scores = calculate_similarity(prompt, questions)\n",
    "        prompt_similarities.append(similarity_scores)\n",
    "\n",
    "    result_dict[html_file] = prompt_similarities\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BELOW FILES ARE BROKEN. THEY GIVE 404 ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139235c7-736c-4237-92f0-92e8c116832c\n",
      "668ad17e-0240-49f7-b5a7-d22e502554c6\n",
      "b0640e51-6879-40cb-a4f5-329f952ef99d\n",
      "da6b70d5-29f6-491a-ad46-037c77067128\n"
     ]
    }
   ],
   "source": [
    "# BELOW FILES ARE CORRUPTED\n",
    "problems = []\n",
    "for key,value in code2prompts.items():\n",
    "    if len(code2prompts[key])==0:\n",
    "        print(key)\n",
    "        problems.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAKING MAXIMUM SIMILARITY FOR EACH QUESTION AMONG ALL PROMPTS FOR A SINGLE FILE\n",
    "\n",
    "#### IF FILE IS BROKEN, WE GIVE ALL 0 TO SIMILARITIES BETWEEN QUESTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q0</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HTML_File</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0031c86e-81f4-4eef-9e0e-28037abf9883</th>\n",
       "      <td>0.242323</td>\n",
       "      <td>0.650829</td>\n",
       "      <td>0.639897</td>\n",
       "      <td>0.790341</td>\n",
       "      <td>0.690072</td>\n",
       "      <td>0.788218</td>\n",
       "      <td>0.428342</td>\n",
       "      <td>0.461473</td>\n",
       "      <td>0.285456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0225686d-b825-4cac-8691-3a3a5343df2b</th>\n",
       "      <td>0.412892</td>\n",
       "      <td>0.822821</td>\n",
       "      <td>0.696175</td>\n",
       "      <td>0.909993</td>\n",
       "      <td>0.766649</td>\n",
       "      <td>0.987573</td>\n",
       "      <td>0.684214</td>\n",
       "      <td>0.729532</td>\n",
       "      <td>0.491343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>041f950b-c013-409a-a642-cffff60b9d4b</th>\n",
       "      <td>0.173714</td>\n",
       "      <td>0.217742</td>\n",
       "      <td>0.274107</td>\n",
       "      <td>0.284707</td>\n",
       "      <td>0.487476</td>\n",
       "      <td>0.249203</td>\n",
       "      <td>0.297617</td>\n",
       "      <td>0.538953</td>\n",
       "      <td>0.142434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04f91058-d0f8-4324-83b2-19c671f433dc</th>\n",
       "      <td>0.285610</td>\n",
       "      <td>0.548311</td>\n",
       "      <td>0.439191</td>\n",
       "      <td>0.645035</td>\n",
       "      <td>0.770841</td>\n",
       "      <td>0.659323</td>\n",
       "      <td>0.553139</td>\n",
       "      <td>0.538424</td>\n",
       "      <td>0.426416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089eb66d-4c3a-4f58-b98f-a3774a2efb34</th>\n",
       "      <td>0.444721</td>\n",
       "      <td>0.758592</td>\n",
       "      <td>0.872447</td>\n",
       "      <td>0.712805</td>\n",
       "      <td>0.821277</td>\n",
       "      <td>0.916046</td>\n",
       "      <td>0.555504</td>\n",
       "      <td>0.945931</td>\n",
       "      <td>0.590542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Q0        Q1        Q2        Q3   \n",
       "HTML_File                                                                      \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883  0.242323  0.650829  0.639897  0.790341  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b  0.412892  0.822821  0.696175  0.909993   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b  0.173714  0.217742  0.274107  0.284707   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc  0.285610  0.548311  0.439191  0.645035   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34  0.444721  0.758592  0.872447  0.712805   \n",
       "\n",
       "                                            Q4        Q5        Q6        Q7   \n",
       "HTML_File                                                                      \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883  0.690072  0.788218  0.428342  0.461473  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b  0.766649  0.987573  0.684214  0.729532   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b  0.487476  0.249203  0.297617  0.538953   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc  0.770841  0.659323  0.553139  0.538424   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34  0.821277  0.916046  0.555504  0.945931   \n",
       "\n",
       "                                            Q8  \n",
       "HTML_File                                       \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883  0.285456  \n",
       "0225686d-b825-4cac-8691-3a3a5343df2b  0.491343  \n",
       "041f950b-c013-409a-a642-cffff60b9d4b  0.142434  \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc  0.426416  \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34  0.590542  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#TAKING MAXIMUM SIMILARITY FOR EACH QUESTION AMONG ALL PROMPTS FOR A SINGLE FILE\n",
    "\n",
    "#test_dict = {\"html_file_1\": [[0.1,0.2,0.1,0.1,0.1,0.1,0.1,0.1,0.1],[0.4,0.9,0.3,0.1,0.1,0.1,0.1,0.1,0.1]],\n",
    " #              \"html_file_2\": []}\n",
    "\n",
    "new_result_dict = {}\n",
    "\n",
    "for html_file, prompt_list in result_dict.items():\n",
    "    if not prompt_list:\n",
    "        # If the prompt list is empty, assign 0 similarity for all questions\n",
    "        new_result_dict[html_file] = [0.0] * 9\n",
    "    else:\n",
    "        # Pad the prompt_list with zeros if the number of questions is less than 9\n",
    "        prompt_list += [[0.0] * 9] * (9 - len(prompt_list))\n",
    "        # Calculate the maximum similarity for each question across all prompts\n",
    "        max_sims = [max(prompt_list[i][j] for i in range(len(prompt_list))) for j in range(9)]\n",
    "        new_result_dict[html_file] = max_sims\n",
    "        \n",
    "        \n",
    "        \n",
    "# Convert the related_questions_dict to a DataFrame\n",
    "df_related_questions = pd.DataFrame.from_dict(new_result_dict, orient='index')\n",
    "\n",
    "# Set column names as 'Q0', 'Q1', ..., 'Q8'\n",
    "df_related_questions.columns = [f'Q{i}' for i in range(df_related_questions.shape[1])]\n",
    "\n",
    "# Reset the index to make 'HTML_File' a column\n",
    "df_related_questions.reset_index(inplace=True)\n",
    "df_related_questions.rename(columns={'index': 'HTML_File'}, inplace=True)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df_related_questions.set_index('HTML_File', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_related_questions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO HAVE AVERAGE SIMILARITY VALUES \n",
    "\n",
    "### i.e we are taking average between all sim1's sim2's among all prompts for a single prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO HAVE AVERAGE SIMILARITY VALUES \n",
    "\n",
    "# Calculate the average similarity for each question across all prompts\n",
    "for html_file, similarities_list in result_dict.items():\n",
    "    num_prompts = len(similarities_list)\n",
    "    num_questions = len(similarities_list[0]) if num_prompts > 0 else 0\n",
    "\n",
    "    if num_prompts > 0:\n",
    "        avg_similarities = [sum(similarities_list[j][i] for j in range(num_prompts)) / num_prompts for i in range(num_questions)]\n",
    "    else:\n",
    "        avg_similarities = [0] * num_questions\n",
    "\n",
    "    result_dict[html_file] = avg_similarities\n",
    "    \n",
    "\n",
    "    \n",
    "# Convert the dictionary to a DataFrame if needed\n",
    "df = pd.DataFrame.from_dict(result_dict, orient='index', columns=[f'Q{i}' for i in range(len(avg_similarities))])\n",
    "\n",
    "# Reset the index to make 'HTML_File' a column\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'HTML_File'}, inplace=True)\n",
    "\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df.set_index('HTML_File', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO HAVE YES OR NO TYPE OF MODEL\n",
    "\n",
    "### FOR A SINGLE FILE WE LOOK AT ALL sim1's among all the prompts, we identify the highest one and put a yes for corresponding question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q0</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HTML_File</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0031c86e-81f4-4eef-9e0e-28037abf9883</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0225686d-b825-4cac-8691-3a3a5343df2b</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>041f950b-c013-409a-a642-cffff60b9d4b</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04f91058-d0f8-4324-83b2-19c671f433dc</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089eb66d-4c3a-4f58-b98f-a3774a2efb34</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Q0   Q1   Q2   Q3   Q4   Q5   Q6   Q7   \n",
       "HTML_File                                                                     \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883  No  Yes  Yes  Yes  Yes  Yes   No  Yes  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b  No  Yes  Yes  Yes  Yes  Yes  Yes  Yes   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b  No  Yes   No   No  Yes   No  Yes  Yes   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc  No  Yes  Yes  Yes  Yes  Yes  Yes  Yes   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34  No  Yes  Yes  Yes  Yes  Yes  Yes  Yes   \n",
       "\n",
       "                                       Q8  \n",
       "HTML_File                                  \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883   No  \n",
       "0225686d-b825-4cac-8691-3a3a5343df2b  Yes  \n",
       "041f950b-c013-409a-a642-cffff60b9d4b   No  \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc  Yes  \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34  Yes  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO HAVE YES OR NO TYPE OF MODEL\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the index of the maximum value in a list\n",
    "def get_max_index(lst):\n",
    "    return max(range(len(lst)), key=lst.__getitem__, default=None)\n",
    "\n",
    "# Create a new dictionary to store related questions for each HTML file\n",
    "related_questions_dict = {}\n",
    "\n",
    "for html_file, similarities_list in result_dict.items():\n",
    "    # Get the index of the question with the highest similarity for each prompt\n",
    "    related_questions = [get_max_index(prompt) for prompt in similarities_list]\n",
    "\n",
    "    # Create a list indicating whether each question is related (Yes) or not (No)\n",
    "    is_related = ['Yes' if i in related_questions else 'No' for i in range(len(similarities_list[0]))] if similarities_list else ['No'] * 9\n",
    "\n",
    "    related_questions_dict[html_file] = is_related\n",
    "\n",
    "# Convert the related_questions_dict to a DataFrame\n",
    "df_related_questions = pd.DataFrame.from_dict(related_questions_dict, orient='index')\n",
    "\n",
    "# Set column names as 'Q0', 'Q1', ..., 'Q8'\n",
    "df_related_questions.columns = [f'Q{i}' for i in range(df_related_questions.shape[1])]\n",
    "\n",
    "# Reset the index to make 'HTML_File' a column\n",
    "df_related_questions.reset_index(inplace=True)\n",
    "df_related_questions.rename(columns={'index': 'HTML_File'}, inplace=True)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df_related_questions.set_index('HTML_File', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_related_questions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Number of prompts that a uers asked\n",
    "- Number of complaints that a user makes e.g \"the code gives this error!\"\n",
    "- User prompts average number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def calculate_discomfort_count(text):\n",
    "    # Discomfort indicators\n",
    "    discomfort_keywords = [\"no\", \"not really\", \"confused\", \"error\",\"not correct\"\n",
    "                       \"disappointed\", \"frustrated\", \"annoyed\", \"doesn't work\", \"apologize\", \"does not work\",\n",
    "                       \"didn't work\", \"did not work\", \"wrong\",\"didnt work\",\"doesnt work\",\"didnt understand\",\n",
    "                           \"did not understand\", \"didn't understand\"\n",
    "                       \"assertionerror\", \"attributeerror\", \"eoferror\", \"floatingpointerror\",\n",
    "                       \"generatorexit\", \"importerror\", \"indexerror\", \"keyerror\", \"keyboardinterrupt\",\n",
    "                       \"memoryerror\", \"nameerror\", \"notimplementederror\", \"oserror\", \"overflowerror\",\n",
    "                       \"referenceerror\", \"runtimeerror\", \"stopiteration\", \"syntaxerror\",\n",
    "                       \"indentationerror\", \"taberror\", \"systemerror\", \"systemexit\", \"typeerror\",\n",
    "                       \"unboundlocalerror\", \"unicodeerror\", \"unicodeencodeerror\", \"unicodedecodeerror\",\n",
    "                       \"unicodetranslateerror\", \"valueerror\", \"zerodivisionerror\"]\n",
    "\n",
    "\n",
    "    # Count the number of discomfort indicators in the text\n",
    "    discomfort_count = sum(1 for keyword in discomfort_keywords if re.search(rf'\\b{re.escape(keyword)}\\b', text.lower(), flags=re.IGNORECASE))\n",
    "\n",
    "\n",
    "    return discomfort_count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comfort_count(text):\n",
    "    # Comfort indicators\n",
    "    comfort_keywords = [\"good\", \"great\", \"awesome\", \"satisfied\", \"happy\", \"well done\", \"thank you\", \"excellent\",\"thanks\",\"worked\",\n",
    "                       \"works\",\"well\"]\n",
    "\n",
    "    # Count the number of comfort indicators in the text\n",
    "    comfort_count = sum(1 for keyword in comfort_keywords if re.search(rf'\\b{re.escape(keyword)}\\b', text.lower(), flags=re.IGNORECASE))\n",
    "\n",
    "    return comfort_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gpt_confusion(text):\n",
    "    confusion_keywords=[\"apologize\",\"confusion\",\"misunderstanding\",\"misunderstood\",\"apologies\",\"oversight\"]\n",
    "    confusion_count = sum(1 for keyword in confusion_keywords if re.search(rf'\\b{re.escape(keyword)}\\b', text.lower(), flags=re.IGNORECASE))\n",
    "\n",
    "    return confusion_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer, util\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39mparaphrase-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msim_between_prompt_and_commentcode\u001b[39m(user_message,gpt_code_comment):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def sim_between_prompt_and_commentcode(user_message,gpt_code_comment):\n",
    "\n",
    "    message_embedding = model.encode(user_message, convert_to_tensor=True)\n",
    "    gpt_commentcode_embedding = model.encode(gpt_code_comment, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_scores = util.pytorch_cos_sim(message_embedding, gpt_commentcode_embedding)\n",
    "\n",
    "    # Convert the tensor to a list of similarity scores\n",
    "    similarity_scores = cosine_scores.tolist()[0]\n",
    "\n",
    "    return similarity_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BELOW FUNCTION USES WORD2VEC MODEL AND CALCULATES THE SIMILARITY AMONG ALL PROMPTS\n",
    "\n",
    "### used for identifying how similar the prompts are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownloader\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m KeyedVectors\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "# Load the GloVe model\n",
    "model_name = \"glove-wiki-gigaword-100\"\n",
    "word_to_vec = gensim.downloader.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_cosine_similarity(user_prompts, word_to_vec):\n",
    "    if len(user_prompts) < 2:\n",
    "        # if there is only one user prompt then there is not one more prompt to compare with\n",
    "        return 0.0\n",
    "\n",
    "    # Vectorize each user prompt\n",
    "    prompt_vectors = [vectorize(prompt) for prompt in user_prompts]\n",
    "\n",
    "    # Calculate cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(prompt_vectors)\n",
    "\n",
    "    # Exclude the diagonal elements (self-similarity)\n",
    "    similarity_matrix_no_diagonal = similarity_matrix - np.eye(len(similarity_matrix))\n",
    "\n",
    "    # Calculate the average similarity\n",
    "    average_similarity = similarity_matrix_no_diagonal.sum() / (len(user_prompts) * (len(user_prompts) - 1))\n",
    "\n",
    "    return average_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vectorize(prompt):\n",
    "    words = prompt.lower().split()  # tokenizing\n",
    "    \n",
    "    # put zero vector if there is no representation in word_to_vec.\n",
    "    word_vectors = [word_to_vec[word] if word in word_to_vec else np.zeros_like(word_to_vec['word'])\n",
    "                    for word in words]  \n",
    "    # take average of all. \n",
    "    # by averaging the size is fixed for all reviews,\n",
    "    # the size is equal to the size of an encoding of a single word in word_to_vec\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_count(text):\n",
    "    # Split the text into words using a regular expression\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return len(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BY USING THE ABOVE FUNCTIONS AND MODELS, WE ARE STARTING TO CREATE OUR DATAFRAME WITH IDENTIFIED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_word_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m     code2features[code][\u001b[39m\"\u001b[39m\u001b[39mtotal_discomfort_count\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m calculate_discomfort_count(text)\n\u001b[1;32m     29\u001b[0m     code2features[code][\u001b[39m\"\u001b[39m\u001b[39mtotal_comfort_count\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m calculate_comfort_count(text)\n\u001b[0;32m---> 30\u001b[0m     code2features[code][\u001b[39m\"\u001b[39m\u001b[39mprompt_avg_words\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m calculate_word_count(text)\n\u001b[1;32m     33\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[39m# ChatGPT Responses\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     code2features[code][\u001b[39m\"\u001b[39m\u001b[39mresponse_avg_words\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m calculate_word_count(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_word_count' is not defined"
     ]
    }
   ],
   "source": [
    "code2features = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "\n",
    "for code, convs in code2convos.items():\n",
    "    if len(convs) == 0:\n",
    "        print(code)\n",
    "        continue\n",
    "    user_prompts = []\n",
    "    similarities_prompt_commentCode=[]\n",
    "    for c in convs:\n",
    "        # iterating over each dictionary\n",
    "        # {\"role\": \"user\", \"text\": }, {\"role\": \"assistant\", \"text\":, \"code\":, \"commentCode\": }\n",
    "        \n",
    "        lastCommentCode=\"\"\n",
    "        if c[\"role\"]==\"assistant\":\n",
    "            \n",
    "            lastCommentCode=c[\"commentCode\"]\n",
    "                \n",
    "                \n",
    "        text = c[\"text\"].lower()\n",
    "        if c[\"role\"] == \"user\":\n",
    "            # User Prompts\n",
    "            user_prompts.append(text)\n",
    "\n",
    "            # count the user prompts\n",
    "            code2features[code][\"#user_prompts\"] += 1\n",
    "\n",
    "            code2features[code][\"total_discomfort_count\"] += calculate_discomfort_count(text)\n",
    "            code2features[code][\"total_comfort_count\"] += calculate_comfort_count(text)\n",
    "            code2features[code][\"prompt_avg_words\"] += calculate_word_count(text)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            # ChatGPT Responses\n",
    "            code2features[code][\"response_avg_words\"] += calculate_word_count(text)\n",
    "            #Response confusion metric\n",
    "            code2features[code][\"response_confusion_total_count\"]+=calculate_gpt_confusion(text)\n",
    "            if lastCommentCode!=\"\":\n",
    "                \n",
    "                similarities_prompt_commentCode.append(sim_between_prompt_and_commentcode(prevText,lastCommentCode))   \n",
    "            \n",
    "        prevText=text\n",
    "\n",
    "    code2features[code][\"prompt_avg_words\"] /= code2features[code][\"#user_prompts\"]\n",
    "    code2features[code][\"response_avg_words\"] /= code2features[code][\"#user_prompts\"]\n",
    "    average_similarity = calculate_average_cosine_similarity(user_prompts, word_to_vec)\n",
    "    code2features[code][\"similarity_between_prompts\"] = average_similarity\n",
    "    \n",
    "    if len(similarities_prompt_commentCode) > 0:\n",
    "        code2features[code][\"avg_sim_between_prompt_codeComment\"] = sum(similarities_prompt_commentCode) / len(similarities_prompt_commentCode)\n",
    "    else:\n",
    "        code2features[code][\"avg_sim_between_prompt_codeComment\"] = 0  \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#user_prompts</th>\n",
       "      <th>total_discomfort_count</th>\n",
       "      <th>total_comfort_count</th>\n",
       "      <th>prompt_avg_words</th>\n",
       "      <th>response_avg_words</th>\n",
       "      <th>response_confusion_total_count</th>\n",
       "      <th>similarity_between_prompts</th>\n",
       "      <th>avg_sim_between_prompt_codeComment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0031c86e-81f4-4eef-9e0e-28037abf9883</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.071429</td>\n",
       "      <td>160.642857</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.959773</td>\n",
       "      <td>0.558252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0225686d-b825-4cac-8691-3a3a5343df2b</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.666667</td>\n",
       "      <td>169.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913921</td>\n",
       "      <td>0.556858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>041f950b-c013-409a-a642-cffff60b9d4b</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.111111</td>\n",
       "      <td>160.444444</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.802151</td>\n",
       "      <td>0.331554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04f91058-d0f8-4324-83b2-19c671f433dc</th>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>148.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.904731</td>\n",
       "      <td>0.419597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089eb66d-4c3a-4f58-b98f-a3774a2efb34</th>\n",
       "      <td>86.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.697674</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.889234</td>\n",
       "      <td>0.470122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      #user_prompts  total_discomfort_count   \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883           14.0                     8.0  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b           18.0                     0.0   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b            9.0                     8.0   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc           20.0                     3.0   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34           86.0                    16.0   \n",
       "\n",
       "                                      total_comfort_count  prompt_avg_words   \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883                  0.0         63.071429  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b                  0.0         39.666667   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b                  1.0         99.111111   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc                  0.0         17.750000   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34                  1.0         30.697674   \n",
       "\n",
       "                                      response_avg_words   \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883          160.642857  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b          169.166667   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b          160.444444   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc          148.500000   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34          123.000000   \n",
       "\n",
       "                                      response_confusion_total_count   \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883                             2.0  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b                             1.0   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b                            12.0   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc                             3.0   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34                            19.0   \n",
       "\n",
       "                                      similarity_between_prompts   \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883                    0.959773  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b                    0.913921   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b                    0.802151   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc                    0.904731   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34                    0.889234   \n",
       "\n",
       "                                      avg_sim_between_prompt_codeComment  \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883                            0.558252  \n",
       "0225686d-b825-4cac-8691-3a3a5343df2b                            0.556858  \n",
       "041f950b-c013-409a-a642-cffff60b9d4b                            0.331554  \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc                            0.419597  \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34                            0.470122  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(code2features).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDING 1 MORE FEATURE --- CREATING AVERAGE VECTOR OF THE ONES WHO GET 100, AND PUTTING SIMILARITY FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmls_100_scores = [\"ba18e4e8-2c26-46d4-ba31-cc21947aabd5\",\n",
    "\"58bee29c-a749-463e-8d56-c4edf0815b3f\",\n",
    "\"51f35201-da77-4b6d-a455-99cc84195c5c\",\n",
    "\"223ae726-cb25-49df-a125-c4af3519c8e8\",\n",
    "\"e264c424-a241-43f7-acca-9fbbf21dc1c6\",\n",
    "\"f2f18684-4a16-4c05-a2d1-c0f96d1de869\",\n",
    "\"c21be348-17bd-4fdd-88f5-137f6a13cdee\",\n",
    "\"c91d6fef-baf5-4e77-8bfc-b14fb7fc770d\",\n",
    "\"58fcd378-aa29-4067-813c-bb4de525428e\",\n",
    "\"5a62a8ee-c67c-475e-bd85-cf6d83c90ea9\",\n",
    "\"089eb66d-4c3a-4f58-b98f-a3774a2efb34\",\n",
    "\"f8ec3336-fd48-4654-ad98-62ccfb96d096\",\n",
    "\"36bab6e3-0d16-4626-846b-33c0384f0c79\",\n",
    "\"5541316f-6fbd-4441-b513-ac252e6355ec\",\n",
    "\"97f57cf9-4f02-4f8c-b65c-8ea0009a82a2\",\n",
    "\"7b0ecddc-caa5-4b81-88ea-cd65a7270900\",\n",
    "\"17bd62e7-6792-4399-a573-07456a73901b\",\n",
    "\"26d95379-e2f1-454c-a9d2-60cd80bc06a5\",\n",
    "\"745bb746-6467-4146-8ef5-55f3ee3f589e\",\n",
    "\"1b54e38b-3b1d-425e-835a-d1e0fb2694fc\",\n",
    "\"27de4332-d81f-47a2-b2a9-c0b023a30919\",\n",
    "\"1038cb22-8ad2-4030-b44a-59f10352e91c\",\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to aggregate the features\n",
    "aggregated_features_100 = defaultdict(float)\n",
    "\n",
    "# Initialize a counter for the number of files that scored 100\n",
    "count_files_score_100 = 0\n",
    "\n",
    "# Iterate through each HTML file that scored 100 and aggregate their features\n",
    "for code in htmls_100_scores:\n",
    "    if code in code2features:\n",
    "        count_files_score_100 += 1\n",
    "        for feature in df.columns.tolist():\n",
    "            aggregated_features_100[feature] += code2features[code].get(feature, 0)\n",
    "\n",
    "average_features_100 = {feature: total / count_files_score_100 for feature, total in aggregated_features_100.items()}\n",
    "\n",
    "vector_list = list(average_features_100.values())\n",
    "\n",
    "\n",
    "def calculate_similarity(row):\n",
    "    row_values = row.values.tolist()\n",
    "    similarity = cosine_similarity([vector_list], [row_values])\n",
    "    return similarity[0][0]\n",
    "\n",
    "\n",
    "df['similarity_between_fullpoints'] = df.apply(calculate_similarity, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#user_prompts</th>\n",
       "      <th>total_discomfort_count</th>\n",
       "      <th>total_comfort_count</th>\n",
       "      <th>prompt_avg_words</th>\n",
       "      <th>response_avg_words</th>\n",
       "      <th>response_confusion_total_count</th>\n",
       "      <th>similarity_between_prompts</th>\n",
       "      <th>avg_sim_between_prompt_codeComment</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0031c86e-81f4-4eef-9e0e-28037abf9883</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.071429</td>\n",
       "      <td>160.642857</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.959773</td>\n",
       "      <td>0.558252</td>\n",
       "      <td>0.984955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0225686d-b825-4cac-8691-3a3a5343df2b</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.666667</td>\n",
       "      <td>169.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913921</td>\n",
       "      <td>0.556858</td>\n",
       "      <td>0.988546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>041f950b-c013-409a-a642-cffff60b9d4b</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.111111</td>\n",
       "      <td>160.444444</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.802151</td>\n",
       "      <td>0.331554</td>\n",
       "      <td>0.950704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04f91058-d0f8-4324-83b2-19c671f433dc</th>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>148.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.904731</td>\n",
       "      <td>0.419597</td>\n",
       "      <td>0.980123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>089eb66d-4c3a-4f58-b98f-a3774a2efb34</th>\n",
       "      <td>86.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.697674</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.889234</td>\n",
       "      <td>0.470122</td>\n",
       "      <td>0.928597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      #user_prompts  total_discomfort_count   \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883           14.0                     8.0  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b           18.0                     0.0   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b            9.0                     8.0   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc           20.0                     3.0   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34           86.0                    16.0   \n",
       "\n",
       "                                      total_comfort_count  prompt_avg_words   \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883                  0.0         63.071429  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b                  0.0         39.666667   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b                  1.0         99.111111   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc                  0.0         17.750000   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34                  1.0         30.697674   \n",
       "\n",
       "                                      response_avg_words   \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883          160.642857  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b          169.166667   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b          160.444444   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc          148.500000   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34          123.000000   \n",
       "\n",
       "                                      response_confusion_total_count   \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883                             2.0  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b                             1.0   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b                            12.0   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc                             3.0   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34                            19.0   \n",
       "\n",
       "                                      similarity_between_prompts   \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883                    0.959773  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b                    0.913921   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b                    0.802151   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc                    0.904731   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34                    0.889234   \n",
       "\n",
       "                                      avg_sim_between_prompt_codeComment   \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883                            0.558252  \\\n",
       "0225686d-b825-4cac-8691-3a3a5343df2b                            0.556858   \n",
       "041f950b-c013-409a-a642-cffff60b9d4b                            0.331554   \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc                            0.419597   \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34                            0.470122   \n",
       "\n",
       "                                      cosine_similarity  \n",
       "0031c86e-81f4-4eef-9e0e-28037abf9883           0.984955  \n",
       "0225686d-b825-4cac-8691-3a3a5343df2b           0.988546  \n",
       "041f950b-c013-409a-a642-cffff60b9d4b           0.950704  \n",
       "04f91058-d0f8-4324-83b2-19c671f433dc           0.980123  \n",
       "089eb66d-4c3a-4f58-b98f-a3774a2efb34           0.928597  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDING NEW FEATURES. LET'S ADD METRICS THAT MEASURE HOW DETAİLED THE PROMPTS ARE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analiz Sonuçları - 0031c86e-81f4-4eef-9e0e-28037abf9883:\n",
      "{'total_message_length': 5701, 'average_word_count_per_sentence': 63.07142857142857, 'average_word_length': 5.438278595696489, 'english_conjunctions_count': 32, 'example_words_count': 2, 'total_parentheses_count': 14}\n",
      "==============================\n",
      "Analiz Sonuçları - 0225686d-b825-4cac-8691-3a3a5343df2b:\n",
      "{'total_message_length': 4700, 'average_word_count_per_sentence': 39.388888888888886, 'average_word_length': 5.423131170662906, 'english_conjunctions_count': 10, 'example_words_count': 0, 'total_parentheses_count': 65}\n",
      "==============================\n",
      "Analiz Sonuçları - 041f950b-c013-409a-a642-cffff60b9d4b:\n",
      "{'total_message_length': 5967, 'average_word_count_per_sentence': 97.55555555555556, 'average_word_length': 5.619589977220957, 'english_conjunctions_count': 13, 'example_words_count': 0, 'total_parentheses_count': 100}\n",
      "==============================\n",
      "Analiz Sonuçları - 04f91058-d0f8-4324-83b2-19c671f433dc:\n",
      "{'total_message_length': 2282, 'average_word_count_per_sentence': 17.75, 'average_word_length': 5.087323943661972, 'english_conjunctions_count': 6, 'example_words_count': 0, 'total_parentheses_count': 12}\n",
      "==============================\n",
      "Analiz Sonuçları - 089eb66d-4c3a-4f58-b98f-a3774a2efb34:\n",
      "{'total_message_length': 18669, 'average_word_count_per_sentence': 28.767441860465116, 'average_word_length': 6.212206952303961, 'english_conjunctions_count': 59, 'example_words_count': 2, 'total_parentheses_count': 271}\n",
      "==============================\n",
      "Analiz Sonuçları - 090d6217-5d69-4929-a342-19abab78324f:\n",
      "{'total_message_length': 21914, 'average_word_count_per_sentence': 33.02739726027397, 'average_word_length': 7.566569888013272, 'english_conjunctions_count': 20, 'example_words_count': 1, 'total_parentheses_count': 297}\n",
      "==============================\n",
      "Analiz Sonuçları - 0c95c563-f1e1-4a35-844c-2e8ccdd1b161:\n",
      "{'total_message_length': 5107, 'average_word_count_per_sentence': 46.86666666666667, 'average_word_length': 5.97724039829303, 'english_conjunctions_count': 9, 'example_words_count': 0, 'total_parentheses_count': 64}\n",
      "==============================\n",
      "Analiz Sonuçları - 0ddfae9c-0dbd-4fbe-9e68-c3e0cb73b8fc:\n",
      "{'total_message_length': 2008, 'average_word_count_per_sentence': 29.454545454545453, 'average_word_length': 5.160493827160494, 'english_conjunctions_count': 5, 'example_words_count': 0, 'total_parentheses_count': 12}\n",
      "==============================\n",
      "Analiz Sonuçları - 0e466794-f7f8-4cc2-b07e-070b13a0b5e1:\n",
      "{'total_message_length': 27590, 'average_word_count_per_sentence': 67.97826086956522, 'average_word_length': 7.228333866325552, 'english_conjunctions_count': 38, 'example_words_count': 0, 'total_parentheses_count': 473}\n",
      "==============================\n",
      "Analiz Sonuçları - 0f0c953a-a472-47c1-809b-9fc14dba9091:\n",
      "{'total_message_length': 3961, 'average_word_count_per_sentence': 34.11764705882353, 'average_word_length': 5.451724137931034, 'english_conjunctions_count': 10, 'example_words_count': 0, 'total_parentheses_count': 38}\n",
      "==============================\n",
      "Analiz Sonuçları - 1029802d-1057-4e3e-b827-e8a9c2ded3b9:\n",
      "{'total_message_length': 14119, 'average_word_count_per_sentence': 35.53846153846154, 'average_word_length': 6.270562770562771, 'english_conjunctions_count': 23, 'example_words_count': 0, 'total_parentheses_count': 193}\n",
      "==============================\n",
      "Analiz Sonuçları - 1038cb22-8ad2-4030-b44a-59f10352e91c:\n",
      "{'total_message_length': 21081, 'average_word_count_per_sentence': 36.08064516129032, 'average_word_length': 7.94456861868574, 'english_conjunctions_count': 21, 'example_words_count': 0, 'total_parentheses_count': 339}\n",
      "==============================\n",
      "Analiz Sonuçları - 106ffe99-c787-4d09-9076-4ba411eb68b1:\n",
      "{'total_message_length': 5643, 'average_word_count_per_sentence': 35.8, 'average_word_length': 5.298324022346368, 'english_conjunctions_count': 15, 'example_words_count': 0, 'total_parentheses_count': 75}\n",
      "==============================\n",
      "Analiz Sonuçları - 139235c7-736c-4237-92f0-92e8c116832c:\n",
      "{'total_message_length': 0, 'average_word_count_per_sentence': 0, 'average_word_length': 0, 'english_conjunctions_count': 0, 'example_words_count': 0, 'total_parentheses_count': 0}\n",
      "==============================\n",
      "Analiz Sonuçları - 14ce054d-4619-4685-ba9b-4b5cd6f81b2d:\n",
      "{'total_message_length': 6648, 'average_word_count_per_sentence': 78.5, 'average_word_length': 7.002547770700637, 'english_conjunctions_count': 7, 'example_words_count': 0, 'total_parentheses_count': 155}\n",
      "==============================\n",
      "Analiz Sonuçları - 152a7787-ecd1-448f-a98e-8af0826d8215:\n",
      "{'total_message_length': 5768, 'average_word_count_per_sentence': 25.933333333333334, 'average_word_length': 6.0347043701799485, 'english_conjunctions_count': 6, 'example_words_count': 0, 'total_parentheses_count': 63}\n",
      "==============================\n",
      "Analiz Sonuçları - 17bd62e7-6792-4399-a573-07456a73901b:\n",
      "{'total_message_length': 7435, 'average_word_count_per_sentence': 30.10810810810811, 'average_word_length': 5.466786355475763, 'english_conjunctions_count': 25, 'example_words_count': 0, 'total_parentheses_count': 99}\n",
      "==============================\n",
      "Analiz Sonuçları - 1b54e38b-3b1d-425e-835a-d1e0fb2694fc:\n",
      "{'total_message_length': 8446, 'average_word_count_per_sentence': 58.44444444444444, 'average_word_length': 6.701520912547529, 'english_conjunctions_count': 12, 'example_words_count': 0, 'total_parentheses_count': 155}\n",
      "==============================\n",
      "Analiz Sonuçları - 223ae726-cb25-49df-a125-c4af3519c8e8:\n",
      "{'total_message_length': 2463, 'average_word_count_per_sentence': 22.875, 'average_word_length': 5.426229508196721, 'english_conjunctions_count': 8, 'example_words_count': 0, 'total_parentheses_count': 14}\n",
      "==============================\n",
      "Analiz Sonuçları - 22bb7162-3399-464a-b30b-cf1fc3210b4e:\n",
      "{'total_message_length': 7852, 'average_word_count_per_sentence': 55.285714285714285, 'average_word_length': 5.359173126614987, 'english_conjunctions_count': 25, 'example_words_count': 0, 'total_parentheses_count': 77}\n",
      "==============================\n",
      "Analiz Sonuçları - 2446216c-c557-4ee8-b470-7e2ae3c88968:\n",
      "{'total_message_length': 4738, 'average_word_count_per_sentence': 115.8, 'average_word_length': 6.853195164075993, 'english_conjunctions_count': 5, 'example_words_count': 0, 'total_parentheses_count': 108}\n",
      "==============================\n",
      "Analiz Sonuçları - 24f01035-0717-4256-9952-c415aa8ecd10:\n",
      "{'total_message_length': 47716, 'average_word_count_per_sentence': 75.13636363636364, 'average_word_length': 7.765073603549102, 'english_conjunctions_count': 51, 'example_words_count': 0, 'total_parentheses_count': 687}\n",
      "==============================\n",
      "Analiz Sonuçları - 26d95379-e2f1-454c-a9d2-60cd80bc06a5:\n",
      "{'total_message_length': 4238, 'average_word_count_per_sentence': 20.3, 'average_word_length': 5.752052545155993, 'english_conjunctions_count': 13, 'example_words_count': 0, 'total_parentheses_count': 70}\n",
      "==============================\n",
      "Analiz Sonuçları - 271b130d-50bd-436e-add6-38d9c618be8a:\n",
      "{'total_message_length': 3672, 'average_word_count_per_sentence': 31.833333333333332, 'average_word_length': 5.209424083769633, 'english_conjunctions_count': 19, 'example_words_count': 0, 'total_parentheses_count': 10}\n",
      "==============================\n",
      "Analiz Sonuçları - 27de4332-d81f-47a2-b2a9-c0b023a30919:\n",
      "{'total_message_length': 6513, 'average_word_count_per_sentence': 37.857142857142854, 'average_word_length': 6.628930817610063, 'english_conjunctions_count': 12, 'example_words_count': 0, 'total_parentheses_count': 84}\n",
      "==============================\n",
      "Analiz Sonuçları - 2b9cf078-c56b-4020-9197-cd9f7d4f909c:\n",
      "{'total_message_length': 9555, 'average_word_count_per_sentence': 69.26666666666667, 'average_word_length': 7.422521655437921, 'english_conjunctions_count': 12, 'example_words_count': 0, 'total_parentheses_count': 153}\n",
      "==============================\n",
      "Analiz Sonuçları - 30283b91-7fc3-4125-985b-b441f0f489d6:\n",
      "{'total_message_length': 8349, 'average_word_count_per_sentence': 77.14285714285714, 'average_word_length': 6.125925925925926, 'english_conjunctions_count': 10, 'example_words_count': 0, 'total_parentheses_count': 114}\n",
      "==============================\n",
      "Analiz Sonuçları - 3104d903-6012-484c-bd00-b93594b289ea:\n",
      "{'total_message_length': 7395, 'average_word_count_per_sentence': 58.4375, 'average_word_length': 6.5037433155080215, 'english_conjunctions_count': 11, 'example_words_count': 0, 'total_parentheses_count': 115}\n",
      "==============================\n",
      "Analiz Sonuçları - 331a45a0-341c-4faf-97de-75c82b31b61f:\n",
      "{'total_message_length': 2055, 'average_word_count_per_sentence': 14.681818181818182, 'average_word_length': 5.021671826625387, 'english_conjunctions_count': 7, 'example_words_count': 0, 'total_parentheses_count': 8}\n",
      "==============================\n",
      "Analiz Sonuçları - 36bab6e3-0d16-4626-846b-33c0384f0c79:\n",
      "{'total_message_length': 18632, 'average_word_count_per_sentence': 42.824561403508774, 'average_word_length': 5.571487095452683, 'english_conjunctions_count': 39, 'example_words_count': 2, 'total_parentheses_count': 210}\n",
      "==============================\n",
      "Analiz Sonuçları - 38296004-7336-4797-9db4-662a48309a1c:\n",
      "{'total_message_length': 921, 'average_word_count_per_sentence': 36.75, 'average_word_length': 5.217687074829932, 'english_conjunctions_count': 0, 'example_words_count': 0, 'total_parentheses_count': 22}\n",
      "==============================\n",
      "Analiz Sonuçları - 410d88de-2489-4a83-8dae-6bc01e8e9f78:\n",
      "{'total_message_length': 227, 'average_word_count_per_sentence': 8.4, 'average_word_length': 4.5, 'english_conjunctions_count': 1, 'example_words_count': 0, 'total_parentheses_count': 0}\n",
      "==============================\n",
      "Analiz Sonuçları - 41b82427-7ae2-4c55-b8a3-310bb4abada0:\n",
      "{'total_message_length': 10121, 'average_word_count_per_sentence': 46.61538461538461, 'average_word_length': 7.031353135313531, 'english_conjunctions_count': 13, 'example_words_count': 0, 'total_parentheses_count': 141}\n",
      "==============================\n",
      "Analiz Sonuçları - 42980d53-7bcd-4a36-bf3a-aa43f7417ac5:\n",
      "{'total_message_length': 5984, 'average_word_count_per_sentence': 59.15384615384615, 'average_word_length': 6.218465539661898, 'english_conjunctions_count': 12, 'example_words_count': 0, 'total_parentheses_count': 84}\n",
      "==============================\n",
      "Analiz Sonuçları - 43ff9786-2b47-425b-8bad-e274d9988a0e:\n",
      "{'total_message_length': 23750, 'average_word_count_per_sentence': 55.41818181818182, 'average_word_length': 6.466535433070866, 'english_conjunctions_count': 44, 'example_words_count': 0, 'total_parentheses_count': 285}\n",
      "==============================\n",
      "Analiz Sonuçları - 450550b4-3bb9-4b12-a7fd-121ac4a36ea9:\n",
      "{'total_message_length': 4093, 'average_word_count_per_sentence': 51.5, 'average_word_length': 5.383495145631068, 'english_conjunctions_count': 7, 'example_words_count': 0, 'total_parentheses_count': 54}\n",
      "==============================\n",
      "Analiz Sonuçları - 45d2c21a-828e-46d9-8fcd-a4a39888773c:\n",
      "{'total_message_length': 3592, 'average_word_count_per_sentence': 27.11111111111111, 'average_word_length': 5.987704918032787, 'english_conjunctions_count': 9, 'example_words_count': 0, 'total_parentheses_count': 42}\n",
      "==============================\n",
      "Analiz Sonuçları - 4e6fdf20-96fa-4f62-bf55-5c4c695afebe:\n",
      "{'total_message_length': 7605, 'average_word_count_per_sentence': 55.2, 'average_word_length': 5.509963768115942, 'english_conjunctions_count': 12, 'example_words_count': 0, 'total_parentheses_count': 126}\n",
      "==============================\n",
      "Analiz Sonuçları - 4fb21782-81f0-47c1-8831-58a748904a2c:\n",
      "{'total_message_length': 3349, 'average_word_count_per_sentence': 38.714285714285715, 'average_word_length': 5.186346863468635, 'english_conjunctions_count': 8, 'example_words_count': 0, 'total_parentheses_count': 53}\n",
      "==============================\n",
      "Analiz Sonuçları - 50a71154-2269-460c-9341-291221c6ef02:\n",
      "{'total_message_length': 65180, 'average_word_count_per_sentence': 99.38571428571429, 'average_word_length': 7.466724162713813, 'english_conjunctions_count': 84, 'example_words_count': 0, 'total_parentheses_count': 939}\n",
      "==============================\n",
      "Analiz Sonuçları - 51f35201-da77-4b6d-a455-99cc84195c5c:\n",
      "{'total_message_length': 4891, 'average_word_count_per_sentence': 54.38461538461539, 'average_word_length': 5.718528995756719, 'english_conjunctions_count': 15, 'example_words_count': 0, 'total_parentheses_count': 28}\n",
      "==============================\n",
      "Analiz Sonuçları - 530b4e58-756d-4627-ad08-65ba0457ad42:\n",
      "{'total_message_length': 5501, 'average_word_count_per_sentence': 50.8125, 'average_word_length': 5.479704797047971, 'english_conjunctions_count': 10, 'example_words_count': 0, 'total_parentheses_count': 45}\n",
      "==============================\n",
      "Analiz Sonuçları - 53ad17d1-5eb2-4af6-9c38-3c90c05ee695:\n",
      "{'total_message_length': 19642, 'average_word_count_per_sentence': 36.39344262295082, 'average_word_length': 7.313063063063063, 'english_conjunctions_count': 18, 'example_words_count': 0, 'total_parentheses_count': 328}\n",
      "==============================\n",
      "Analiz Sonuçları - 5541316f-6fbd-4441-b513-ac252e6355ec:\n",
      "{'total_message_length': 7602, 'average_word_count_per_sentence': 55.4, 'average_word_length': 5.7048736462093865, 'english_conjunctions_count': 19, 'example_words_count': 0, 'total_parentheses_count': 65}\n",
      "==============================\n",
      "Analiz Sonuçları - 56c6f8dd-f37c-44d2-9820-9459aa34c8af:\n",
      "{'total_message_length': 2273, 'average_word_count_per_sentence': 13.25925925925926, 'average_word_length': 5.337988826815643, 'english_conjunctions_count': 5, 'example_words_count': 0, 'total_parentheses_count': 22}\n",
      "==============================\n",
      "Analiz Sonuçları - 58bee29c-a749-463e-8d56-c4edf0815b3f:\n",
      "{'total_message_length': 9679, 'average_word_count_per_sentence': 33.80952380952381, 'average_word_length': 5.5, 'english_conjunctions_count': 20, 'example_words_count': 0, 'total_parentheses_count': 116}\n",
      "==============================\n",
      "Analiz Sonuçları - 58fcd378-aa29-4067-813c-bb4de525428e:\n",
      "{'total_message_length': 16746, 'average_word_count_per_sentence': 32.983870967741936, 'average_word_length': 6.796577017114914, 'english_conjunctions_count': 35, 'example_words_count': 0, 'total_parentheses_count': 269}\n",
      "==============================\n",
      "Analiz Sonuçları - 597c7a93-6b16-4af9-9846-154599f596e1:\n",
      "{'total_message_length': 927, 'average_word_count_per_sentence': 12.76923076923077, 'average_word_length': 4.650602409638554, 'english_conjunctions_count': 0, 'example_words_count': 0, 'total_parentheses_count': 2}\n",
      "==============================\n",
      "Analiz Sonuçları - 5a62a8ee-c67c-475e-bd85-cf6d83c90ea9:\n",
      "{'total_message_length': 64231, 'average_word_count_per_sentence': 96.05333333333333, 'average_word_length': 7.181704608550805, 'english_conjunctions_count': 72, 'example_words_count': 0, 'total_parentheses_count': 1233}\n",
      "==============================\n",
      "Analiz Sonuçları - 5dbf76be-6634-4d42-a87f-b62b7fa6ceae:\n",
      "{'total_message_length': 15607, 'average_word_count_per_sentence': 41.58695652173913, 'average_word_length': 6.880292733925771, 'english_conjunctions_count': 20, 'example_words_count': 0, 'total_parentheses_count': 230}\n",
      "==============================\n",
      "Analiz Sonuçları - 5e481e20-f714-4f11-b941-0ef2fd5976d3:\n",
      "{'total_message_length': 1694, 'average_word_count_per_sentence': 210.0, 'average_word_length': 6.109523809523809, 'english_conjunctions_count': 2, 'example_words_count': 0, 'total_parentheses_count': 17}\n",
      "==============================\n",
      "Analiz Sonuçları - 6312a21b-c6be-44f9-ad81-46307b339fb6:\n",
      "{'total_message_length': 666, 'average_word_count_per_sentence': 9.5, 'average_word_length': 4.947368421052632, 'english_conjunctions_count': 2, 'example_words_count': 1, 'total_parentheses_count': 2}\n",
      "==============================\n",
      "Analiz Sonuçları - 63216e9b-5d0b-4047-97ee-302495d61640:\n",
      "{'total_message_length': 5154, 'average_word_count_per_sentence': 36.473684210526315, 'average_word_length': 5.8311688311688314, 'english_conjunctions_count': 16, 'example_words_count': 0, 'total_parentheses_count': 80}\n",
      "==============================\n",
      "Analiz Sonuçları - 65ea56c3-e205-4ed9-8b85-bd1876228cee:\n",
      "{'total_message_length': 3669, 'average_word_count_per_sentence': 44.9, 'average_word_length': 5.351893095768374, 'english_conjunctions_count': 8, 'example_words_count': 0, 'total_parentheses_count': 26}\n",
      "==============================\n",
      "Analiz Sonuçları - 663f8b4e-b683-4365-8316-b1dd1d325110:\n",
      "{'total_message_length': 8416, 'average_word_count_per_sentence': 30.25, 'average_word_length': 7.268595041322314, 'english_conjunctions_count': 16, 'example_words_count': 0, 'total_parentheses_count': 90}\n",
      "==============================\n",
      "Analiz Sonuçları - 668ad17e-0240-49f7-b5a7-d22e502554c6:\n",
      "{'total_message_length': 0, 'average_word_count_per_sentence': 0, 'average_word_length': 0, 'english_conjunctions_count': 0, 'example_words_count': 0, 'total_parentheses_count': 0}\n",
      "==============================\n",
      "Analiz Sonuçları - 67c4a788-ec23-48c4-b6db-d76be6e118d6:\n",
      "{'total_message_length': 13447, 'average_word_count_per_sentence': 46.125, 'average_word_length': 5.930623306233063, 'english_conjunctions_count': 35, 'example_words_count': 0, 'total_parentheses_count': 151}\n",
      "==============================\n",
      "Analiz Sonuçları - 69426dc0-b745-4f89-9c9a-943828d19db9:\n",
      "{'total_message_length': 23658, 'average_word_count_per_sentence': 70.02702702702703, 'average_word_length': 7.503280586646083, 'english_conjunctions_count': 33, 'example_words_count': 0, 'total_parentheses_count': 294}\n",
      "==============================\n",
      "Analiz Sonuçları - 6a2003ad-a05a-41c9-9d48-e98491a90499:\n",
      "{'total_message_length': 92, 'average_word_count_per_sentence': 8.0, 'average_word_length': 4.875, 'english_conjunctions_count': 0, 'example_words_count': 0, 'total_parentheses_count': 0}\n",
      "==============================\n",
      "Analiz Sonuçları - 6a903495-c5be-4263-b4dd-75e2bbc30434:\n",
      "{'total_message_length': 3259, 'average_word_count_per_sentence': 29.823529411764707, 'average_word_length': 5.455621301775148, 'english_conjunctions_count': 7, 'example_words_count': 0, 'total_parentheses_count': 45}\n",
      "==============================\n",
      "Analiz Sonuçları - 6b4e988c-eead-46ff-a35b-b2fd325b2698:\n",
      "{'total_message_length': 302, 'average_word_count_per_sentence': 5.363636363636363, 'average_word_length': 4.288135593220339, 'english_conjunctions_count': 0, 'example_words_count': 0, 'total_parentheses_count': 0}\n",
      "==============================\n",
      "Analiz Sonuçları - 6c37a2d7-f786-4fc2-ba7a-04c3f961a365:\n",
      "{'total_message_length': 18026, 'average_word_count_per_sentence': 71.96296296296296, 'average_word_length': 7.504374678332476, 'english_conjunctions_count': 15, 'example_words_count': 0, 'total_parentheses_count': 291}\n",
      "==============================\n",
      "Analiz Sonuçları - 6d5742c1-77c4-429c-8f6e-ef1262ca5557:\n",
      "{'total_message_length': 17422, 'average_word_count_per_sentence': 39.15151515151515, 'average_word_length': 5.59094427244582, 'english_conjunctions_count': 50, 'example_words_count': 0, 'total_parentheses_count': 178}\n",
      "==============================\n",
      "Analiz Sonuçları - 6de08d44-bb5b-491d-a11e-51caa1eccd0c:\n",
      "{'total_message_length': 972, 'average_word_count_per_sentence': 14.8, 'average_word_length': 5.635135135135135, 'english_conjunctions_count': 1, 'example_words_count': 0, 'total_parentheses_count': 4}\n",
      "==============================\n",
      "Analiz Sonuçları - 7421916d-a0b5-4f0b-ad47-25a0cbf1b239:\n",
      "{'total_message_length': 4238, 'average_word_count_per_sentence': 42.333333333333336, 'average_word_length': 5.437795275590551, 'english_conjunctions_count': 8, 'example_words_count': 0, 'total_parentheses_count': 73}\n",
      "==============================\n",
      "Analiz Sonuçları - 745bb746-6467-4146-8ef5-55f3ee3f589e:\n",
      "{'total_message_length': 4872, 'average_word_count_per_sentence': 30.464285714285715, 'average_word_length': 4.731535756154748, 'english_conjunctions_count': 24, 'example_words_count': 0, 'total_parentheses_count': 16}\n",
      "==============================\n",
      "Analiz Sonuçları - 745ffa9e-f540-488a-b752-a3add11cb30b:\n",
      "{'total_message_length': 772, 'average_word_count_per_sentence': 9.384615384615385, 'average_word_length': 5.434426229508197, 'english_conjunctions_count': 1, 'example_words_count': 0, 'total_parentheses_count': 2}\n",
      "==============================\n",
      "Analiz Sonuçları - 746b8f06-1e89-43b8-b73c-1121eecfc854:\n",
      "{'total_message_length': 103, 'average_word_count_per_sentence': 21.0, 'average_word_length': 3.9523809523809526, 'english_conjunctions_count': 1, 'example_words_count': 0, 'total_parentheses_count': 0}\n",
      "==============================\n",
      "Analiz Sonuçları - 76a73730-6432-4f30-bb59-7a609bc9ba43:\n",
      "{'total_message_length': 4306, 'average_word_count_per_sentence': 37.88235294117647, 'average_word_length': 5.458074534161491, 'english_conjunctions_count': 10, 'example_words_count': 0, 'total_parentheses_count': 46}\n",
      "==============================\n",
      "Analiz Sonuçları - 79cf72c1-d3aa-4e09-8ff2-6a8ad51b22ef:\n",
      "{'total_message_length': 2500, 'average_word_count_per_sentence': 38.2, 'average_word_length': 5.4319371727748695, 'english_conjunctions_count': 7, 'example_words_count': 0, 'total_parentheses_count': 30}\n",
      "==============================\n",
      "Analiz Sonuçları - 7ac3f7a5-bdbb-470c-a1b3-03da5887a408:\n",
      "{'total_message_length': 8665, 'average_word_count_per_sentence': 19.265625, 'average_word_length': 5.855636658556366, 'english_conjunctions_count': 21, 'example_words_count': 0, 'total_parentheses_count': 42}\n",
      "==============================\n",
      "Analiz Sonuçları - 7af13aaa-dd6a-4850-856d-73ea55d0c2ff:\n",
      "{'total_message_length': 4584, 'average_word_count_per_sentence': 29.5, 'average_word_length': 10.024213075060533, 'english_conjunctions_count': 8, 'example_words_count': 2, 'total_parentheses_count': 30}\n",
      "==============================\n",
      "Analiz Sonuçları - 7b0ecddc-caa5-4b81-88ea-cd65a7270900:\n",
      "{'total_message_length': 2569, 'average_word_count_per_sentence': 19.954545454545453, 'average_word_length': 4.876993166287016, 'english_conjunctions_count': 19, 'example_words_count': 0, 'total_parentheses_count': 24}\n",
      "==============================\n",
      "Analiz Sonuçları - 7c734c12-e18b-4de8-9004-c2523878db31:\n",
      "{'total_message_length': 1899, 'average_word_count_per_sentence': 35.875, 'average_word_length': 5.585365853658536, 'english_conjunctions_count': 5, 'example_words_count': 0, 'total_parentheses_count': 31}\n",
      "==============================\n",
      "Analiz Sonuçları - 8121427e-e5b1-400b-8342-d5d1c865f1d7:\n",
      "{'total_message_length': 4693, 'average_word_count_per_sentence': 18.822222222222223, 'average_word_length': 4.442739079102715, 'english_conjunctions_count': 31, 'example_words_count': 0, 'total_parentheses_count': 4}\n",
      "==============================\n",
      "Analiz Sonuçları - 81fdeb2a-e7e5-4a05-8058-d31ea579b0d9:\n",
      "{'total_message_length': 4061, 'average_word_count_per_sentence': 28.05, 'average_word_length': 5.83065953654189, 'english_conjunctions_count': 10, 'example_words_count': 1, 'total_parentheses_count': 12}\n",
      "==============================\n",
      "Analiz Sonuçları - 8505dcd7-9a9e-4ac3-b708-d3ffea7e6bb8:\n",
      "{'total_message_length': 2287, 'average_word_count_per_sentence': 25.928571428571427, 'average_word_length': 5.289256198347108, 'english_conjunctions_count': 6, 'example_words_count': 0, 'total_parentheses_count': 14}\n",
      "==============================\n",
      "Analiz Sonuçları - 854397cb-a264-4bad-b30f-a9c1ba012511:\n",
      "{'total_message_length': 35602, 'average_word_count_per_sentence': 55.6, 'average_word_length': 7.861095738793581, 'english_conjunctions_count': 49, 'example_words_count': 0, 'total_parentheses_count': 553}\n",
      "==============================\n",
      "Analiz Sonuçları - 8a84e6e5-d200-4cc2-a288-5c81201100c7:\n",
      "{'total_message_length': 7400, 'average_word_count_per_sentence': 13.225, 'average_word_length': 6.034026465028355, 'english_conjunctions_count': 17, 'example_words_count': 2, 'total_parentheses_count': 57}\n",
      "==============================\n",
      "Analiz Sonuçları - 8be8e839-6dd0-44e0-b039-170b5b77cf2a:\n",
      "{'total_message_length': 5351, 'average_word_count_per_sentence': 29.25, 'average_word_length': 10.188034188034187, 'english_conjunctions_count': 2, 'example_words_count': 0, 'total_parentheses_count': 26}\n",
      "==============================\n",
      "Analiz Sonuçları - 918f3066-0b5a-46dc-92b8-b4279f0cb26e:\n",
      "{'total_message_length': 67379, 'average_word_count_per_sentence': 100.09859154929578, 'average_word_length': 7.458984100182918, 'english_conjunctions_count': 63, 'example_words_count': 0, 'total_parentheses_count': 601}\n",
      "==============================\n",
      "Analiz Sonuçları - 941a3ef2-7559-430c-8682-830a04a6864c:\n",
      "{'total_message_length': 10328, 'average_word_count_per_sentence': 29.48780487804878, 'average_word_length': 6.795698924731183, 'english_conjunctions_count': 14, 'example_words_count': 0, 'total_parentheses_count': 100}\n",
      "==============================\n",
      "Analiz Sonuçları - 97f57cf9-4f02-4f8c-b65c-8ea0009a82a2:\n",
      "{'total_message_length': 1688, 'average_word_count_per_sentence': 26.3, 'average_word_length': 5.452471482889734, 'english_conjunctions_count': 5, 'example_words_count': 0, 'total_parentheses_count': 4}\n",
      "==============================\n",
      "Analiz Sonuçları - a0132e5d-4e16-4600-b2a7-5b6cd68a8b6a:\n",
      "{'total_message_length': 7748, 'average_word_count_per_sentence': 61.473684210526315, 'average_word_length': 5.302226027397261, 'english_conjunctions_count': 25, 'example_words_count': 0, 'total_parentheses_count': 69}\n",
      "==============================\n",
      "Analiz Sonuçları - a014d72a-81ad-43a4-8a2c-8046b7666320:\n",
      "{'total_message_length': 5776, 'average_word_count_per_sentence': 21.64516129032258, 'average_word_length': 7.5201192250372575, 'english_conjunctions_count': 6, 'example_words_count': 0, 'total_parentheses_count': 55}\n",
      "==============================\n",
      "Analiz Sonuçları - a1e834df-f4f6-4962-bcda-17f8aefc7f86:\n",
      "{'total_message_length': 5240, 'average_word_count_per_sentence': 26.5, 'average_word_length': 4.898584905660377, 'english_conjunctions_count': 28, 'example_words_count': 1, 'total_parentheses_count': 20}\n",
      "==============================\n",
      "Analiz Sonuçları - a70ebc32-7ee1-456f-9fa1-bef302fb0e78:\n",
      "{'total_message_length': 1128, 'average_word_count_per_sentence': 14.692307692307692, 'average_word_length': 4.81151832460733, 'english_conjunctions_count': 3, 'example_words_count': 0, 'total_parentheses_count': 2}\n",
      "==============================\n",
      "Analiz Sonuçları - a8270e39-5d70-4f1a-b031-6b4fcc55de01:\n",
      "{'total_message_length': 5769, 'average_word_count_per_sentence': 34.523809523809526, 'average_word_length': 6.5296551724137935, 'english_conjunctions_count': 8, 'example_words_count': 0, 'total_parentheses_count': 59}\n",
      "==============================\n",
      "Analiz Sonuçları - a948a3e5-70e5-447a-b3b4-baf3661f7b7e:\n",
      "{'total_message_length': 15864, 'average_word_count_per_sentence': 89.41666666666667, 'average_word_length': 6.166821994408202, 'english_conjunctions_count': 38, 'example_words_count': 0, 'total_parentheses_count': 127}\n",
      "==============================\n",
      "Analiz Sonuçları - aa15f751-f65f-4397-b138-df7b8301a12f:\n",
      "{'total_message_length': 10719, 'average_word_count_per_sentence': 70.75, 'average_word_length': 5.2249705535924615, 'english_conjunctions_count': 27, 'example_words_count': 0, 'total_parentheses_count': 107}\n",
      "==============================\n",
      "Analiz Sonuçları - ab775974-7ddb-4a60-aef2-4655d7bb746d:\n",
      "{'total_message_length': 8888, 'average_word_count_per_sentence': 20.76923076923077, 'average_word_length': 5.497037037037037, 'english_conjunctions_count': 18, 'example_words_count': 0, 'total_parentheses_count': 108}\n",
      "==============================\n",
      "Analiz Sonuçları - adb31914-fa31-471d-b476-6111c393ec42:\n",
      "{'total_message_length': 5377, 'average_word_count_per_sentence': 15.586206896551724, 'average_word_length': 4.772123893805309, 'english_conjunctions_count': 14, 'example_words_count': 6, 'total_parentheses_count': 71}\n",
      "==============================\n",
      "Analiz Sonuçları - b0640e51-6879-40cb-a4f5-329f952ef99d:\n",
      "{'total_message_length': 0, 'average_word_count_per_sentence': 0, 'average_word_length': 0, 'english_conjunctions_count': 0, 'example_words_count': 0, 'total_parentheses_count': 0}\n",
      "==============================\n",
      "Analiz Sonuçları - b13a2e11-1f0a-4f1e-9735-5f0674713593:\n",
      "{'total_message_length': 27730, 'average_word_count_per_sentence': 114.70833333333333, 'average_word_length': 8.56738103886669, 'english_conjunctions_count': 38, 'example_words_count': 4, 'total_parentheses_count': 369}\n",
      "==============================\n",
      "Analiz Sonuçları - b24c3a33-2952-4ae4-9f2d-643d8fdbc600:\n",
      "{'total_message_length': 44974, 'average_word_count_per_sentence': 84.16949152542372, 'average_word_length': 7.386830447039871, 'english_conjunctions_count': 61, 'example_words_count': 0, 'total_parentheses_count': 595}\n",
      "==============================\n",
      "Analiz Sonuçları - b47559b3-3b5f-4958-bfa5-ec9f7a36aec0:\n",
      "{'total_message_length': 7411, 'average_word_count_per_sentence': 38.34615384615385, 'average_word_length': 5.936810431293882, 'english_conjunctions_count': 26, 'example_words_count': 0, 'total_parentheses_count': 54}\n",
      "==============================\n",
      "Analiz Sonuçları - b57fe283-97a4-457f-a5d0-ef441650c968:\n",
      "{'total_message_length': 1477, 'average_word_count_per_sentence': 19.75, 'average_word_length': 5.206751054852321, 'english_conjunctions_count': 4, 'example_words_count': 0, 'total_parentheses_count': 16}\n",
      "==============================\n",
      "Analiz Sonuçları - b5fe7d4d-e4a5-4ebd-8058-069d6505eb01:\n",
      "{'total_message_length': 5727, 'average_word_count_per_sentence': 39.89473684210526, 'average_word_length': 6.014511873350924, 'english_conjunctions_count': 7, 'example_words_count': 1, 'total_parentheses_count': 78}\n",
      "==============================\n",
      "Analiz Sonuçları - b61f3f4c-785e-4d7c-b963-4480ece4656f:\n",
      "{'total_message_length': 13801, 'average_word_count_per_sentence': 53.407407407407405, 'average_word_length': 7.6900138696255205, 'english_conjunctions_count': 16, 'example_words_count': 0, 'total_parentheses_count': 241}\n",
      "==============================\n",
      "Analiz Sonuçları - b73f91f8-732f-4a48-bcbd-eadbbb457a94:\n",
      "{'total_message_length': 9189, 'average_word_count_per_sentence': 67.47058823529412, 'average_word_length': 6.446381865736704, 'english_conjunctions_count': 19, 'example_words_count': 0, 'total_parentheses_count': 133}\n",
      "==============================\n",
      "Analiz Sonuçları - ba18e4e8-2c26-46d4-ba31-cc21947aabd5:\n",
      "{'total_message_length': 7229, 'average_word_count_per_sentence': 36.96296296296296, 'average_word_length': 6.057114228456914, 'english_conjunctions_count': 10, 'example_words_count': 2, 'total_parentheses_count': 62}\n",
      "==============================\n",
      "Analiz Sonuçları - c0f51763-0c69-45fb-a4d7-6906d53db43d:\n",
      "{'total_message_length': 208, 'average_word_count_per_sentence': 12.333333333333334, 'average_word_length': 4.675675675675675, 'english_conjunctions_count': 1, 'example_words_count': 0, 'total_parentheses_count': 0}\n",
      "==============================\n",
      "Analiz Sonuçları - c21be348-17bd-4fdd-88f5-137f6a13cdee:\n",
      "{'total_message_length': 3464, 'average_word_count_per_sentence': 28.5, 'average_word_length': 5.502923976608187, 'english_conjunctions_count': 9, 'example_words_count': 0, 'total_parentheses_count': 31}\n",
      "==============================\n",
      "Analiz Sonuçları - c65a33f5-6acf-4ff1-86fe-6003f165d44e:\n",
      "{'total_message_length': 2810, 'average_word_count_per_sentence': 21.6, 'average_word_length': 7.037037037037037, 'english_conjunctions_count': 5, 'example_words_count': 0, 'total_parentheses_count': 32}\n",
      "==============================\n",
      "Analiz Sonuçları - c91d6fef-baf5-4e77-8bfc-b14fb7fc770d:\n",
      "{'total_message_length': 9473, 'average_word_count_per_sentence': 30.085714285714285, 'average_word_length': 7.027540360873695, 'english_conjunctions_count': 10, 'example_words_count': 0, 'total_parentheses_count': 121}\n",
      "==============================\n",
      "Analiz Sonuçları - cc9ecae2-a3bf-43df-9628-56f587f400be:\n",
      "{'total_message_length': 8042, 'average_word_count_per_sentence': 63.5, 'average_word_length': 5.396325459317585, 'english_conjunctions_count': 21, 'example_words_count': 0, 'total_parentheses_count': 83}\n",
      "==============================\n",
      "Analiz Sonuçları - d4ab3f85-fdfb-434b-9774-96d94fd15d69:\n",
      "{'total_message_length': 7574, 'average_word_count_per_sentence': 59.21052631578947, 'average_word_length': 5.578666666666667, 'english_conjunctions_count': 18, 'example_words_count': 0, 'total_parentheses_count': 62}\n",
      "==============================\n",
      "Analiz Sonuçları - d61b757d-88bc-4a33-a620-0d76712207c3:\n",
      "{'total_message_length': 11711, 'average_word_count_per_sentence': 35.68, 'average_word_length': 5.378363228699552, 'english_conjunctions_count': 29, 'example_words_count': 0, 'total_parentheses_count': 160}\n",
      "==============================\n",
      "Analiz Sonuçları - d846484d-9257-40b4-98b9-6854288e8add:\n",
      "{'total_message_length': 10021, 'average_word_count_per_sentence': 52.30769230769231, 'average_word_length': 6.10514705882353, 'english_conjunctions_count': 24, 'example_words_count': 0, 'total_parentheses_count': 158}\n",
      "==============================\n",
      "Analiz Sonuçları - da219169-aacb-48b8-abdc-e25f08ad029e:\n",
      "{'total_message_length': 1838, 'average_word_count_per_sentence': 19.533333333333335, 'average_word_length': 5.296928327645051, 'english_conjunctions_count': 8, 'example_words_count': 0, 'total_parentheses_count': 8}\n",
      "==============================\n",
      "Analiz Sonuçları - da6b70d5-29f6-491a-ad46-037c77067128:\n",
      "{'total_message_length': 0, 'average_word_count_per_sentence': 0, 'average_word_length': 0, 'english_conjunctions_count': 0, 'example_words_count': 0, 'total_parentheses_count': 0}\n",
      "==============================\n",
      "Analiz Sonuçları - db921a07-c234-41c6-8891-ef1b8a22a2f5:\n",
      "{'total_message_length': 4254, 'average_word_count_per_sentence': 25.344827586206897, 'average_word_length': 4.650340136054422, 'english_conjunctions_count': 19, 'example_words_count': 0, 'total_parentheses_count': 11}\n",
      "==============================\n",
      "Analiz Sonuçları - dc9d3dc4-de7d-44e0-916c-04c85f94012a:\n",
      "{'total_message_length': 20846, 'average_word_count_per_sentence': 56.47727272727273, 'average_word_length': 6.622132796780684, 'english_conjunctions_count': 40, 'example_words_count': 0, 'total_parentheses_count': 272}\n",
      "==============================\n",
      "Analiz Sonuçları - dd898b12-b04b-404e-9b59-f846a162c177:\n",
      "{'total_message_length': 9388, 'average_word_count_per_sentence': 24.97872340425532, 'average_word_length': 6.680579216354344, 'english_conjunctions_count': 22, 'example_words_count': 0, 'total_parentheses_count': 110}\n",
      "==============================\n",
      "Analiz Sonuçları - dfe46143-c07c-4bb6-bddc-7458995dba2f:\n",
      "{'total_message_length': 15052, 'average_word_count_per_sentence': 41.11363636363637, 'average_word_length': 6.911553344389166, 'english_conjunctions_count': 29, 'example_words_count': 0, 'total_parentheses_count': 215}\n",
      "==============================\n",
      "Analiz Sonuçları - e264c424-a241-43f7-acca-9fbbf21dc1c6:\n",
      "{'total_message_length': 18764, 'average_word_count_per_sentence': 70.42857142857143, 'average_word_length': 6.440162271805274, 'english_conjunctions_count': 47, 'example_words_count': 0, 'total_parentheses_count': 232}\n",
      "==============================\n",
      "Analiz Sonuçları - e469cc3a-40fc-44e1-a7b7-3b2fbe621a8b:\n",
      "{'total_message_length': 1533, 'average_word_count_per_sentence': 39.0, 'average_word_length': 6.364102564102564, 'english_conjunctions_count': 3, 'example_words_count': 0, 'total_parentheses_count': 6}\n",
      "==============================\n",
      "Analiz Sonuçları - e76b910b-3156-4801-a73e-ddb16c763d43:\n",
      "{'total_message_length': 11137, 'average_word_count_per_sentence': 44.21212121212121, 'average_word_length': 6.4084989718985605, 'english_conjunctions_count': 24, 'example_words_count': 0, 'total_parentheses_count': 161}\n",
      "==============================\n",
      "Analiz Sonuçları - e779a2d6-6e4e-4ade-8a30-d624166c2ab3:\n",
      "{'total_message_length': 2901, 'average_word_count_per_sentence': 31.0, 'average_word_length': 5.3986175115207375, 'english_conjunctions_count': 4, 'example_words_count': 0, 'total_parentheses_count': 10}\n",
      "==============================\n",
      "Analiz Sonuçları - e8fd2278-1620-432d-81cb-02cac8543456:\n",
      "{'total_message_length': 6991, 'average_word_count_per_sentence': 79.0, 'average_word_length': 5.189873417721519, 'english_conjunctions_count': 16, 'example_words_count': 0, 'total_parentheses_count': 102}\n",
      "==============================\n",
      "Analiz Sonuçları - ebe86296-3cae-429a-9a7c-aa8f21cfd7cb:\n",
      "{'total_message_length': 2581, 'average_word_count_per_sentence': 18.761904761904763, 'average_word_length': 5.401015228426396, 'english_conjunctions_count': 12, 'example_words_count': 0, 'total_parentheses_count': 2}\n",
      "==============================\n",
      "Analiz Sonuçları - ef5b3fbc-f5d2-4446-bb4f-7d8b2a3026e9:\n",
      "{'total_message_length': 18712, 'average_word_count_per_sentence': 23.73, 'average_word_length': 6.321533923303835, 'english_conjunctions_count': 28, 'example_words_count': 0, 'total_parentheses_count': 228}\n",
      "==============================\n",
      "Analiz Sonuçları - f24219d6-07f0-4baf-80ac-18475dc5b66f:\n",
      "{'total_message_length': 4035, 'average_word_count_per_sentence': 30.857142857142858, 'average_word_length': 5.219135802469136, 'english_conjunctions_count': 9, 'example_words_count': 1, 'total_parentheses_count': 69}\n",
      "==============================\n",
      "Analiz Sonuçları - f2f18684-4a16-4c05-a2d1-c0f96d1de869:\n",
      "{'total_message_length': 23006, 'average_word_count_per_sentence': 41.76056338028169, 'average_word_length': 6.075210792580101, 'english_conjunctions_count': 34, 'example_words_count': 1, 'total_parentheses_count': 192}\n",
      "==============================\n",
      "Analiz Sonuçları - f852596d-fdca-45aa-9050-d4f76ce6a53c:\n",
      "{'total_message_length': 21776, 'average_word_count_per_sentence': 84.43333333333334, 'average_word_length': 7.281879194630872, 'english_conjunctions_count': 31, 'example_words_count': 0, 'total_parentheses_count': 307}\n",
      "==============================\n",
      "Analiz Sonuçları - f8ec3336-fd48-4654-ad98-62ccfb96d096:\n",
      "{'total_message_length': 7143, 'average_word_count_per_sentence': 69.85714285714286, 'average_word_length': 5.768916155419223, 'english_conjunctions_count': 10, 'example_words_count': 0, 'total_parentheses_count': 78}\n",
      "==============================\n",
      "Analiz Sonuçları - fb8de815-224c-4d06-9fd4-7156d1a9920d:\n",
      "{'total_message_length': 11417, 'average_word_count_per_sentence': 46.61290322580645, 'average_word_length': 6.3695501730103805, 'english_conjunctions_count': 13, 'example_words_count': 0, 'total_parentheses_count': 199}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_messages(user_id, messages):\n",
    "    total_message_length = sum(len(message) for message in messages)\n",
    "    average_message_length = total_message_length / len(messages) if len(messages) > 0 else 0\n",
    "\n",
    "    all_text = \" \".join(messages)\n",
    "    words = all_text.split()\n",
    "\n",
    "    total_word_count = len(words)\n",
    "    average_word_count_per_sentence = total_word_count / len(messages) if len(messages) > 0 else 0\n",
    "\n",
    "    average_word_length = sum(len(word) for word in words) / total_word_count if total_word_count > 0 else 0\n",
    "\n",
    "    english_conjunctions_count = sum(1 for word in words if word.lower() in {\"and\", \"or\", \"but\", \"so\", \"yet\"})\n",
    "\n",
    "    example_words_count = sum(1 for word in words if word.lower() in {\"example\", \"illustrate\", \"demonstrate\"})\n",
    "\n",
    "    total_parentheses_count = sum(message.count('(') + message.count(')') for message in messages)\n",
    "\n",
    "    prompt_num = len(messages)\n",
    "\n",
    "    return {\n",
    "        \"total_message_length\": total_message_length, #EKLE\n",
    "        \n",
    "        \"average_word_count_per_sentence\": average_word_count_per_sentence, #EKLE\n",
    "        \"average_word_length\": average_word_length, #EKLE\n",
    "        \"english_conjunctions_count\": english_conjunctions_count, #EKLE\n",
    "        \"example_words_count\": example_words_count, #EKLE\n",
    "        \"total_parentheses_count\": total_parentheses_count #EKLE\n",
    "    }\n",
    "\n",
    "user_analyses = {user_id: analyze_messages(user_id, messages) for user_id, messages in code2prompts.items()}\n",
    "\n",
    "# Sonuçları yazdırma\n",
    "for user_id, analysis in user_analyses.items():\n",
    "    print(f\"Analiz Sonuçları - {user_id}:\\n{analysis}\\n{'='*30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtotal_parentheses_count\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mexample_words_count\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39menglish_conjunctions_count\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['total_parentheses_count'] = None\n",
    "df['example_words_count'] = None\n",
    "df['english_conjunctions_count'] = None\n",
    "df['average_word_length'] = None\n",
    "df['average_word_count_per_sentence'] = None\n",
    "df['total_message_length'] = None\n",
    "\n",
    "\n",
    "for code, values in user_analyses.items():\n",
    "    mask = df['user_prompts'] == code  # Code'a uygun satırları bul\n",
    "    for key, value in values.items():\n",
    "        df.loc[mask, key] = value\n",
    "\n",
    "# DataFrame'i göster\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINALIZE YOUR DATAFRAME AND CONCATANATE WITH SCORES\n",
    "\n",
    "### AFTER THAT START CREATING YOUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6a2003ad-a05a-41c9-9d48-e98491a90499</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04f91058-d0f8-4324-83b2-19c671f433dc</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81fdeb2a-e7e5-4a05-8058-d31ea579b0d9</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6a903495-c5be-4263-b4dd-75e2bbc30434</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6d5742c1-77c4-429c-8f6e-ef1262ca5557</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   code  grade\n",
       "0  6a2003ad-a05a-41c9-9d48-e98491a90499   90.0\n",
       "1  04f91058-d0f8-4324-83b2-19c671f433dc   97.0\n",
       "2  81fdeb2a-e7e5-4a05-8058-d31ea579b0d9   94.0\n",
       "3  6a903495-c5be-4263-b4dd-75e2bbc30434   97.0\n",
       "4  6d5742c1-77c4-429c-8f6e-ef1262ca5557   93.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the scores\n",
    "scores = pd.read_csv(\"data/scores.csv\", sep=\",\")\n",
    "scores[\"code\"] = scores[\"code\"].apply(lambda x: x.strip())\n",
    "\n",
    "# selecting the columns we need and we care\n",
    "scores = scores[[\"code\", \"grade\"]]\n",
    "\n",
    "# show some examples\n",
    "scores.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINALLY LETS ADD 1 MORE FEATURE ABOUT CHATGPT CODE RESPONSE\n",
    "\n",
    "#### FROM THE GIVEN CODE ANSWERS, WE SELECT THE STUDENTS WHO RECEIVE 100 AND ARE CLOSEST TO EACH OTHER. THEN LET'S MEASURE THE SIMILAR  CODES OF EACH STUDENT TO THESE STUDENTS' CODES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity1(code_dict):\n",
    "    # Extract code values from the dictionary\n",
    "    code_values = list(code_dict.values())\n",
    "\n",
    "    # Use TfidfVectorizer to convert code values into TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(code_values)\n",
    "\n",
    "    # Calculate cosine similarity between the TF-IDF vectors\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Create a dictionary to store similarities\n",
    "    similarity_dict = {}\n",
    "    names = list(code_dict.keys())\n",
    "\n",
    "    # Fill the similarity dictionary\n",
    "    for i in range(len(names)):\n",
    "        for j in range(i + 1, len(names)):\n",
    "            pair = f\"{names[i]} - {names[j]}\"\n",
    "            similarity_dict[pair] = similarity_matrix[i, j]\n",
    "\n",
    "    return similarity_dict\n",
    "\n",
    "# Example usage\n",
    "similarity_dict1 = calculate_similarity1(code_dict)\n",
    "\n",
    "# Print the similarity values\n",
    "for pair, similarity in similarity_dict1.items():\n",
    "    print(f\"Similarity between {pair}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_students = scores[scores[\"grade\"] == 100]\n",
    "\n",
    "# Eğer birden fazla varsa, tüm öğrencilerin kodlarını bir liste olarak alabilirsiniz\n",
    "top_students_codes = top_students[\"code\"].tolist()\n",
    "\n",
    "max_similiars = []\n",
    "\n",
    "for pair, similarity in similarity_dict1.items():\n",
    "    if similarity >= 0.1:\n",
    "        #max_similiars.append(pair)\n",
    "        element1, element2 = pair.split(\" - \")\n",
    "        if element1 in top_students_codes and element2 in top_students_codes:\n",
    "            max_similiars.append(element1)\n",
    "            max_similiars.append(element2)\n",
    "    \n",
    "max_similiars = list(set(max_similiars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_similarities = {}\n",
    "\n",
    "print(average_similarities)\n",
    "\n",
    "for code in scores[\"code\"]:\n",
    "    points = []\n",
    "    for best in max_similiars:\n",
    "        if code != best:\n",
    "            pair1 = code + \" - \" + best\n",
    "            pair2 = best + \" - \" + code\n",
    "            if pair1 in similarity_dict1:\n",
    "                #print(\"p1 \" + str(similarity_dict1[pair1]))\n",
    "                points.append(similarity_dict1[pair1])\n",
    "            elif pair2 in similarity_dict1:\n",
    "                #print(\"p2 \" + str(similarity_dict1[pair2]))\n",
    "                points.append(similarity_dict1[pair2])\n",
    "    if len(points) != 0:\n",
    "        avg = sum(points) / len(points)\n",
    "    else:\n",
    "        avg = 0\n",
    "        print(code)\n",
    "    average_similarities[code] = avg\n",
    "    \n",
    "\n",
    "print(average_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values[0] = 'user_id'\n",
    "\n",
    "df['code_similarity_compare2_bestgrades'] = df['new_column_name'].map(average_similarities)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We know that some of the data is inaccurate and problematic. Now let's fill in the missing data of these faulty people with average values ​​according to the norm range they received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 86 puan için 88 ve 84 aralığı\n",
    "# 99 puan için 98 ve 100 aralığı\n",
    "# 98 puan için 97 ve 99 aralığı\n",
    "# 95 puan için 94 ve 96 aralığı\n",
    "\n",
    "recive_86 = filtered_codes = df.loc[(df['score'] >= 84) & (df['score'] <= 88), 'code'].tolist()\n",
    "recive_99 = filtered_codes = df.loc[(df['score'] >= 98) & (df['score'] <= 100), 'code'].tolist()\n",
    "recive_98 = filtered_codes = df.loc[(df['score'] >= 97) & (df['score'] <= 99), 'code'].tolist()\n",
    "recive_95 = filtered_codes = df.loc[(df['score'] >= 94) & (df['score'] <= 95), 'code'].tolist()\n",
    "\n",
    "averages_dict86 = {}\n",
    "\n",
    "# Her bir kolon için ortalama değeri hesapla ve dictionary'e ekle\n",
    "for column in df.columns[1:]:  # İlk sütun user_id, onu hariç tutuyoruz\n",
    "    averages_dict86[column] = df[df['user_id'].isin(recive_86)][column].mean()\n",
    "\n",
    "averages_dict99 = {}\n",
    "\n",
    "# Her bir kolon için ortalama değeri hesapla ve dictionary'e ekle\n",
    "for column in df.columns[1:]:  # İlk sütun user_id, onu hariç tutuyoruz\n",
    "    averages_dict99[column] = df[df['user_id'].isin(recive_99)][column].mean()\n",
    "\n",
    "\n",
    "averages_dict98 = {}\n",
    "\n",
    "# Her bir kolon için ortalama değeri hesapla ve dictionary'e ekle\n",
    "for column in df.columns[1:]:  # İlk sütun user_id, onu hariç tutuyoruz\n",
    "    averages_dict98[column] = df[df['user_id'].isin(recive_98)][column].mean()\n",
    "\n",
    "averages_dict95 = {}\n",
    "\n",
    "# Her bir kolon için ortalama değeri hesapla ve dictionary'e ekle\n",
    "for column in df.columns[1:]:  # İlk sütun user_id, onu hariç tutuyoruz\n",
    "    averages_dict95[column] = df[df['user_id'].isin(recive_95)][column].mean()\n",
    "\n",
    "\n",
    "def update_dataframe_with_dict(df, user_id, update_dict):\n",
    "    # Belirli user_id'ye sahip olan satırı bul\n",
    "    user_index = df.index[df['user_id'] == user_id].tolist()\n",
    "\n",
    "    # Eğer user_id bulunamazsa hata mesajı yazdır\n",
    "    if not user_index:\n",
    "        print(f\"User with user_id {user_id} not found.\")\n",
    "        return df\n",
    "\n",
    "    user_index = user_index[0]\n",
    "\n",
    "    # Her bir kolon için dictionary'den gelen değeri DataFrame'e yaz\n",
    "    for column, value in update_dict.items():\n",
    "        if column in df.columns:\n",
    "            df.loc[user_index, column] = value\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = update_dataframe_with_dict(df, '139235c7-736c-4237-92f0-92e8c116832c', averages_dict86)\n",
    "\n",
    "df = update_dataframe_with_dict(df, '668ad17e-0240-49f7-b5a7-d22e502554c6', averages_dict99)\n",
    "\n",
    "df = update_dataframe_with_dict(df, 'b0640e51-6879-40cb-a4f5-329f952ef99d', averages_dict98)\n",
    "\n",
    "df = update_dataframe_with_dict(df, 'da6b70d5-29f6-491a-ad46-037c77067128', averages_dict95)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's check grades distribution\n",
    "\n",
    "plt.title('Histogram Grades')\n",
    "plt.hist(scores[\"grade\"], rwidth=.8, bins=np.arange(min(scores[\"grade\"]), max(scores[\"grade\"])+2) - 0.5)\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HTML_File</th>\n",
       "      <th>Q0</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0031c86e-81f4-4eef-9e0e-28037abf9883</td>\n",
       "      <td>0.131876</td>\n",
       "      <td>0.285558</td>\n",
       "      <td>0.380305</td>\n",
       "      <td>0.294066</td>\n",
       "      <td>0.365903</td>\n",
       "      <td>0.294589</td>\n",
       "      <td>0.238968</td>\n",
       "      <td>0.332627</td>\n",
       "      <td>0.124415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0225686d-b825-4cac-8691-3a3a5343df2b</td>\n",
       "      <td>0.122576</td>\n",
       "      <td>0.273231</td>\n",
       "      <td>0.337141</td>\n",
       "      <td>0.305693</td>\n",
       "      <td>0.296665</td>\n",
       "      <td>0.323831</td>\n",
       "      <td>0.312685</td>\n",
       "      <td>0.376181</td>\n",
       "      <td>0.209524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>041f950b-c013-409a-a642-cffff60b9d4b</td>\n",
       "      <td>0.010530</td>\n",
       "      <td>0.111217</td>\n",
       "      <td>0.168907</td>\n",
       "      <td>0.115451</td>\n",
       "      <td>0.081844</td>\n",
       "      <td>0.124755</td>\n",
       "      <td>0.130175</td>\n",
       "      <td>0.235636</td>\n",
       "      <td>0.061148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04f91058-d0f8-4324-83b2-19c671f433dc</td>\n",
       "      <td>0.075920</td>\n",
       "      <td>0.184396</td>\n",
       "      <td>0.277807</td>\n",
       "      <td>0.223789</td>\n",
       "      <td>0.240406</td>\n",
       "      <td>0.197946</td>\n",
       "      <td>0.239033</td>\n",
       "      <td>0.298362</td>\n",
       "      <td>0.167645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>089eb66d-4c3a-4f58-b98f-a3774a2efb34</td>\n",
       "      <td>0.069941</td>\n",
       "      <td>0.156857</td>\n",
       "      <td>0.249157</td>\n",
       "      <td>0.239491</td>\n",
       "      <td>0.215537</td>\n",
       "      <td>0.198821</td>\n",
       "      <td>0.210744</td>\n",
       "      <td>0.289835</td>\n",
       "      <td>0.165927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              HTML_File        Q0        Q1        Q2   \n",
       "0  0031c86e-81f4-4eef-9e0e-28037abf9883  0.131876  0.285558  0.380305  \\\n",
       "1  0225686d-b825-4cac-8691-3a3a5343df2b  0.122576  0.273231  0.337141   \n",
       "2  041f950b-c013-409a-a642-cffff60b9d4b  0.010530  0.111217  0.168907   \n",
       "3  04f91058-d0f8-4324-83b2-19c671f433dc  0.075920  0.184396  0.277807   \n",
       "4  089eb66d-4c3a-4f58-b98f-a3774a2efb34  0.069941  0.156857  0.249157   \n",
       "\n",
       "         Q3        Q4        Q5        Q6        Q7        Q8  \n",
       "0  0.294066  0.365903  0.294589  0.238968  0.332627  0.124415  \n",
       "1  0.305693  0.296665  0.323831  0.312685  0.376181  0.209524  \n",
       "2  0.115451  0.081844  0.124755  0.130175  0.235636  0.061148  \n",
       "3  0.223789  0.240406  0.197946  0.239033  0.298362  0.167645  \n",
       "4  0.239491  0.215537  0.198821  0.210744  0.289835  0.165927  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(inplace=True, drop=False)\n",
    "df.rename(columns={\"index\": \"code\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question_mapping_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df, \u001b[43mquestion_mapping_scores\u001b[49m, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'question_mapping_scores' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, question_mapping_scores, on=\"code\", how=\"left\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging scores with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>#user_prompts</th>\n",
       "      <th>#error</th>\n",
       "      <th>#no</th>\n",
       "      <th>#thank</th>\n",
       "      <th>#next</th>\n",
       "      <th>#entropy</th>\n",
       "      <th>prompt_avg_chars</th>\n",
       "      <th>response_avg_chars</th>\n",
       "      <th>Q_0</th>\n",
       "      <th>Q_1</th>\n",
       "      <th>Q_2</th>\n",
       "      <th>Q_3</th>\n",
       "      <th>Q_4</th>\n",
       "      <th>Q_5</th>\n",
       "      <th>Q_6</th>\n",
       "      <th>Q_7</th>\n",
       "      <th>Q_8</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0031c86e-81f4-4eef-9e0e-28037abf9883</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.205748</td>\n",
       "      <td>212.206370</td>\n",
       "      <td>0.153541</td>\n",
       "      <td>0.294879</td>\n",
       "      <td>0.522169</td>\n",
       "      <td>0.352243</td>\n",
       "      <td>0.460606</td>\n",
       "      <td>0.553876</td>\n",
       "      <td>0.167250</td>\n",
       "      <td>0.182938</td>\n",
       "      <td>0.195440</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0225686d-b825-4cac-8691-3a3a5343df2b</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.304163</td>\n",
       "      <td>113.633582</td>\n",
       "      <td>0.192013</td>\n",
       "      <td>0.795607</td>\n",
       "      <td>0.772184</td>\n",
       "      <td>0.882656</td>\n",
       "      <td>0.607114</td>\n",
       "      <td>0.987511</td>\n",
       "      <td>0.892586</td>\n",
       "      <td>0.570741</td>\n",
       "      <td>0.543866</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>041f950b-c013-409a-a642-cffff60b9d4b</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.262324</td>\n",
       "      <td>17.187601</td>\n",
       "      <td>0.258306</td>\n",
       "      <td>0.295923</td>\n",
       "      <td>0.624824</td>\n",
       "      <td>0.351872</td>\n",
       "      <td>0.643038</td>\n",
       "      <td>0.454314</td>\n",
       "      <td>0.540269</td>\n",
       "      <td>0.546506</td>\n",
       "      <td>0.325793</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04f91058-d0f8-4324-83b2-19c671f433dc</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.123346</td>\n",
       "      <td>107.092566</td>\n",
       "      <td>0.145965</td>\n",
       "      <td>0.117841</td>\n",
       "      <td>0.267346</td>\n",
       "      <td>0.316809</td>\n",
       "      <td>0.333889</td>\n",
       "      <td>0.309084</td>\n",
       "      <td>0.192434</td>\n",
       "      <td>0.261892</td>\n",
       "      <td>0.407106</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>089eb66d-4c3a-4f58-b98f-a3774a2efb34</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>18.850913</td>\n",
       "      <td>0.344182</td>\n",
       "      <td>0.575528</td>\n",
       "      <td>0.782883</td>\n",
       "      <td>0.624833</td>\n",
       "      <td>0.724872</td>\n",
       "      <td>0.872171</td>\n",
       "      <td>0.684797</td>\n",
       "      <td>0.945305</td>\n",
       "      <td>0.511769</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   code  #user_prompts  #error  #no  #thank  \\\n",
       "0  0031c86e-81f4-4eef-9e0e-28037abf9883           14.0     3.0  1.0     0.0   \n",
       "1  0225686d-b825-4cac-8691-3a3a5343df2b           18.0     0.0  0.0     0.0   \n",
       "2  041f950b-c013-409a-a642-cffff60b9d4b            9.0     3.0  0.0     1.0   \n",
       "3  04f91058-d0f8-4324-83b2-19c671f433dc           20.0     1.0  1.0     0.0   \n",
       "4  089eb66d-4c3a-4f58-b98f-a3774a2efb34           86.0     1.0  5.0     0.0   \n",
       "\n",
       "   #next  #entropy  prompt_avg_chars  response_avg_chars       Q_0       Q_1  \\\n",
       "0    0.0       0.0          2.205748          212.206370  0.153541  0.294879   \n",
       "1    0.0       3.0          0.304163          113.633582  0.192013  0.795607   \n",
       "2    0.0       3.0          0.262324           17.187601  0.258306  0.295923   \n",
       "3    0.0       3.0          0.123346          107.092566  0.145965  0.117841   \n",
       "4    0.0      26.0          0.017986           18.850913  0.344182  0.575528   \n",
       "\n",
       "        Q_2       Q_3       Q_4       Q_5       Q_6       Q_7       Q_8  grade  \n",
       "0  0.522169  0.352243  0.460606  0.553876  0.167250  0.182938  0.195440   48.0  \n",
       "1  0.772184  0.882656  0.607114  0.987511  0.892586  0.570741  0.543866   99.0  \n",
       "2  0.624824  0.351872  0.643038  0.454314  0.540269  0.546506  0.325793   90.0  \n",
       "3  0.267346  0.316809  0.333889  0.309084  0.192434  0.261892  0.407106   97.0  \n",
       "4  0.782883  0.624833  0.724872  0.872171  0.684797  0.945305  0.511769  100.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.merge(df, scores, on='code', how=\"left\")\n",
    "temp_df.dropna(inplace=True)\n",
    "temp_df.drop_duplicates(\"code\",inplace=True, keep=\"first\")\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122, 17) (122,)\n"
     ]
    }
   ],
   "source": [
    "X = temp_df[temp_df.columns[1:-1]].to_numpy()\n",
    "y = temp_df[\"grade\"].to_numpy()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 97\n",
      "Test set size: 25\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting and Analyzing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=10, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=10, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=10, random_state=0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = DecisionTreeRegressor(random_state=0,criterion='squared_error', max_depth=10)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 0 has MSE 163.42034222552866\n",
      "Node 1 has MSE 126.39409722222263\n",
      "Node 2 has MSE 64.35004526935154\n",
      "Node 3 has MSE 167.02777777777737\n",
      "Node 4 has MSE 83.96484375\n",
      "Node 5 has MSE 51.6875\n",
      "Node 6 has MSE 0.0\n",
      "Node 7 has MSE 10.489795918369055\n",
      "Node 8 has MSE 0.6666666666660603\n",
      "Node 9 has MSE 0.0\n",
      "Node 10 has MSE 0.25\n",
      "Node 11 has MSE 0.0\n",
      "Node 12 has MSE 0.0\n",
      "Node 13 has MSE 0.0\n",
      "Node 14 has MSE 54.359375\n",
      "Node 15 has MSE 1.0\n",
      "Node 16 has MSE 0.0\n",
      "Node 17 has MSE 0.0\n",
      "Node 18 has MSE 26.58333333333303\n",
      "Node 19 has MSE 12.6875\n",
      "Node 20 has MSE 4.66666666666697\n",
      "Node 21 has MSE 0.25\n",
      "Node 22 has MSE 0.0\n",
      "Node 23 has MSE 0.0\n",
      "Node 24 has MSE 0.0\n",
      "Node 25 has MSE 0.0\n",
      "Node 26 has MSE 9.0\n",
      "Node 27 has MSE 0.0\n",
      "Node 28 has MSE 0.0\n",
      "Node 29 has MSE 100.0\n",
      "Node 30 has MSE 0.0\n",
      "Node 31 has MSE 0.0\n",
      "Node 32 has MSE 27.533240997230678\n",
      "Node 33 has MSE 25.170927684439448\n",
      "Node 34 has MSE 22.85062487601499\n",
      "Node 35 has MSE 6.888888888887777\n",
      "Node 36 has MSE 0.25\n",
      "Node 37 has MSE 0.0\n",
      "Node 38 has MSE 0.0\n",
      "Node 39 has MSE 0.0\n",
      "Node 40 has MSE 20.180579584774023\n",
      "Node 41 has MSE 18.778792604143746\n",
      "Node 42 has MSE 19.199762187869055\n",
      "Node 43 has MSE 11.173333333332266\n",
      "Node 44 has MSE 11.0\n",
      "Node 45 has MSE 1.636363636363967\n",
      "Node 46 has MSE 19.876690102755674\n",
      "Node 47 has MSE 17.644557823128707\n",
      "Node 48 has MSE 0.0\n",
      "Node 49 has MSE 4.765432098765814\n",
      "Node 50 has MSE 0.12244897959135415\n",
      "Node 51 has MSE 0.0\n",
      "Node 52 has MSE 0.0\n",
      "Node 53 has MSE 6.25\n",
      "Node 54 has MSE 0.0\n",
      "Node 55 has MSE 0.0\n",
      "Node 56 has MSE 0.0\n",
      "Node 57 has MSE 4.222222222223536\n",
      "Node 58 has MSE 1.0\n",
      "Node 59 has MSE 0.0\n",
      "Node 60 has MSE 0.0\n",
      "Node 61 has MSE 0.0\n",
      "Node 62 has MSE 0.0\n",
      "Node 63 has MSE 1806.25\n",
      "Node 64 has MSE 0.0\n",
      "Node 65 has MSE 0.0\n",
      "Node 66 has MSE 0.0\n"
     ]
    }
   ],
   "source": [
    "extracted_MSEs = regressor.tree_.impurity   \n",
    "for idx, MSE in enumerate(regressor.tree_.impurity):\n",
    "    print(\"Node {} has MSE {}\".format(idx,MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hw.pdf'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting the Tree \n",
    "dot_data = tree.export_graphviz(regressor, out_file=None, feature_names=temp_df.columns[1:-1])\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"hw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 8.279086892488953\n",
      "MSE TEST: 101.55122448979591\n",
      "R2 Train: 0.9493387005574656\n",
      "R2 TEST: 0.09543774326422416\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculation of Mean Squared Error (MSE)\n",
    "print(\"MSE Train:\", mean_squared_error(y_train,y_train_pred))\n",
    "print(\"MSE TEST:\", mean_squared_error(y_test,y_test_pred))\n",
    "\n",
    "print(\"R2 Train:\", r2_score(y_train,y_train_pred))\n",
    "print(\"R2 TEST:\", r2_score(y_test,y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
